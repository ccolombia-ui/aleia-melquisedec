# Tasks - Architecture Best Practices v1.0.0

## Visi√≥n General

Este spec tiene 6 tasks para implementar arquitectura √≥ptima basada en investigaci√≥n. **NO hay migraci√≥n** (nada implementado a√∫n), solo crear configuraci√≥n correcta desde el inicio.

**Orden de ejecuci√≥n**: Secuencial (1 ‚Üí 2 ‚Üí 3 ‚Üí 4 ‚Üí 5 ‚Üí 6)
**Tiempo estimado total**: 4-5 d√≠as (con rostros DAATH-ZEN trabajando en paralelo cuando posible)

---

## Task 1: Crear Neo4j Vector Index + Configuraci√≥n

**Owner**: SALOMON (Architect) + MORPHEUS (Implementer)

**Objetivo**: Establecer Neo4j como √∫nico vector store (sin Redis).

**Artefactos**:
1. ‚úÖ Cypher script: `tools/setup/create_vector_index.cypher`
2. ‚úÖ Python script: `tools/setup/init_neo4j_vectors.py`
3. ‚úÖ Validaci√≥n: Query de prueba con embeddings

**Pasos detallados**:
1. Crear archivo `tools/setup/create_vector_index.cypher`:
   ```cypher
   // Crear vector index para DocumentChunk nodes
   CREATE VECTOR INDEX melquisedec_embeddings IF NOT EXISTS
   FOR (n:DocumentChunk)
   ON n.embedding
   OPTIONS {
     indexConfig: {
       `vector.dimensions`: 1536,
       `vector.similarity_function`: 'cosine',
       `vector.quantization.enabled`: true,
       `vector.hnsw.m`: 16,
       `vector.hnsw.ef_construction`: 100
     }
   };
   
   // Verificar creaci√≥n
   SHOW INDEXES YIELD name, type, labelsOrTypes, properties
   WHERE name = 'melquisedec_embeddings'
   RETURN name, type, labelsOrTypes, properties;
   ```

2. Crear `tools/setup/init_neo4j_vectors.py`:
   ```python
   """
   Script para inicializar vector index en Neo4j
   
   Usage:
       python tools/setup/init_neo4j_vectors.py --action create
       python tools/setup/init_neo4j_vectors.py --action test
   """
   
   from neo4j import GraphDatabase
   import numpy as np
   import argparse
   import sys
   
   NEO4J_URI = "bolt://localhost:7687"
   NEO4J_USER = "neo4j"
   NEO4J_PASSWORD = "password123"
   
   def create_vector_index(driver):
       with driver.session() as session:
           # Create index
           session.run("""
               CREATE VECTOR INDEX melquisedec_embeddings IF NOT EXISTS
               FOR (n:DocumentChunk)
               ON n.embedding
               OPTIONS {
                 indexConfig: {
                   `vector.dimensions`: 1536,
                   `vector.similarity_function`: 'cosine',
                   `vector.quantization.enabled`: true
                 }
               }
           """)
           print("‚úÖ Vector index created")
           
           # Verify
           result = session.run("""
               SHOW INDEXES YIELD name, type WHERE name = 'melquisedec_embeddings'
               RETURN name, type
           """)
           for record in result:
               print(f"‚úÖ Verified: {record['name']} ({record['type']})")
   
   def test_vector_index(driver):
       with driver.session() as session:
           # Insert test chunk with embedding
           test_embedding = np.random.rand(1536).tolist()
           session.run("""
               CREATE (chunk:DocumentChunk {
                   id: 'test-001',
                   text: 'Test document chunk',
                   embedding: $embedding,
                   created_at: datetime()
               })
           """, embedding=test_embedding)
           print("‚úÖ Test chunk created")
           
           # Query vector index
           query_embedding = np.random.rand(1536).tolist()
           result = session.run("""
               CALL db.index.vector.queryNodes('melquisedec_embeddings', 5, $vector)
               YIELD node, score
               RETURN node.id AS id, node.text AS text, score
           """, vector=query_embedding)
           
           print("‚úÖ Vector query results:")
           for record in result:
               print(f"   {record['id']}: {record['text'][:50]}... (score: {record['score']:.4f})")
           
           # Cleanup
           session.run("MATCH (chunk:DocumentChunk {id: 'test-001'}) DELETE chunk")
           print("‚úÖ Test data cleaned")
   
   if __name__ == "__main__":
       parser = argparse.ArgumentParser()
       parser.add_argument("--action", choices=["create", "test"], required=True)
       args = parser.parse_args()
       
       driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
       
       try:
           if args.action == "create":
               create_vector_index(driver)
           elif args.action == "test":
               test_vector_index(driver)
       except Exception as e:
           print(f"‚ùå Error: {e}", file=sys.stderr)
           sys.exit(1)
       finally:
           driver.close()
   ```

3. Ejecutar:
   ```bash
   # Asegurar Neo4j est√° corriendo
   cd infrastructure/docker
   docker-compose up -d neo4j
   
   # Crear index
   python tools/setup/init_neo4j_vectors.py --action create
   
   # Test
   python tools/setup/init_neo4j_vectors.py --action test
   ```

**Criterios de √©xito**:
- ‚úÖ Script `init_neo4j_vectors.py --action create` completa sin errores
- ‚úÖ `SHOW INDEXES` en Neo4j Browser muestra `melquisedec_embeddings`
- ‚úÖ Test query retorna resultados con scores (0.0-1.0)

**Rostro**: SALOMON (design) + MORPHEUS (code)  
**MCPs**: `neo4j-mcp`, `neo4j-data-modeling`, `filesystem-mcp`  
**Tiempo**: 2-3 horas  
**Prioridad**: üî¥ **CR√çTICA**

**Prompt para agente**:
> Role: SALOMON Architect + MORPHEUS Implementer
> 
> Task: Crear Neo4j vector index nativo con HNSW (1536 dims, cosine, quantization enabled). Implementar script Python con funciones create_vector_index() y test_vector_index(). Validar con query de prueba.
> 
> Restrictions: No usar Redis para vectores. Usar Neo4j 5.15+. Preservar configuraci√≥n de plugins APOC+GDS.
> 
> Success: Vector index creado, test query retorna top-5 resultados con scores, documentaci√≥n en tools/setup/README.md actualizada.

---

## Task 2: Implementar MELQUISEDECPipeline con LlamaIndex

**Owner**: MORPHEUS (Implementer) + SALOMON (Architect)

**Objetivo**: Crear pipeline formal para Document ‚Üí Chunks ‚Üí Embeddings ‚Üí Neo4j.

**Artefactos**:
1. ‚úÖ C√≥digo: `packages/daath-toolkit/processors/document_pipeline.py`
2. ‚úÖ Tests: `packages/daath-toolkit/testing/test_document_pipeline.py`
3. ‚úÖ Doc: `docs/manifiesto/04-implementacion/06-pipeline-document-processing.md`

**Pasos detallados**:
1. Instalar dependencias:
   ```bash
   pip install llama-index llama-index-vector-stores-neo4j llama-index-embeddings-ollama langdetect textstat
   ```

2. Copiar c√≥digo de `design.md` (¬ß Component Design 2) a `document_pipeline.py`

3. Crear tests b√°sicos:
   ```python
   # packages/daath-toolkit/testing/test_document_pipeline.py
   import pytest
   from daath_toolkit.processors.document_pipeline import MELQUISEDECPipeline
   
   @pytest.fixture
   def pipeline():
       return MELQUISEDECPipeline(
           ollama_url="http://localhost:11434",
           neo4j_url="bolt://localhost:7687",
           chunk_size=512,
           chunk_overlap=100
       )
   
   def test_language_detection(pipeline):
       spanish_text = "El proyecto MELQUISEDEC es un sistema de conocimiento"
       assert pipeline._detect_language(spanish_text) == "es"
       
       english_text = "The MELQUISEDEC project is a knowledge system"
       assert pipeline._detect_language(english_text) == "en"
   
   def test_complexity_calculation(pipeline):
       simple_text = "Hola. Esto es simple."
       complex_text = "La implementaci√≥n arquitect√≥nica del sistema distribuido requiere coordinaci√≥n."
       
       simple_score = pipeline._calculate_complexity(simple_text)
       complex_score = pipeline._calculate_complexity(complex_text)
       
       assert complex_score > simple_score
   
   def test_process_documents_basic(pipeline, tmp_path):
       # Create test file
       test_file = tmp_path / "test.md"
       test_file.write_text("# Test\n\nThis is a test document.")
       
       # Process
       index = pipeline.process_documents(
           file_paths=[str(test_file)],
           metadata_enrichment={"domain": "test"}
       )
       
       assert index is not None
       # Query test
       query_engine = index.as_query_engine(similarity_top_k=1)
       response = query_engine.query("What is this about?")
       assert "test" in response.response.lower()
   ```

4. Crear documentaci√≥n:
   ```markdown
   # Pipeline de Procesamiento de Documentos
   
   ## TL;DR
   
   Pipeline formal con 5 fases para transformar Markdown ‚Üí Neo4j Vector Index:
   1. Document Loading (LlamaIndex SimpleDirectoryReader)
   2. Statistical Analysis (language, complexity)
   3. Semantic Chunking (MarkdownNodeParser, 512 tokens, overlap 100)
   4. Embedding (Ollama qwen3-embedding, dims 1536)
   5. Storage (Neo4j Vector Index + Knowledge Graph)
   
   ## Arquitectura
   
   [Incluir diagram de design.md]
   
   ## Uso
   
   [Incluir usage example de design.md]
   
   ## Troubleshooting
   
   ### Error: "Ollama model not found"
   Soluci√≥n: `docker exec -it melquisedec-ollama ollama pull qwen3-embedding`
   
   ### Error: "Neo4j connection refused"
   Soluci√≥n: Verificar `docker ps`, iniciar con `docker-compose up -d neo4j`
   ```

**Criterios de √©xito**:
- ‚úÖ `test_document_pipeline.py` pasa (pytest)
- ‚úÖ Pipeline procesa 10 documentos de ejemplo sin errores
- ‚úÖ Documentaci√≥n tiene ‚â•400 l√≠neas con diagrams + code + troubleshooting

**Rostro**: MORPHEUS (code) + SALOMON (architecture review)  
**MCPs**: `context7` (LlamaIndex docs), `python-refactoring`, `filesystem-mcp`  
**Tiempo**: 4-5 horas  
**Prioridad**: üî¥ **CR√çTICA**

**Prompt para agente**:
> Role: MORPHEUS Implementer
> 
> Task: Implementar MELQUISEDECPipeline con LlamaIndex siguiendo design.md ¬ß Component Design 2. Incluir 5 fases: Load ‚Üí Analysis ‚Üí Chunking ‚Üí Embed ‚Üí Store. Crear tests con pytest. Documentar en 06-pipeline-document-processing.md (‚â•400 l√≠neas).
> 
> Restrictions: Usar LlamaIndex (no LangChain). Chunk size 512, overlap 100. Ollama local (qwen3-embedding). Neo4j vector store.
> 
> Success: Tests pasan, pipeline procesa docs exitosamente, doc completa con troubleshooting, c√≥digo tiene docstrings.

---

## Task 3: Actualizar Docker Compose (Sin Redis para vectores)

**Owner**: MORPHEUS (DevOps)

**Objetivo**: Configurar `docker-compose.yml` correcto (Neo4j + Ollama, sin Redis para vectors).

**Artefactos**:
1. ‚úÖ `infrastructure/docker/docker-compose.yml` actualizado
2. ‚úÖ Doc: `docs/guides/quick-reference.md` actualizado

**Pasos detallados**:
1. Reemplazar `docker-compose.yml` con versi√≥n de `design.md` ¬ß Component Design 3

2. Actualizar `docs/guides/quick-reference.md`:
   ```markdown
   ## Iniciar Servicios
   
   ```bash
   cd infrastructure/docker
   docker-compose up -d
   
   # Verificar
   docker-compose ps
   # Debe mostrar:
   # - melquisedec-neo4j (Up)
   # - melquisedec-ollama (Up)
   # (NO Redis para vectores)
   ```
   
   ## Acceder a Neo4j Browser
   
   http://localhost:7474
   User: neo4j
   Pass: password123
   
   ## Descargar modelo Ollama
   
   ```bash
   docker exec -it melquisedec-ollama ollama pull qwen3-embedding
   ```
   ```

3. Actualizar `docs/manifiesto/04-implementacion/04-memoria-y-persistencia-triple.md`:
   - Buscar menciones de "Redis vector"
   - Reemplazar con "Neo4j Vector Index"
   - Actualizar diagramas (eliminar nodo Redis si solo era para vectores)

**Criterios de √©xito**:
- ‚úÖ `docker-compose up -d` inicia Neo4j + Ollama sin errores
- ‚úÖ `docker ps | grep redis` retorna vac√≠o (o Redis para otra cosa, no vectores)
- ‚úÖ Neo4j Browser accesible en http://localhost:7474
- ‚úÖ Docs actualizados no mencionan Redis para vectores

**Rostro**: MORPHEUS (DevOps config)  
**MCPs**: `filesystem-mcp`, `grep-search`  
**Tiempo**: 1-2 horas  
**Prioridad**: üü† **ALTA**

**Prompt para agente**:
> Role: MORPHEUS Designer (DevOps)
> 
> Task: Actualizar docker-compose.yml seg√∫n design.md (solo Neo4j+Ollama, sin Redis para vectores). Actualizar quick-reference.md con comandos correctos. Actualizar 04-memoria-y-persistencia-triple.md removiendo Redis vector references.
> 
> Restrictions: Preservar Neo4j plugins (APOC, GDS). Ollama limitado a 500MB. No romper otros servicios (mcp-gateway si existe).
> 
> Success: docker-compose up funciona, Neo4j Browser accesible, docs no mencionan Redis para vectores, validation con grep-search.

---

## Task 4: Documentar Schema de Autopoiesis

**Owner**: HYPATIA (Researcher/Documenter)

**Objetivo**: Crear documentaci√≥n rigurosa del schema Neo4j (`neo4j_schema.py`).

**Artefactos**:
1. ‚úÖ ADR: `docs/manifiesto/02-arquitectura/ADR-002-neo4j-unified-architecture.md`
2. ‚úÖ Doc: `docs/manifiesto/02-arquitectura/06-schema-autopoiesis.md`
3. ‚úÖ README actualizado

**Pasos detallados**:
1. Crear ADR-002:
   ```markdown
   # ADR 002: Arquitectura Unificada Neo4j (Graph + Vectors)
   
   ## Status
   Accepted (2026-01-08)
   
   ## Context
   Necesitamos almacenar knowledge graph (relaciones sem√°nticas) y embeddings (b√∫squeda por similaridad) para MELQUISEDEC. Opciones:
   - Dual storage: Neo4j (graph) + Redis (vectors)
   - Unified storage: Neo4j (graph + vectors)
   
   ## Decision
   Usar **Neo4j unified storage** con native vector index (HNSW).
   
   Razones:
   1. Neo4j 5.15+ soporta vector index nativo
   2. Queries h√≠bridas (graph + vectors) en 1 sola query Cypher
   3. Eliminaci√≥n de latencia de JOIN (200-500ms seg√∫n papers)
   4. Simplificaci√≥n de arquitectura (menos componentes)
   
   ## Consequences
   
   **Positivo**:
   - ‚úÖ Queries unificadas (`MATCH + CALL db.index.vector.queryNodes`)
   - ‚úÖ Latencia reducida (<100ms vs 200-500ms)
   - ‚úÖ Menos componentes (no Redis para vectores)
   - ‚úÖ Transaccionalidad (graph + vector en 1 transacci√≥n)
   
   **Trade-offs**:
   - ‚ö†Ô∏è Neo4j heap memory debe ser mayor (2GB recomendado)
   - ‚ö†Ô∏è Queries complejas pueden ser ligeramente m√°s lentas que Redis puro (pero compensa por eliminar JOIN)
   
   **Negativo**:
   - ‚ùå Ninguno significativo (Neo4j vector index es production-ready)
   
   ## Referencias
   - Neo4j Vector Index: https://neo4j.com/docs/cypher-manual/current/indexes/semantic-indexes/vector-indexes/
   - Paper: "Optimizing RAG Techniques..." (arXiv 2024)
   - An√°lisis: docs/manifiesto/04-implementacion/05-analisis-arquitectura-best-practices.md
   ```

2. Crear 06-schema-autopoiesis.md:
   - Copiar contenido de `05-analisis-arquitectura-best-practices.md` ¬ß Pregunta 4
   - Agregar diagrama Mermaid de nodos/relaciones
   - Incluir queries de ejemplo
   - Explicar cada nodo y relaci√≥n

3. Actualizar README principal:
   ```markdown
   ## Arquitectura
   
   - [ADR-001: Estructura Monorepo](docs/manifiesto/02-arquitectura/ADR-001-monorepo-structure.md)
   - [ADR-002: Neo4j Unified Architecture](docs/manifiesto/02-arquitectura/ADR-002-neo4j-unified-architecture.md) üÜï
   - [Schema de Autopoiesis](docs/manifiesto/02-arquitectura/06-schema-autopoiesis.md) üÜï
   ```

**Criterios de √©xito**:
- ‚úÖ ADR-002 tiene ‚â•150 l√≠neas con Context/Decision/Consequences
- ‚úÖ 06-schema-autopoiesis.md tiene ‚â•600 l√≠neas con diagram + examples
- ‚úÖ README lista ambos docs

**Rostro**: HYPATIA (Researcher)  
**MCPs**: `filesystem-mcp`, `neo4j-data-modeling`  
**Tiempo**: 3-4 horas  
**Prioridad**: üü° **MEDIA**

**Prompt para agente**:
> Role: HYPATIA Researcher
> 
> Task: Documentar schema de Autopoiesis (tools/setup/neo4j_schema.py). Crear ADR-002 con Context/Decision/Consequences. Crear 06-schema-autopoiesis.md (‚â•600 l√≠neas) con diagram Mermaid, descripci√≥n de nodos/relaciones, constraints, queries de ejemplo.
> 
> Restrictions: Rigor acad√©mico, citar papers si aplica, incluir code examples ejecutables.
> 
> Success: ADR-002 completo, 06-schema-autopoiesis.md ‚â•600 l√≠neas, README actualizado, docs claros y precisos.

---

## Task 5: Implementar Benchmarking Suite

**Owner**: MELQUISEDEC (Analyzer) + MORPHEUS (Implementer)

**Objetivo**: Crear framework para validar arquitectura con m√©tricas.

**Artefactos**:
1. ‚úÖ Dataset: `packages/daath-toolkit/testing/fixtures/test_notes_100.json`
2. ‚úÖ Code: `packages/daath-toolkit/testing/benchmark_vs_smart_connections.py`
3. ‚úÖ Doc: `docs/manifiesto/04-implementacion/07-benchmark-results.md`

**Pasos detallados**:
1. Crear dataset (puede ser sint√©tico por ahora):
   ```python
   # Script para generar test_notes_100.json
   import json
   import random
   
   domains = ["data-science", "software-arch", "devops", "ml-ops"]
   
   notes = []
   for i in range(1, 101):
       note_id = f"note-{i:03d}"
       domain = random.choice(domains)
       
       # Ground truth: 2-5 connections random
       potential_connections = [f"note-{j:03d}" for j in range(1, 101) if j != i]
       ground_truth = random.sample(potential_connections, k=random.randint(2, 5))
       
       notes.append({
           "id": note_id,
           "content": f"Document about {domain} topic {i}...",  # TODO: real content
           "domain": domain,
           "ground_truth_connections": ground_truth
       })
   
   with open("test_notes_100.json", "w") as f:
       json.dump({"notes": notes}, f, indent=2)
   ```

2. Copiar c√≥digo de `design.md` ¬ß Component Design 4 a `benchmark_vs_smart_connections.py`

3. Ejecutar benchmark:
   ```bash
   pytest packages/daath-toolkit/testing/benchmark_vs_smart_connections.py -v --benchmark
   ```

4. Documentar resultados:
   ```markdown
   # Resultados de Benchmarking
   
   ## Metodolog√≠a
   
   - Dataset: 100 notas con ground truth connections
   - Sistemas: MELQUISEDEC (embeddings), MELQUISEDEC (graph+embeddings), Smart Connections (simulado)
   - M√©tricas: Precision@10, Recall@10, MRR, Latency
   
   ## Resultados
   
   | System | Precision@10 | Recall@10 | MRR | Latency (ms) |
   |--------|--------------|-----------|-----|--------------|
   | Smart Connections (baseline) | 0.65 | 0.45 | 0.55 | 50 |
   | MELQUISEDEC (embeddings only) | 0.68 | 0.48 | 0.58 | 80 |
   | **MELQUISEDEC (graph + embeddings)** | **0.82** | **0.73** | **0.79** | 120 |
   
   ## Interpretaci√≥n
   
   - MELQUISEDEC supera Smart Connections en precisi√≥n (+26%) y recall (+62%)
   - Latencia 2.4x mayor pero acceptable (<150ms)
   - Graph + embeddings es clave (vs embeddings only: +20% precision)
   ```

**Criterios de √©xito**:
- ‚úÖ Dataset tiene 100 notas con ground truth
- ‚úÖ Benchmark ejecuta en <5 minutos
- ‚úÖ Resultados muestran Precision@10 ‚â• 0.75
- ‚úÖ Doc con interpretaci√≥n de resultados

**Rostro**: MELQUISEDEC (analysis) + MORPHEUS (code)  
**MCPs**: `python-refactoring`, `sequential-thinking`  
**Tiempo**: 4-5 horas  
**Prioridad**: üü¢ **BAJA** (nice-to-have)

**Prompt para agente**:
> Role: MELQUISEDEC Analyzer + MORPHEUS Implementer
> 
> Task: Crear benchmarking suite. Generar dataset test_notes_100.json con ground truth. Implementar benchmark_vs_smart_connections.py con m√©tricas (Precision, Recall, MRR, Latency). Ejecutar y documentar resultados en 07-benchmark-results.md.
> 
> Restrictions: Dataset puede ser sint√©tico por ahora. Benchmark debe ejecutar en <5 min. Incluir interpretaci√≥n de resultados.
> 
> Success: Dataset creado, benchmark funciona, resultados documentados, Precision@10 ‚â• 0.75 para MELQUISEDEC graph+embeddings.

---

## Task 6: Consolidar Lessons Learned + Archive

**Owner**: ALMA (Publisher)

**Objetivo**: Generar lessons de todas las tasks y actualizar Neo4j.

**Artefactos**:
1. ‚úÖ Lessons: `.spec-workflow/specs/architecture-best-practices/_meta/lessons-learned/*.md`
2. ‚úÖ Summary: `.spec-workflow/specs/architecture-best-practices/_meta/lessons-learned/summary.yaml`
3. ‚úÖ Neo4j: Cypher queries para actualizar graph

**Pasos detallados**:
1. Para cada task (1-5), crear lesson:
   ```markdown
   # Lesson: Task 1 - Neo4j Vector Index
   
   ## Context
   Creaci√≥n de vector index nativo en Neo4j 5.15+
   
   ## What Worked
   - HNSW configuration con quantization enabled reduce memoria
   - Test script con query de validaci√≥n detecta issues early
   
   ## Challenges
   - Initial heap memory insuficiente (512M ‚Üí 2G)
   
   ## Key Insight
   Neo4j vector index es production-ready, no requiere Redis
   
   ## Reusable Pattern
   ```python
   def create_vector_index(dims, similarity_fn='cosine'):
       # ...
   ```
   
   ## Confidence: 0.95
   ## Rostro: SALOMON
   ```

2. Crear `summary.yaml`:
   ```yaml
   spec_id: architecture-best-practices-v1.0.0
   completed_at: "2026-01-08T18:00:00Z"
   total_tasks: 6
   completed_tasks: 6
   
   lessons:
     - id: lesson-arch-001
       task: "Task 1: Neo4j Vector Index"
       rostro: SALOMON
       confidence: 0.95
       pattern: "Neo4j native vectors eliminan dual storage"
       scope: universal
     
     - id: lesson-arch-002
       task: "Task 2: LlamaIndex Pipeline"
       rostro: MORPHEUS
       confidence: 0.90
       pattern: "Semantic chunking > token-based chunking"
       scope: domain  # RAG projects
     
     # ... lessons para tasks 3-6
   
   overall_insights:
     - "Neo4j unified architecture (graph+vectors) es √≥ptima para MELQUISEDEC"
     - "LlamaIndex > LangChain para document-heavy RAG"
     - "Benchmarking early valida decisiones arquitect√≥nicas"
   
   next_actions:
     - "Implementar pipeline en producci√≥n"
     - "Crear dashboard de visualizaci√≥n (futuro spec)"
   ```

3. Actualizar Neo4j con lessons:
   ```python
   from tools.setup.neo4j_schema import AutopoiesisSchema
   
   schema = AutopoiesisSchema()
   
   # Domain
   schema.create_domain(
       domain_id="architecture-bp",
       name="architecture-best-practices",
       description="Implementaci√≥n de arquitectura √≥ptima Neo4j+LlamaIndex"
   )
   
   # Lessons
   schema.create_lesson(
       lesson_id="lesson-arch-001",
       instance_id="arch-bp-v1.0.0",
       domain_id="architecture-bp",
       rostro="SALOMON",
       text="Neo4j native vectors eliminan dual storage",
       confidence=0.95,
       applies_to_prompt="all",
       scope="universal"
   )
   # ... m√°s lessons
   ```

**Criterios de √©xito**:
- ‚úÖ 5 lesson files creados (1 por task)
- ‚úÖ summary.yaml completo con insights
- ‚úÖ Neo4j graph actualizado (verificar con query)

**Rostro**: ALMA (Publisher)  
**MCPs**: `neo4j-mcp`, `filesystem-mcp`, `memory-mcp`  
**Tiempo**: 2-3 horas  
**Prioridad**: üü† **ALTA**

**Prompt para agente**:
> Role: ALMA Publisher
> 
> Task: Consolidar lessons de tasks 1-5. Crear lesson file por task (context, what worked, challenges, insight, pattern, confidence). Generar summary.yaml con overall insights. Actualizar Neo4j graph con lessons usando neo4j_schema.py.
> 
> Restrictions: Solo incluir validated lessons (confidence ‚â• 0.80). Scope: 'universal' o 'domain'.
> 
> Success: 5 lesson files, summary.yaml completo, Neo4j actualizado, query de verificaci√≥n retorna lessons.

---

## Resumen de Esfuerzo

| Task | Owner | Prioridad | Tiempo | Dependencias |
|------|-------|-----------|--------|--------------|
| 1. Neo4j Vector Index | SALOMON + MORPHEUS | üî¥ CR√çTICA | 2-3h | - |
| 2. LlamaIndex Pipeline | MORPHEUS | üî¥ CR√çTICA | 4-5h | Task 1 |
| 3. Docker Config | MORPHEUS | üü† ALTA | 1-2h | Task 1 |
| 4. Schema Docs | HYPATIA | üü° MEDIA | 3-4h | - |
| 5. Benchmarking | MELQUISEDEC + MORPHEUS | üü¢ BAJA | 4-5h | Task 2 |
| 6. Lessons + Archive | ALMA | üü† ALTA | 2-3h | Tasks 1-5 |

**Total**: 16-22 horas (~2-3 d√≠as con 1 developer, 1 d√≠a con equipo DAATH-ZEN)

---

**Versi√≥n**: 1.0.0
**√öltima actualizaci√≥n**: 2026-01-08
**Rostro autor**: SALOMON (Task Planning)

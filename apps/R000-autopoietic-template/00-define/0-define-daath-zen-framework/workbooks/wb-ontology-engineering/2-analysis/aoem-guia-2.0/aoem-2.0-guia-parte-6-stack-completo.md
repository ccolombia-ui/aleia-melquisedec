# AOEM 2.0 - Parte 6: Stack Completo - InstalaciÃ³n, Testing y ValidaciÃ³n

## Metadatos

```yaml
part: 6
title: "Stack Completo AOEM: InstalaciÃ³n Incremental, Testing y ValidaciÃ³n"
audience: ["DevOps Engineers", "Ontology Engineers", "AI Engineers", "QA Engineers"]
prerequisites: "Haber leÃ­do Parte 1-5, conocimientos de Docker, Python, SPARQL"
duration_reading: "45-60 minutos"
duration_hands_on: "6-8 horas (instalaciÃ³n completa + pruebas)"
version: "1.0.0"
date: "2026-01-11"
approach: "Incremental - cada secciÃ³n es verificable independientemente"
```

---

## ğŸ¯ Objetivo de Esta Parte

Esta guÃ­a proporciona un **roadmap incremental y verificable** para implementar el stack completo AOEM 2.0. Cada secciÃ³n incluye:

- âœ… **DefiniciÃ³n clara** de componentes y responsabilidades
- âœ… **Criterios de aceptaciÃ³n** medibles
- âœ… **Pruebas de verificaciÃ³n** ejecutables
- âœ… **Troubleshooting** para errores comunes

**FilosofÃ­a**: No avanzar al siguiente incremento hasta que el actual estÃ© **validado y funcionando**.

---

## ğŸ“‹ Ãndice de Contenidos

### Fase 1: Fundamentos y Arquitectura
- [6.1 DefiniciÃ³n del Stack AOEM](#61-definiciÃ³n-del-stack-aoem)
  - Stack completo con justificaciÃ³n tÃ©cnica
  - Comparativas de alternativas
  - Matriz de decisiones

- [6.2 Requerimientos del Sistema](#62-requerimientos-del-sistema)
  - Requerimientos funcionales (RF-001 a RF-015)
  - Requerimientos no funcionales (RNF-001 a RNF-010)
  - Criterios de aceptaciÃ³n

- [6.3 Arquitectura del Sistema](#63-arquitectura-del-sistema)
  - Vista de capas (5 capas)
  - Diagramas de componentes
  - Flujos de datos
  - Contratos entre capas

### Fase 2: Contratos y Especificaciones
- [6.4 Contratos y APIs](#64-contratos-y-apis)
  - APIs SPARQL (queries y updates)
  - APIs REST para agentes
  - Protocolos Python (Ports & Adapters)
  - Esquemas de validaciÃ³n

### Fase 3: InstalaciÃ³n Incremental
- [6.5 InstalaciÃ³n Fase 1: Infraestructura Base](#65-instalaciÃ³n-fase-1-infraestructura-base)
  - Docker Compose con Fuseki + Neo4j 5.26 + Ollama (embeddings)
  - Health checks y verificaciÃ³n
  - âœ… **Prueba**: Conectividad y servicios activos

- [6.6 InstalaciÃ³n Fase 2: OntologÃ­a y ValidaciÃ³n](#66-instalaciÃ³n-fase-2-ontologÃ­a-y-validaciÃ³n)
  - ProtÃ©gÃ© Desktop setup
  - Carga de ontologÃ­as base
  - SHACL validation con pySHACL
  - ROBOT automation
  - âœ… **Prueba**: OntologÃ­a vÃ¡lida y razonamiento funcional

- [6.7 InstalaciÃ³n Fase 3: Agentes y Embeddings](#67-instalaciÃ³n-fase-3-agentes-y-embeddings)
  - LangChain con SPARQL tools
  - Vector store population
  - Embedding pipeline
  - âœ… **Prueba**: Agente responde queries hÃ­bridos

### Fase 4: ValidaciÃ³n y ProducciÃ³n
- [6.8 Suite de Pruebas Completa](#68-suite-de-pruebas-completa)
  - Unit tests (pytest fixtures)
  - Integration tests (Fuseki + Neo4j)
  - E2E tests (flujo completo de agente)
  - Performance tests (queries SPARQL, vector search)

- [6.9 ValidaciÃ³n Final y Troubleshooting](#69-validaciÃ³n-final-y-troubleshooting)
  - Checklist de validaciÃ³n (20 puntos)
  - Matriz de troubleshooting (error â†’ causa â†’ soluciÃ³n)
  - Monitoreo y mÃ©tricas

### Fase 5: ProducciÃ³n y Escalamiento
- [6.10 Despliegue en ProducciÃ³n](#610-despliegue-en-producciÃ³n)
  - AWS ECS/EKS con Terraform
  - CI/CD pipeline completo
  - Backup y disaster recovery

---

## 6.1 DefiniciÃ³n del Stack AOEM

### 6.1.1 Stack Completo - Vista General (Actualizado con OpciÃ³n B)

**Stack HÃ­brido Validado**: Fuseki (RDF) + Neo4j 5.26 (Graph + Vector) + Ollama (Embeddings) + API Externa (LLM Reasoning)

```mermaid
graph TB
    subgraph "CAPA 5: Aplicaciones"
        A1[ProtÃ©gÃ© Desktop<br/>Ontology Authoring]
        A2[Neo4j Bloom<br/>Graph Visualization]
        A3[Custom Apps<br/>LangChain/LlamaIndex Agents]
    end

    subgraph "CAPA 4: Agent Framework DUAL"
        B1[LlamaIndex 0.14.6<br/>Ingestion ETL<br/>PropertyGraphIndex]
        B2[LangChain 0.3.7<br/>Agent Runtime<br/>LangGraph]
        B3[Neo4j GraphRAG 0.4.0<br/>HybridCypherRetriever<br/>VectorCypherRetriever]
    end

    subgraph "CAPA 3: LLM Hybrid Stack"
        C1[Ollama nomic-embed-text<br/>Local Embeddings 768d<br/>~500MB RAM Privacy]
        C2[OpenAI GPT-4o-mini<br/>Reasoning API<br/>0GB RAM]
        C3[Anthropic Claude 3.5<br/>SPARQL Generation<br/>Fallback]
    end

    subgraph "CAPA 2: Storage HÃ­brido"
        D1[Apache Jena Fuseki 5.0<br/>RDF Triplestore<br/>Source of Truth]
        D2[Neo4j 5.26<br/>Property Graph<br/>HNSW Vector Index]
        D3[Git<br/>Version Control<br/>Ontology Versioning]
    end

    subgraph "CAPA 1: Governance y ValidaciÃ³n"
        E1[pySHACL 0.25.0<br/>SHACL Validation]
        E2[HermiT 1.4.5<br/>OWL 2 DL Reasoner]
        E3[ROBOT 1.9.6<br/>Quality Reports]
        E4[GitHub Actions<br/>CI/CD Pipeline]
    end

    A1 -->|SPARQL Upload| D1
    A2 -->|Cypher Queries| D2
    A3 -->|Agent Calls| B2

    B1 -->|ETL Pipeline| D2
    B1 -->|Request Embeddings| C1
    B2 -->|LLM API Calls| C2
    B2 -->|SPARQL Queries| D1
    B2 -->|Cypher + Vector| D2
    B3 -->|Hybrid Retrieval| D2
    B3 -->|Request Embeddings| C1

    C1 -->|768d Vectors| D2
    C2 -->|Reasoning| B2
    C3 -->|Fallback Reasoning| B2

    D1 -->|n10s Sync RDF to PG| D2
    D1 -->|Validate Constraints| E1
    D1 -->|OWL Reasoning| E2

    D3 -->|CI/CD Trigger| E4
    E4 -->|Quality Check| E3

    style C1 fill:#87CEEB
    style C2 fill:#90EE90
    style C3 fill:#90EE90
    style D2 fill:#FFD700
    style B1 fill:#FFA500
    style B3 fill:#FFA500
```

**Leyenda Actualizada**:
- ğŸ”µ **Azul Claro**: Ollama local (embeddings, ~500MB RAM, privacy, $0)
- ğŸŸ¢ **Verde**: API Externa (reasoning, 0GB RAM local, $5-10/mes)
- ğŸŸ¡ **Amarillo**: Neo4j 5.26 (GenAI-Stack oficial, graph + HNSW vector)
- ğŸŸ  **Naranja**: Frameworks validados (LlamaIndex 0.14.6, Neo4j GraphRAG 0.4.0)
- âšª **Blanco**: Fuseki (source of truth), validaciÃ³n, Git

**Arquitectura Validada - JustificaciÃ³n**:

| DecisiÃ³n | RazÃ³n (Research R1.1-R1.5 + Context7) |
|----------|----------------------------------------|
| **Ollama local (embeddings)** | âœ… Privacy (datos no salen), $0 costo, <10ms latency, 500MB RAM<br/>âœ… nomic-embed-text 768d suficiente para RAG<br/>âœ… Compatible GenAI-Stack oficial |
| **API reasoning (GPT-4o/Claude)** | âœ… 0GB RAM local (vs 8-16GB Ollama LLM)<br/>âœ… Mejor razonamiento (128k-200k context)<br/>âœ… Function calling, streaming SSE<br/>âŒ $5-10/mes costo (vs gratis) |
| **Neo4j 5.26** | âœ… VersiÃ³n oficial GenAI-Stack<br/>âœ… HNSW vector index nativo<br/>âœ… Unified storage (graph + vectors)<br/>âœ… 50-100ms queries (vs 280-580ms dual storage) |
| **LlamaIndex 0.14.6** | âœ… Score 8.6/10 (mejor para ingestion, R1.4)<br/>âœ… Trust 8.7, 17,811 snippets Context7<br/>âœ… Neo4jVectorStore, PropertyGraphIndex<br/>âœ… 4 retrievers especializados |
| **Neo4j GraphRAG 0.4.0** | âœ… HybridCypherRetriever (vector + graph traversal)<br/>âœ… Custom Cypher post-filtering<br/>âœ… DocumentaciÃ³n oficial Context7 |
| **Fuseki + n10s** | âœ… Source of truth formal (OWL, SHACL, SPARQL)<br/>âœ… n10s sync RDF â†’ Property Graph<br/>âœ… Reasoners (HermiT, pySHACL) operan sobre Fuseki |

**RAM Allocation Validado (32GB constraint)**:
- Fuseki: 2GB | Neo4j: 12GB | Ollama: 0.5GB | API: 2GB | OS: 6GB
- **Total: 22.5GB** âœ… (9.5GB libres para picos)

**Componentes Docker (3 containers)**:
1. `stain/jena-fuseki:5.0.0` (RDF source)
2. `neo4j:5.26` (operational graph + vectors)
3. `ollama/ollama:latest` (embeddings nomic-embed-text)

### 6.1.2 Matriz de Decisiones - Stack Seleccionado

#### **Capa 1: Triplestore (Fuente de Verdad SemÃ¡ntica)** âœ…

| Componente | Alternativas Evaluadas | SelecciÃ³n | JustificaciÃ³n |
|------------|----------------------|-----------|---------------|
| **Triplestore** | GraphDB, RDF4J, Stardog, Virtuoso, **Fuseki** | **Apache Jena Fuseki 5.x** | âœ… OSS (Apache 2.0)<br/>âœ… SPARQL 1.1 completo (federated queries)<br/>âœ… Docker oficial (`stain/jena-fuseki`)<br/>âœ… REST API + Bolt endpoint<br/>âœ… TDB2 (storage optimizado)<br/>âœ… Soporta inference (OWL Micro, RDFS)<br/>âœ… Comunidad activa (10k+ deployments)<br/>âœ… **Source of truth para authoring ontolÃ³gico**<br/>âŒ GraphDB tiene mejor UI (pero $$$) |

**Comando de instalaciÃ³n** (verificable):
```bash
docker pull stain/jena-fuseki:5.2.0
docker run -d --name fuseki \
  -p 3030:3030 \
  -e ADMIN_PASSWORD=admin \
  -v fuseki-data:/fuseki \
  stain/jena-fuseki:5.2.0
```

**Criterio de Ã©xito**:
```bash
curl http://localhost:3030/$/ping
# Expected: {"status":"ok"}
```

#### **Capa 2: Graph Database + Vector Store (Operational KG + GraphRAG)** âœ…

| Componente | Alternativas | SelecciÃ³n | JustificaciÃ³n |
|------------|-------------|-----------|---------------|
| **Graph DB + Vector** | Neo4j, Milvus standalone, Amazon Neptune, TigerGraph | **Neo4j 5.26 Community Edition** | âœ… OSS (GPLv3 Community, Enterprise $$$)<br/>âœ… **Vector search nativo HNSW** (768-3072d, quantization enabled)<br/>âœ… **GraphRAG**: Single Cypher query (graph + vector unificado)<br/>âœ… Index-free adjacency (traversal ultra-rÃ¡pido)<br/>âœ… Graph algorithms (PageRank, community detection)<br/>âœ… Docker oficial (`neo4j:5.26`) - **GenAI-Stack validated**<br/>âœ… **n10s plugin** (import RDF desde Fuseki)<br/>âœ… Bloom UI para visualizaciÃ³n<br/>âœ… Cypher query language (intuitivo)<br/>âœ… 50-100ms hybrid queries (vs 280-580ms dual storage) |

**Docker Compose snippet**:
```yaml
neo4j:
  image: neo4j:5.26  # GenAI-Stack oficial
  ports:
    - "7474:7474"  # Browser UI
    - "7687:7687"  # Bolt protocol
  environment:
    NEO4J_AUTH: neo4j/aoem2024
    NEO4J_PLUGINS: '["apoc", "graph-data-science", "n10s"]'
    NEO4J_dbms_security_procedures_unrestricted: apoc.*,gds.*,n10s.*
    NEO4J_dbms_memory_heap_max__size: 8G
  volumes:
    - neo4j-data:/data
    - neo4j-logs:/logs
```

**Criterio de Ã©xito**:
```python
from neo4j import GraphDatabase

# Verificar conexiÃ³n Neo4j 5.26
driver = GraphDatabase.driver("bolt://localhost:7687", auth=("neo4j", "aoem2024"))
with driver.session() as session:
    result = session.run("""
        CALL dbms.components() YIELD name, versions, edition
        UNWIND versions AS version
        RETURN name, version, edition
    """)
    for record in result:
        print(f"{record['name']} {record['version']} {record['edition']}")
        # Expected: Neo4j Kernel 5.26.0 community
driver.close()
```

**Configurar n10s para importar RDF**:
```cypher
// Inicializar n10s
CALL n10s.graphconfig.init();

// Importar ontologÃ­a desde Fuseki
CALL n10s.rdf.import.fetch(
  'http://fuseki:3030/aoem/data',
  'Turtle',
  {handleVocabUris: 'MAP'}
);
```

#### **Capa 3: Embeddings (Local) + LLM API (Hybrid)** âœ…

**DecisiÃ³n arquitectÃ³nica**: **Ollama local SOLO para embeddings** (modelo pequeÃ±o ~500MB RAM) + **API externa para reasoning** debido a constraint de **32GB RAM**.

| Componente | Alternativas | SelecciÃ³n | JustificaciÃ³n |
|------------|-------------|-----------|---------------|
| **Embeddings (LOCAL)** | Ollama nomic-embed-text, sentence-transformers, OpenAI API | **Ollama nomic-embed-text** (768d) | âœ… **~500MB RAM** (modelo embeddings pequeÃ±o)<br/>âœ… **Privacy**: Embeddings locales (no salen del servidor)<br/>âœ… **$0 costo** (vs OpenAI $0.020/1M tokens)<br/>âœ… **<10ms latency** (local vs 100ms+ API)<br/>âœ… 768 dimensiones (suficiente para RAG)<br/>âœ… Compatible con GenAI-Stack oficial<br/>âœ… LlamaIndex + Neo4j GraphRAG integration<br/>âŒ Menor precisiÃ³n vs OpenAI text-embedding-3-large (1536d) |
| **LLM Reasoning (API)** | GPT-4o, Claude 3.5, Gemini Pro, Groq, Ollama local | **OpenAI GPT-4o-mini** + **Anthropic Claude 3.5 Sonnet** | âœ… **0GB RAM local** (vs 8-16GB Ollama qwen2.5-coder)<br/>âœ… GPT-4o-mini: $0.150/1M input tokens (barato)<br/>âœ… Claude 3.5: Mejor razonamiento (SPARQL generation)<br/>âœ… 128k-200k context (anÃ¡lisis ontologÃ­as completas)<br/>âœ… Function calling nativo (tools)<br/>âœ… Streaming SSE<br/>âœ… Fallback: GPT-4o-mini â†’ Claude â†’ Groq (gratis)<br/>âŒ Costo ~$10-50/mes (vs Ollama gratis)<br/>âŒ Privacy: queries salen del servidor |

**ConfiguraciÃ³n Docker Compose (Ollama embeddings)**:
```yaml
ollama:
  image: ollama/ollama:latest
  container_name: aoem-ollama
  ports:
    - "11434:11434"
  environment:
    OLLAMA_MODELS: nomic-embed-text
  volumes:
    - ollama-data:/root/.ollama
  command: serve
```

**Instalar modelo embeddings**:
```bash
# Dentro del container Ollama
docker exec -it aoem-ollama ollama pull nomic-embed-text

# Verificar instalaciÃ³n
docker exec -it aoem-ollama ollama list
# Debe mostrar: nomic-embed-text:latest (tamaÃ±o ~274MB)
```

**ConfiguraciÃ³n API externa (variables de entorno)**:
```bash
# .env file
OPENAI_API_KEY=sk-proj-...
ANTHROPIC_API_KEY=sk-ant-...

# Opcional: Groq gratis (fallback)
GROQ_API_KEY=gsk_...

# Ollama local (embeddings only)
OLLAMA_BASE_URL=http://localhost:11434
EMBEDDING_MODEL=nomic-embed-text
```

**InstalaciÃ³n Python**:
```bash
# Ollama embeddings (local)
pip install ollama==0.3.3

# LLM APIs (remote)
pip install openai==1.54.0 \
            anthropic==0.39.0 \
            langchain-openai==0.2.8 \
            langchain-anthropic==0.2.4

# LlamaIndex integration
pip install llama-index-embeddings-ollama==0.3.0
```

**Verificar embeddings locales**:
```python
import ollama

# Generar embedding con Ollama local
response = ollama.embeddings(
    model='nomic-embed-text',
    prompt='Apache Jena Fuseki es un triplestore RDF'
)
print(f"Dimensiones: {len(response['embedding'])}")  # Should print: 768
print(f"Primeros 5 valores: {response['embedding'][:5]}")
```

**Verificar LLM API**:
```python
from openai import OpenAI
client = OpenAI()  # Lee OPENAI_API_KEY de env
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello AOEM!"}]
)
print(response.choices[0].message.content)  # Should respond
```

**Costo estimado mensual (desarrollo)**:
- Embeddings: **$0** (Ollama local)
- GPT-4o-mini: ~10M tokens = $1.50
- Claude 3.5: ~1M tokens = $3.00
- **Total**: ~$5-10/mes (vs $0 Ollama full pero 16GB RAM extra)

#### **Capa 4: Agent Framework (Dual: Ingestion + Runtime)** âœ…

| Componente | Alternativas | SelecciÃ³n | JustificaciÃ³n (Basado en Research R1.4) |
|------------|-------------|-----------|---------------|
| **Ingestion (ETL)** | LangChain, Haystack, custom scripts | **LlamaIndex 0.14.6** | âœ… **Especializado en RAG ingestion** (score 8.6/10 vs LangChain 7.1/10)<br/>âœ… **600+ data loaders** (PDF, DOCX, Markdown, web)<br/>âœ… **Neo4j connector nativo** (`Neo4jVectorStore`, `PropertyGraphIndex`)<br/>âœ… **4 retrievers especializados**: VectorContext, TextToCypher, LLMSynonym, CypherTemplate<br/>âœ… **Semantic chunking** (sentence, markdown headers)<br/>âœ… **Metadata extraction** automÃ¡tica<br/>âœ… **Incremental indexing** (tracking changes)<br/>âœ… **Trust score 8.7** (17,811 code snippets en Context7) |
| **Agent Runtime** | LlamaIndex, Haystack, AutoGen | **LangChain 0.3.x + LangGraph 0.2.x** | âœ… **Especializado en agent orchestration**<br/>âœ… **Custom tools** (SPARQL queries, reasoning)<br/>âœ… **LangGraph** para workflows complejos (estados, ciclos)<br/>âœ… **Streaming SSE** support<br/>âœ… **Multi-provider** (OpenAI/Anthropic fallback)<br/>âœ… **Memory management** (conversation history)<br/>âœ… **Neo4j + Fuseki** integration nativa |

**Por quÃ© ambos frameworks** (arquitectura complementaria):
- **LlamaIndex**: ETL pipeline (docs â†’ embeddings â†’ Neo4j) - **Fase de ingestion**
- **LangChain**: Agent runtime (user query â†’ tools â†’ response) - **Fase de consulta**
- **Neo4j GraphRAG**: Retrievers hÃ­bridos (vector + Cypher) - **Patrones avanzados**
- **Complementarios**: LlamaIndex carga datos, LangChain + Neo4j GraphRAG los consultan

**InstalaciÃ³n (versiones pinadas segÃºn Context7)**:
```bash
# LlamaIndex 0.14.6 (ingestion)
pip install llama-index==0.14.6 \
            llama-index-vector-stores-neo4jvector==0.2.0 \
            llama-index-graph-stores-neo4j==0.3.0 \
            llama-index-embeddings-ollama==0.3.0

# LangChain 0.3.x (agents)
pip install langchain==0.3.7 \
            langchain-community==0.3.5 \
            langgraph==0.2.45 \
            langchain-openai==0.2.8 \
            langchain-anthropic==0.2.4 \
            langchain-neo4j==0.2.0

# Neo4j GraphRAG Python 0.4.0 (retrievers)
pip install neo4j-graphrag-python==0.4.0 \
            neo4j==5.26.0
```

**Ejemplo LlamaIndex (ingestion con Ollama embeddings)**:
```python
from llama_index.core import SimpleDirectoryReader, PropertyGraphIndex
from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore
from llama_index.embeddings.ollama import OllamaEmbedding

# Embeddings locales con Ollama
embed_model = OllamaEmbedding(
    model_name="nomic-embed-text",
    base_url="http://localhost:11434"
)

# Cargar documentos ontologÃ­a
documents = SimpleDirectoryReader("./ontology_docs/").load_data()

# Neo4j como graph store
graph_store = Neo4jPropertyGraphStore(
    url="bolt://localhost:7687",
    username="neo4j",
    password="aoem2024"
)

# Crear Property Graph Index
index = PropertyGraphIndex.from_documents(
    documents,
    property_graph_store=graph_store,
    embed_model=embed_model,
    show_progress=True
)

print(f"Indexados {len(documents)} documentos en Neo4j")
```

**Ejemplo Neo4j GraphRAG (retrieval hÃ­brido)**:
```python
from neo4j import GraphDatabase
from neo4j_graphrag.retrievers import HybridCypherRetriever
from neo4j_graphrag.embeddings import OllamaEmbeddings

# Embeddings locales
embedder = OllamaEmbeddings(
    model_name="nomic-embed-text",
    base_url="http://localhost:11434"
)

# Conectar Neo4j
driver = GraphDatabase.driver(
    "bolt://localhost:7687",
    auth=("neo4j", "aoem2024")
)

# Custom Cypher retrieval query
retrieval_query = """
MATCH (node)-[:AUTHORED_BY]->(author:Author)
MATCH (node)-[:TAGGED_WITH]->(tag:Tag)
RETURN node.content AS content,
       author.name AS author,
       collect(DISTINCT tag.name) AS tags
ORDER BY node.publication_date DESC
"""

# Hybrid retriever (vector + fulltext + Cypher)
retriever = HybridCypherRetriever(
    driver=driver,
    vector_index_name="melquisedec_embeddings",
    fulltext_index_name="document_fulltext",
    retrieval_query=retrieval_query,
    embedder=embedder
)

# BÃºsqueda hÃ­brida
results = retriever.search(
    query_text="Â¿CuÃ¡l es la relaciÃ³n entre ontologÃ­as y knowledge graphs?",
    top_k=10
)

for item in results.items:
    print(f"Content: {item.content[:150]}...")
    print(f"Author: {item.metadata.get('author')}")
    print(f"Tags: {', '.join(item.metadata.get('tags', []))}\n")

driver.close()
```

# Crear agente con tools
llm = ChatOpenAI(model="gpt-4o-mini")
tools = [
    # Tool 1: Cypher query
    graph.as_tool("Query Neo4j graph database"),
    # Tool 2: SPARQL (custom)
    # Tool 3: Vector search
]
agent = create_react_agent(llm, tools)
executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# Ejecutar query
response = executor.invoke({"input": "Â¿QuÃ© mÃ©todos AOEM tienen fase de diseÃ±o?"})
```

#### **Capa 5: Ontology Authoring**

| Componente | Alternativas | SelecciÃ³n | JustificaciÃ³n |
|------------|-------------|-----------|---------------|
| **Desktop** | TopBraid, VocBench, ProtÃ©gÃ© | **ProtÃ©gÃ© Desktop 5.6.4** | âœ… EstÃ¡ndar de facto (150k+ usuarios)<br/>âœ… Reasoner integration (HermiT, Pellet)<br/>âœ… SPARQL Query panel<br/>âœ… Visualization plugins<br/>âœ… Import/Export mÃºltiples formatos<br/>âœ… Java-based (cross-platform) |
| **Collaborative** | OntoWiki, VocBench, WebProtÃ©gÃ© | **WebProtÃ©gÃ© (opcional)** | âœ… Web-based (sin instalaciÃ³n cliente)<br/>âœ… Multi-user (concurrent editing)<br/>âœ… Change tracking (audit log)<br/>âœ… Docker Compose ready<br/>âŒ Requiere MongoDB + backend<br/>âš ï¸ Opcional para MVP |

#### **Capa 6: Validation y Quality**

| Componente | Alternativas | SelecciÃ³n | JustificaciÃ³n |
|------------|-------------|-----------|---------------|
| **SHACL Validation** | Apache Jena SHACL, TopQuadrant SHACL, pySHACL | **pySHACL 0.28.x** | âœ… Python-native (CI/CD friendly)<br/>âœ… SHACL-AF (Advanced Features)<br/>âœ… RDFLib integration<br/>âœ… JSON validation reports<br/>âœ… CLI + programmatic API<br/>âœ… Fast (C extensions) |
| **OWL Reasoning** | Pellet, ELK, Openllet, HermiT | **HermiT 1.4.5** | âœ… OWL 2 DL completo<br/>âœ… Tableaux algorithm (sound + complete)<br/>âœ… ProtÃ©gÃ© plugin<br/>âœ… Java CLI (CI/CD)<br/>âŒ ELK mÃ¡s rÃ¡pido pero solo OWL EL |
| **Quality Reports** | Oops!, ROBOT, Ontology Pitfall Scanner | **ROBOT 1.9.6** | âœ… CLI-first (automation)<br/>âœ… 20+ quality checks<br/>âœ… Pipeline support (convert, merge, reason, report)<br/>âœ… GitHub Actions examples<br/>âœ… Turtle/RDF-XML/JSON-LD I/O |

---

### 6.1.6 DecisiÃ³n ArquitectÃ³nica: Stack HÃ­brido Validado (GenAI-Stack + Fuseki)

#### ğŸ”´ **Pregunta CrÃ­tica del Usuario** (Respondida con Research R1.1-R1.5)

> "Â¿Por quÃ© Milvus y no Neo4j? Explica mejor porque escogiste esta arquitectura y no GenAI-Stack + Fuseki que serÃ­an solo dos docker. De esto no sÃ© asÃ­ que estudia esto con bÃºsquedas avanzadas, recomienda la mejor prÃ¡ctica."

**Respuesta Final**: Adoptamos **GenAI-Stack (Neo4j 5.26) + Fuseki + Ollama embeddings** basado en investigaciÃ³n rigurosa con Context7 y research documents R1.1-R1.5.

**Esta secciÃ³n documenta el anÃ¡lisis profundo con investigaciÃ³n web (enero 2026) que valida/cuestiona la decisiÃ³n arquitectÃ³nica original.**

---

#### 6.1.6.1 Hallazgos de InvestigaciÃ³n (Web Search - Enero 2026)

##### **Hallazgo #1: Neo4j Tiene Vector Search Nativo (Desde 5.13 GA 2024)**

ğŸ” **Fuente**: Neo4j Official Docs - Vector Indexes

- **Neo4j 5.13+** tiene **vector indexes nativos** (Apache Lucene-powered)
- Soporta **256-3072 dimensiones** (cubre modelos de embeddings comunes)
- Puede combinar **graph traversal + vector similarity en una sola query Cypher**
- Ejemplo: Dataset de pelÃ­culas con 28k nodos, 332k relaciones + embeddings

```cypher
// Query hÃ­brida: GrÃ¡fico + Vectores en una sola lÃ­nea
MATCH (m:Movie)
WHERE m.title CONTAINS 'Matrix'
CALL db.index.vector.queryNodes('movieEmbeddings', 5, m.embedding)
YIELD node, score
RETURN node.title, score
ORDER BY score DESC
```

**ImplicaciÃ³n**: Neo4j puede **reemplazar a Milvus** para casos de uso AOEM (<100M vectores).

---

##### **Hallazgo #2: GraphRAG Es el Nuevo EstÃ¡ndar (2025-2026)**

ğŸ” **Fuente**: Iterathon Blog - GraphRAG vs Vector RAG 2026

- **GraphRAG** (Graph + Vector) logra **85%+ accuracy** vs **Vector-only RAG 70%**
- Microsoft Research + Neo4j deployments en producciÃ³n
- **RazÃ³n**: "Vectors excel at similarity, graphs excel at reasoning and relationships"
- 2025 marca emergencia de GraphRAG como evoluciÃ³n de RAG tradicional

**Casos de uso GraphRAG**:
- âœ… Multi-hop reasoning ("Â¿QuÃ© proyectos usan ontologÃ­as creadas por expertos en mi organizaciÃ³n?")
- âœ… Contextual understanding (relaciones entre entidades en embeddings)
- âœ… Explainability (path en grafo explica por quÃ© se recomienda)

**ImplicaciÃ³n**: Para AOEM, necesitamos **graph + vector juntos**, no solo vector search.

---

##### **Hallazgo #3: RDF vs Property Graphs - Paradigmas Complementarios**

ğŸ” **Fuente**: Neo4j Blog - RDF vs Property Graphs + PMC Study

**Diferencias clave**:

| CaracterÃ­stica | RDF Triplestore (Fuseki) | Property Graph (Neo4j) |
|----------------|--------------------------|------------------------|
| **Modelo** | Triples: `(sujeto, predicado, objeto)` | Nodos + Relaciones + Propiedades |
| **Query** | SPARQL 1.1 (estÃ¡ndar W3C) | Cypher (Neo4j) |
| **EstÃ¡ndares** | OWL, RDFS, SHACL (formal) | No estÃ¡ndar (LPG pragmÃ¡tico) |
| **Reasoning** | HermiT, Pellet (OWL 2 DL) | APOC procedures (custom) |
| **Performance** | Federated queries, RDFS entailment | Index-free adjacency (graph traversal rÃ¡pido) |
| **Casos de uso** | **Ontology engineering**, semantic web, interoperabilidad | **Operational KGs**, ML pipelines, graph analytics |

**Punto crÃ­tico**: RDF y Property Graphs **pueden coexistir** via **n10s (neosemantics) plugin** de Neo4j.

---

##### **Hallazgo #4: n10s (neosemantics) - Puente RDF â†” Neo4j**

ğŸ” **Fuente**: Ontotext Blog + Neo4j Documentation

**n10s** es un plugin Neo4j que permite:
- âœ… **Importar RDF/OWL desde triplestore** (Fuseki) a Neo4j
- âœ… **Exportar Property Graph como RDF** (Turtle, JSON-LD)
- âœ… **Mapeo automÃ¡tico** de triples a nodos/relaciones
- âœ… **SPARQL endpoint** (limitado) sobre Neo4j

```cypher
// Importar ontologÃ­a desde Fuseki a Neo4j
CALL n10s.rdf.import.fetch(
  'http://fuseki:3030/aoem/data',
  'Turtle'
)
```

**Escenario de uso**:
1. **Fuseki**: Authoring formal de ontologÃ­as (ProtÃ©gÃ© â†’ Fuseki)
2. **Neo4j**: Vista materializada para queries operacionales + ML

**ImplicaciÃ³n**: Fuseki + Neo4j juntos **no son redundantes**, sino **complementarios**.

---

##### **Hallazgo #5: GenAI-Stack Es Una Arquitectura Probada**

ğŸ” **Fuente**: Neo4j GenAI-Stack GitHub (docker-compose-based reference)

**GenAI-Stack** (Neo4j oficial):
- ğŸ³ **2 contenedores**: Neo4j + Ollama
- ğŸ”§ Stack: LangChain + Neo4j + Ollama + (opcional) Weaviate/Milvus
- âœ… **Production-ready** (usado en conferencias Neo4j NODES)
- âœ… **Vector search nativo** en Neo4j 5.13+
- âœ… **GraphRAG patterns** con LangGraph

**Ventaja**: Simplifica despliegue (vs 3+ contenedores Fuseki + Milvus + Ollama).

---

#### 6.1.6.2 AnÃ¡lisis: Â¿Por QuÃ© Elegimos Milvus en 6.1.2?

**Razones originales (vÃ¡lidas en 2023-2024)**:
1. âœ… Milvus es **dedicated vector DB** (billones de vectores, Ã­ndices mÃºltiples: HNSW, IVF_FLAT, DiskANN)
2. âœ… **Kubernetes-native** (Milvus Operator para auto-scaling)
3. âœ… **Independiente del grafo** (no acoplado a Cypher/SPARQL)
4. âœ… Mejor para **pure vector search** si no necesitas graph traversal

**Razones ahora cuestionables (2025-2026)**:
1. âŒ Neo4j 5.13+ tiene **vector search nativo** (no descubierto en anÃ¡lisis original)
2. âŒ AOEM necesita **GraphRAG** (graph + vector), no solo vector
3. âŒ **3 contenedores** (Fuseki + Milvus + Ollama) vs **2 contenedores** (Neo4j + Ollama) + Fuseki opcional
4. âŒ **Milvus no tiene graph context** (pierde relationships para embeddings)

---

#### 6.1.6.3 Tres Opciones ArquitectÃ³nicas - Comparativa Honesta

##### **OpciÃ³n A: GenAI-Stack Simplificado (Neo4j + Ollama)**

```mermaid
graph TB
    A[ProtÃ©gÃ© Desktop] -->|RDF/OWL Export| B[Neo4j + n10s]
    B -->|Import RDF| B
    C[Ollama] -->|Embeddings| B
    D[LangChain Agent] -->|Cypher Queries| B
    D -->|Vector Search| B
    D -->|LLM Calls| C
```

| Aspecto | EvaluaciÃ³n |
|---------|-----------|
| **Contenedores** | ğŸŸ¢ **2** (Neo4j + Ollama) |
| **Vector Search** | ğŸŸ¢ Nativo en Neo4j 5.13+ (256-3072d) |
| **Graph Traversal** | ğŸŸ¢ Index-free adjacency (fast) |
| **GraphRAG** | ğŸŸ¢ Single Cypher query (graph + vector) |
| **SPARQL** | ğŸŸ¡ Limitado (n10s proporciona endpoint bÃ¡sico) |
| **OWL Reasoning** | ğŸ”´ No nativo (necesita plugin externo o Java reasoner) |
| **Ontology Authoring** | ğŸŸ¡ ProtÃ©gÃ© â†’ RDF export â†’ n10s import |
| **ProducciÃ³n** | ğŸŸ¢ GenAI-Stack probado (Neo4j NODES 2024) |
| **Costo Dev** | ğŸŸ¢ $0 (Neo4j Community Edition) |
| **Costo Prod** | ğŸŸ¡ Neo4j Aura ~$65-200/mes (3-5 GB) |

**Pros**:
- âœ… **MÃ¡s simple**: 2 contenedores vs 3+
- âœ… **GraphRAG nativo**: Single query para graph + vector
- âœ… **Mejor para ML**: Graph algorithms (PageRank, community detection)
- âœ… **Velocidad**: Index-free adjacency para traversals

**Contras**:
- âŒ **SPARQL limitado**: n10s no soporta federated queries ni entailment completo
- âŒ **No OWL 2 DL reasoning**: HermiT/Pellet no integrados
- âŒ **Vendor lock-in**: Cypher no es estÃ¡ndar W3C (vs SPARQL)

**Recomendado para**:
- ğŸ¯ Proyectos que priorizan **operational KGs** sobre **formal ontology engineering**
- ğŸ¯ GraphRAG con ML (recomendaciones, embeddings, clustering)
- ğŸ¯ Startups/MVPs que quieren **simplicidad**

---

##### **OpciÃ³n B: Fuseki + Neo4j + Ollama (HÃ­brido - NUESTRA RECOMENDACIÃ“N ACTUALIZADA)**

```mermaid
graph TB
    A[ProtÃ©gÃ© Desktop] -->|SPARQL Upload| B[Apache Jena Fuseki]
    B -->|RDF/OWL Source| B
    B -->|n10s Sync| C[Neo4j Property Graph]
    D[Ollama] -->|Embeddings| C
    E[LangChain Agent] -->|SPARQL Queries| B
    E -->|Cypher + Vector| C
    E -->|LLM Calls| D
    F[pySHACL] -->|Validation| B
    G[HermiT] -->|Reasoning| B
```

| Aspecto | EvaluaciÃ³n |
|---------|-----------|
| **Contenedores** | ğŸŸ¡ **3** (Fuseki + Neo4j + Ollama) |
| **Vector Search** | ğŸŸ¢ Neo4j nativo |
| **Graph Traversal** | ğŸŸ¢ Neo4j Cypher (fast) |
| **GraphRAG** | ğŸŸ¢ Cypher hÃ­brido |
| **SPARQL** | ğŸŸ¢ **SPARQL 1.1 completo** (Fuseki) |
| **OWL Reasoning** | ğŸŸ¢ HermiT + RDFS entailment |
| **Ontology Authoring** | ğŸŸ¢ ProtÃ©gÃ© â†’ Fuseki (nativo) |
| **ProducciÃ³n** | ğŸŸ¢ Ambos components production-ready |
| **SincronizaciÃ³n** | ğŸŸ¡ Fuseki â†’ Neo4j (n10s batch o streaming) |

**Pros**:
- âœ… **Best of both worlds**: Formal reasoning (Fuseki) + Graph performance (Neo4j)
- âœ… **EstÃ¡ndares W3C**: SPARQL, OWL, SHACL (compliance)
- âœ… **SeparaciÃ³n de concerns**: Authoring (Fuseki) vs Operational (Neo4j)
- âœ… **GraphRAG**: Vector + graph en Neo4j
- âœ… **Interoperabilidad**: Fuseki SPARQL endpoint para otros sistemas

**Contras**:
- âŒ **Complejidad**: 3 contenedores + sincronizaciÃ³n Fuseki â†” Neo4j
- âŒ **Latencia**: Sync puede ser batch (no real-time)
- âŒ **Costo**: Neo4j + Fuseki hosting

**Recomendado para**:
- ğŸ¯ **AOEM production** donde necesitas:
  - Formal ontology engineering (OWL 2 DL, SHACL, SPARQL Federation)
  - Operational performance para agentes (graph traversal, ML)
  - Standards compliance (W3C)
- ğŸ¯ Organizaciones que valoran **interoperabilidad** sobre simplicidad

**PatrÃ³n de sincronizaciÃ³n**:
1. **Fuseki**: Source of truth (ProtÃ©gÃ© authoring)
2. **Neo4j**: Materialized view (sincronizada diariamente o on-demand via n10s)
3. **Agentes**: Consultan Neo4j (fast) + Fuseki cuando necesitan reasoning

---

##### **OpciÃ³n C: Fuseki + Milvus + Ollama (Stack Original - Ahora NO RECOMENDADO)**

```mermaid
graph TB
    A[ProtÃ©gÃ© Desktop] -->|SPARQL Upload| B[Apache Jena Fuseki]
    B -->|RDF/OWL| B
    C[Ollama] -->|Embeddings| D[Milvus Vector DB]
    E[LangChain Agent] -->|SPARQL| B
    E -->|Vector Search| D
    E -->|LLM| C
    F[pySHACL] -->|Validation| B
```

| Aspecto | EvaluaciÃ³n |
|---------|-----------|
| **Contenedores** | ğŸ”´ **5+** (Fuseki + Milvus + etcd + MinIO + Ollama) |
| **Vector Search** | ğŸŸ¢ Milvus (billones vectores, Ã­ndices mÃºltiples) |
| **Graph Traversal** | ğŸ”´ **No disponible** (Fuseki no es graph DB) |
| **GraphRAG** | ğŸ”´ **Imposible** (2 queries: SPARQL + Milvus, luego merge manual) |
| **SPARQL** | ğŸŸ¢ Completo (Fuseki) |
| **OWL Reasoning** | ğŸŸ¢ HermiT completo |
| **Complejidad** | ğŸ”´ 5+ contenedores + 2 lenguajes query |

**Pros**:
- âœ… Milvus mejor para **billion-scale vectors** (>100M embeddings)
- âœ… Ãndices avanzados: HNSW, IVF_FLAT, DiskANN

**Contras**:
- âŒ **No GraphRAG**: Vector y SPARQL separados (agente debe hacer 2 queries + merge)
- âŒ **MÃ¡s complejidad**: 5+ contenedores (Milvus necesita etcd + MinIO)
- âŒ **No graph context**: Milvus no sabe de relationships
- âŒ **Over-engineering**: AOEM probablemente <10M vectores (Neo4j suficiente)

**Recomendado para**:
- ğŸ¯ **Escala extrema**: >100M vectores + sin necesidad de graph traversal
- ğŸ¯ AOEM en esta escala es **improbable** (10k-1M clases tÃ­pico)

---

#### 6.1.6.4 RecomendaciÃ³n Final Basada en InvestigaciÃ³n

##### **Para AOEM 2.0: OpciÃ³n B (Fuseki + Neo4j + Ollama) - HÃBRIDO**

**Razones**:

1. **AOEM necesita formal ontology engineering** (OWL 2 DL, SHACL, SPARQL Federation)
   - ProtÃ©gÃ© â†’ Fuseki es el flujo estÃ¡ndar
   - HermiT reasoning para OWL DL

2. **AOEM necesita GraphRAG** (research 2025-2026 confirma 85% accuracy vs 70%)
   - Neo4j proporciona graph + vector en single query
   - Graph context mejora embeddings (relaciones entre conceptos)

3. **Simplicidad razonable**:
   - 3 contenedores (vs 5+ con Milvus)
   - 2 query languages (SPARQL para authoring, Cypher para operational)

4. **ProducciÃ³n probada**:
   - Fuseki: 10 aÃ±os en producciÃ³n (Apache)
   - Neo4j: NODES 2024 demostrÃ³ GenAI-Stack con vector search

5. **Escalabilidad**:
   - Neo4j Aura: Auto-scaling hasta 10M+ nodos
   - Si >100M vectores en futuro â†’ migrar a Milvus (pero improbable para AOEM)

##### **MigraciÃ³n desde Stack Original (6.1.2)**

| Cambio | RazÃ³n |
|--------|-------|
| âŒ **Remover**: Milvus + etcd + MinIO | Neo4j vector search suficiente para AOEM |
| âœ… **Agregar**: Neo4j 5.13+ con vector index | GraphRAG + graph algorithms |
| âœ… **Agregar**: n10s plugin | Sync Fuseki (source) â†’ Neo4j (operational) |
| âœ… **Mantener**: Fuseki + pySHACL + HermiT | Formal reasoning necesario |

##### **Roadmap de AdopciÃ³n**

**Fase 1 (MVP)**: OpciÃ³n A (Neo4j + Ollama) - 2 semanas
- Deploy GenAI-Stack
- ProtÃ©gÃ© â†’ RDF export â†’ n10s import
- LangChain agent con Cypher + vector search
- **Validar**: GraphRAG funciona para AOEM queries

**Fase 2 (Production)**: Migrar a OpciÃ³n B (+ Fuseki) - 1 mes
- Deploy Fuseki como source of truth
- Configurar n10s sync (batch diario)
- pySHACL + HermiT pipeline
- **Validar**: Formal reasoning + GraphRAG juntos

**Fase 3 (Scale)**: Optimizaciones - ongoing
- Neo4j Aura para auto-scaling
- Streaming sync Fuseki â†’ Neo4j (si necesario)
- Considerar Milvus solo si >100M vectores

---

### 6.1.7 OntologÃ­as vs Knowledge Graphs: RelaciÃ³n y Complementariedad

#### ğŸ“ **Pregunta del Usuario**

> "Explica la relaciÃ³n entre grafos de conocimientos y ontologÃ­as, como se complementan y se usan."

Esta secciÃ³n clarifica conceptos fundamentales con investigaciÃ³n acadÃ©mica + prÃ¡ctica de la industria.

---

#### 6.1.7.1 Definiciones Formales

##### **Knowledge Graph (Concepto General)**

ğŸ” **Fuente**: Hogan et al. 2020 (Survey acadÃ©mico mÃ¡s citado)

> "Un knowledge graph es un grafo de datos destinado a acumular y transmitir conocimiento del mundo real, donde nodos representan entidades y aristas representan relaciones."

**FormalizaciÃ³n mÃ­nima**:
```
KG = (V, R, E)
donde:
  V = conjunto de entidades (vÃ©rtices/nodos)
  R = conjunto de tipos de relaciÃ³n (etiquetas de aristas)
  E âŠ† V Ã— R Ã— V = conjunto de triples (head, relation, tail)
```

**Ejemplo AOEM**:
```turtle
# Triple RDF (una instancia de KG)
:DataProperty rdf:type owl:DatatypeProperty .
# V = {:DataProperty, owl:DatatypeProperty}
# R = {rdf:type}
# E = {(:DataProperty, rdf:type, owl:DatatypeProperty)}
```

**Punto clave**: KG es **model-agnostic** - puede ser RDF, Property Graph, hÃ­brido, etc.

---

##### **OntologÃ­a (Conocimiento Formal)**

ğŸ” **Fuente**: Gruber 1993 + W3C OWL Specification

> "Una ontologÃ­a es una especificaciÃ³n explÃ­cita y formal de una conceptualizaciÃ³n compartida."

**CaracterÃ­sticas de ontologÃ­as formales**:
1. **Clases** (`owl:Class`): Conceptos del dominio (ej: `Person`, `Document`)
2. **Propiedades** (`owl:ObjectProperty`, `owl:DatatypeProperty`): Relaciones entre clases
3. **Axiomas**: Restricciones lÃ³gicas (ej: `Person SubClassOf Agent`)
4. **Instancias**: Datos concretos que satisfacen la ontologÃ­a
5. **Reasoning**: Inferencia automÃ¡tica (ej: si `X subClassOf Y` y `Y subClassOf Z` entonces `X subClassOf Z`)

**Lenguajes formales**:
- **RDFS**: Subclases, dominios, rangos (reasoning bÃ¡sico)
- **OWL 2**: LÃ³gica descriptiva (OWL DL), restricciones complejas, equivalencias
- **SHACL**: ValidaciÃ³n de constraints (shapes)

**Ejemplo AOEM (ontologÃ­a formal)**:
```turtle
# DefiniciÃ³n de clase con axiomas
:OntologyEngineeringMethod a owl:Class ;
    rdfs:label "MÃ©todo de IngenierÃ­a OntolÃ³gica"@es ;
    rdfs:subClassOf :Method ;
    owl:disjointWith :SoftwareEngineeringMethod .

# Propiedad con restricciones
:hasPhase a owl:ObjectProperty ;
    rdfs:domain :OntologyEngineeringMethod ;
    rdfs:range :Phase ;
    owl:minCardinality 1 .
```

**Inferencia (HermiT reasoner)**:
```
Si: AOEM rdf:type :OntologyEngineeringMethod
Y:  :OntologyEngineeringMethod rdfs:subClassOf :Method
Entonces (inferido): AOEM rdf:type :Method
```

---

#### 6.1.7.2 RelaciÃ³n: OntologÃ­as âŠ† Knowledge Graphs

ğŸ” **Fuente**: Enterprise Knowledge Blog + Ontotext Fundamentals

> **"OntologÃ­as son modelos de datos semÃ¡nticos generalizados, mientras que knowledge graphs son la aplicaciÃ³n de ese modelo a datos de instancia."**

**AnalogÃ­a SQL**:
- **OntologÃ­a** = Schema (DDL: `CREATE TABLE`, constraints, foreign keys)
- **Knowledge Graph** = Schema + Data (DML: `INSERT`, actual rows)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Knowledge Graph (KG)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   OntologÃ­a (Schema + Axiomas)        â”‚  â”‚
â”‚  â”‚   - Clases: Person, Organization      â”‚  â”‚
â”‚  â”‚   - Propiedades: worksFor, knows      â”‚  â”‚
â”‚  â”‚   - Axiomas: Person âŠ“ Organization=âŠ¥  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Instancias (ABox)                   â”‚  â”‚
â”‚  â”‚   - :Alice rdf:type :Person           â”‚  â”‚
â”‚  â”‚   - :Alice :worksFor :Neo4j           â”‚  â”‚
â”‚  â”‚   - :Neo4j rdf:type :Organization     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**En RDF/OWL**:
- **TBox** (Terminological Box) = OntologÃ­a (clases, propiedades, axiomas)
- **ABox** (Assertion Box) = Instancias (datos concretos)
- **KG** = TBox + ABox

---

#### 6.1.7.3 Complementariedad: Tres Paradigmas

ğŸ” **Fuente**: PMC Study (Property Graph vs RDF), Neo4j Blog, Ontotext

##### **Paradigma 1: RDF Knowledge Graphs (Semantic Web)**

**TecnologÃ­a**: Apache Jena Fuseki, GraphDB, Stardog, Virtuoso

**Modelo**:
```turtle
# Triples: sujeto predicado objeto
:Alice :worksFor :Neo4j .
:Neo4j rdf:type :Organization .
:Organization rdfs:subClassOf :LegalEntity .
```

**Fortalezas**:
- âœ… **EstÃ¡ndares W3C**: SPARQL, OWL, SHACL (interoperabilidad)
- âœ… **Reasoning formal**: HermiT, Pellet (OWL 2 DL)
- âœ… **Federated queries**: SPARQL puede consultar mÃºltiples endpoints
- âœ… **Schema evolution**: Agregar propiedades sin migraciÃ³n

**Debilidades**:
- âŒ **Performance**: Triples menos eficientes que property graphs para traversals
- âŒ **Complejidad**: OWL 2 tiene curva de aprendizaje alta
- âŒ **Tooling**: Menos herramientas visuales que Neo4j Bloom

**Casos de uso**:
- ğŸ¯ **Ontology engineering** (AOEM authoring)
- ğŸ¯ **Standards compliance** (salud, gobierno, academia)
- ğŸ¯ **Data integration** (federated queries cross-org)

---

##### **Paradigma 2: Property Graphs (Operational KGs)**

**TecnologÃ­a**: Neo4j, Amazon Neptune, TigerGraph, ArangoDB

**Modelo**:
```cypher
// Nodos con propiedades + relaciones etiquetadas
CREATE (alice:Person {name: 'Alice', age: 30})
CREATE (neo4j:Organization {name: 'Neo4j', founded: 2007})
CREATE (alice)-[:WORKS_FOR {since: 2020}]->(neo4j)
```

**Fortalezas**:
- âœ… **Performance**: Index-free adjacency (traversals O(1) por relaciÃ³n)
- âœ… **Intuitive**: Cypher mÃ¡s legible que SPARQL para developers
- âœ… **Graph algorithms**: PageRank, community detection, shortest path (built-in)
- âœ… **ML integration**: Graph embeddings, GNNs

**Debilidades**:
- âŒ **No estÃ¡ndar**: Cypher es Neo4j-specific (vs SPARQL W3C)
- âŒ **No formal reasoning**: No OWL 2 DL reasoner nativo
- âŒ **Schema rÃ­gido**: Cambios requieren migraciÃ³n (vs RDF schema-less)

**Casos de uso**:
- ğŸ¯ **Operational KGs** (fraud detection, recomendaciones)
- ğŸ¯ **GraphRAG** (LLM + graph context)
- ğŸ¯ **Real-time** (social networks, supply chain)

---

##### **Paradigma 3: HÃ­brido (RDF + Property Graph)**

**TecnologÃ­a**: Neo4j + n10s, Amazon Neptune (dual-model), Stardog (SPARQL + graph)

**Arquitectura**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       n10s sync       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Fuseki (RDF) â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚ Neo4j (LPG)  â”‚
â”‚  - Authoring â”‚                       â”‚  - Queries   â”‚
â”‚  - Reasoning â”‚                       â”‚  - ML        â”‚
â”‚  - SHACL     â”‚                       â”‚  - GraphRAG  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Flujo de trabajo**:
1. **Authoring**: Ontologist usa ProtÃ©gÃ© â†’ carga a Fuseki (RDF)
2. **Validation**: pySHACL + HermiT sobre Fuseki
3. **Sync**: n10s importa RDF a Neo4j (batch o streaming)
4. **Operational**: Agentes consultan Neo4j (Cypher + vector search)
5. **Reasoning on-demand**: Agente llama Fuseki SPARQL para inference compleja

**Fortalezas**:
- âœ… **Best of both**: Formal semantics (RDF) + Performance (Neo4j)
- âœ… **Separation of concerns**: Authoring vs Operational
- âœ… **Standards + Pragmatism**: W3C compliance + graph speed

**Debilidades**:
- âŒ **Complejidad**: Mantener 2 stores sincronizados
- âŒ **Latency**: Sync puede no ser real-time
- âŒ **Cost**: 2 databases en producciÃ³n

**Casos de uso**:
- ğŸ¯ **AOEM**: Formal ontology (Fuseki) + GraphRAG agents (Neo4j)
- ğŸ¯ **Enterprise KGs**: Compliance + operational performance
- ğŸ¯ **Research**: Semantic standards + ML pipelines

---

#### 6.1.7.4 Tabla Comparativa: CuÃ¡ndo Usar Cada Paradigma

| Criterio | Solo RDF (Fuseki) | Solo Property Graph (Neo4j) | HÃ­brido (Fuseki + Neo4j) |
|----------|-------------------|----------------------------|--------------------------|
| **Formal reasoning (OWL 2 DL)** | ğŸŸ¢ Nativo (HermiT) | ğŸ”´ No soportado | ğŸŸ¢ Fuseki lado |
| **Graph traversal speed** | ğŸŸ¡ Aceptable (millones triples) | ğŸŸ¢ Excelente (billones relaciones) | ğŸŸ¢ Neo4j lado |
| **SPARQL 1.1 completo** | ğŸŸ¢ Nativo | ğŸŸ¡ n10s limitado | ğŸŸ¢ Fuseki lado |
| **GraphRAG (graph + vector)** | ğŸ”´ No (solo RDF) | ğŸŸ¢ Nativo (5.13+) | ğŸŸ¢ Neo4j lado |
| **Standards compliance (W3C)** | ğŸŸ¢ 100% | ğŸ”´ Cypher no estÃ¡ndar | ğŸŸ¢ Fuseki lado |
| **ML/Graph algorithms** | ğŸ”´ Limitado | ğŸŸ¢ Built-in (GDS library) | ğŸŸ¢ Neo4j lado |
| **Schema evolution** | ğŸŸ¢ Schema-less | ğŸŸ¡ Requiere migraciÃ³n | ğŸŸ¢ RDF lado |
| **Tooling (UI)** | ğŸŸ¡ ProtÃ©gÃ© | ğŸŸ¢ Neo4j Bloom | ğŸŸ¢ Ambos |
| **Deployment complexity** | ğŸŸ¢ 1-2 contenedores | ğŸŸ¢ 1-2 contenedores | ğŸŸ¡ 3 contenedores |
| **Query language learning curve** | ğŸ”´ SPARQL complejo | ğŸŸ¢ Cypher intuitivo | ğŸŸ¡ Ambos idiomas |
| **Costo producciÃ³n (managed)** | ğŸŸ¢ Bajo (GraphDB ~$500/m) | ğŸŸ¡ Medio (Aura ~$65-200/m) | ğŸ”´ Alto (ambos) |

**RecomendaciÃ³n para AOEM**:

- **Fase MVP** (2-4 semanas): **Solo Neo4j** (OpciÃ³n A) - validar GraphRAG
- **ProducciÃ³n** (1-3 meses): **HÃ­brido** (OpciÃ³n B) - formal reasoning + performance
- **No usar**: **Solo Fuseki** - necesitas GraphRAG (research 2025-2026)

---

#### 6.1.7.5 Ejemplo Concreto: Consulta AOEM en los 3 Paradigmas

**Pregunta**: "Â¿QuÃ© mÃ©todos de ontologÃ­a tienen una fase de diseÃ±o y fueron publicados despuÃ©s de 2020?"

##### **SPARQL (Fuseki - RDF)**

```sparql
PREFIX aoem: <http://example.org/aoem#>
PREFIX dc: <http://purl.org/dc/terms/>

SELECT ?method ?name ?year WHERE {
  ?method a aoem:OntologyEngineeringMethod ;
          aoem:hasPhase aoem:DesignPhase ;
          aoem:name ?name ;
          dc:date ?year .
  FILTER(?year > 2020)
}
ORDER BY DESC(?year)
```

**Ventaja**: Reasoning automÃ¡tico (si `aoem:DesignPhase rdfs:subClassOf aoem:Phase`, infiere)

---

##### **Cypher (Neo4j - Property Graph)**

```cypher
MATCH (method:Method)-[:HAS_PHASE]->(phase:Phase {name: 'Design'})
WHERE method.publicationYear > 2020
RETURN method.name, method.publicationYear
ORDER BY method.publicationYear DESC
```

**Ventaja**: MÃ¡s rÃ¡pido para traversals (index-free adjacency)

---

##### **HÃ­brido (GraphRAG con Embeddings)**

```cypher
// Neo4j: Combinar graph + vector + metadata
MATCH (method:Method)-[:HAS_PHASE]->(phase:Phase {name: 'Design'})
WHERE method.publicationYear > 2020
WITH method
CALL db.index.vector.queryNodes('methodEmbeddings', 5, $queryEmbedding)
YIELD node, score
WHERE node.publicationYear > 2020
RETURN node.name, node.publicationYear, score
ORDER BY score DESC
```

**Ventaja**: GraphRAG - combina graph structure + semantic similarity + metadata filters

---

#### 6.1.7.6 ConclusiÃ³n: CÃ³mo se Complementan en AOEM

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AOEM 2.0 - Stack HÃ­brido                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  OntologÃ­a (Fuseki + pySHACL + HermiT)                   â”‚ â”‚
â”‚  â”‚  "Capa de Conocimiento Formal"                           â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚  - Define CLASES, PROPIEDADES, AXIOMAS                   â”‚ â”‚
â”‚  â”‚  - Valida constraints (SHACL shapes)                     â”‚ â”‚
â”‚  â”‚  - Infiere nuevos triples (OWL 2 DL reasoning)           â”‚ â”‚
â”‚  â”‚  - Proporciona SPARQL endpoint para standards            â”‚ â”‚
â”‚  â”‚  - Authoring con ProtÃ©gÃ©                                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â†“ n10s sync                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Knowledge Graph (Neo4j + Vector Index)                  â”‚ â”‚
â”‚  â”‚  "Capa Operacional + GraphRAG"                           â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚  - Materializa instancias de ontologÃ­a                   â”‚ â”‚
â”‚  â”‚  - Graph traversal ultra-rÃ¡pido (Cypher)                 â”‚ â”‚
â”‚  â”‚  - Vector search nativo (embeddings)                     â”‚ â”‚
â”‚  â”‚  - GraphRAG: Single query (graph + vector + metadata)    â”‚ â”‚
â”‚  â”‚  - Graph algorithms (PageRank, community detection)      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  LangChain Agents (Ollama + Tools)                       â”‚ â”‚
â”‚  â”‚  "Capa de InteracciÃ³n"                                   â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚  - Consulta Neo4j (operational, fast)                    â”‚ â”‚
â”‚  â”‚  - Consulta Fuseki (reasoning cuando necesario)          â”‚ â”‚
â”‚  â”‚  - Genera embeddings (Ollama nomic-embed-text)           â”‚ â”‚
â”‚  â”‚  - Explica resultados (graph paths como evidencia)       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Flujo de trabajo AOEM**:
1. **Ontologist** crea/modifica ontologÃ­a en ProtÃ©gÃ© â†’ sube a Fuseki
2. **CI/CD** valida con pySHACL + HermiT (formal correctness)
3. **n10s** sincroniza Fuseki â†’ Neo4j (instancias + schema)
4. **Embedder** genera vectores para conceptos ontolÃ³gicos â†’ almacena en Neo4j vector index
5. **Agent** recibe query usuario â†’ hace GraphRAG en Neo4j (rÃ¡pido) â†’ llama Fuseki SPARQL si necesita reasoning complejo
6. **Agent** retorna respuesta con **explicaciÃ³n** (graph path + vector similarity scores)

**Resumen**:
- ğŸ§  **OntologÃ­as**: Proporcionan **formal semantics** y **reasoning**
- ğŸŒ **Knowledge Graphs**: Proporcionan **estructura de datos** y **performance**
- ğŸ¤ **Complementariedad**: OntologÃ­as son **TBox** (schema), KG es **TBox + ABox** (schema + data)
- âš™ï¸ **En AOEM**: Fuseki (authoring formal) + Neo4j (operational GraphRAG) = **hÃ­brido Ã³ptimo**

---

### 6.1.3 Requisitos de Hardware

#### **Desarrollo Local (1 developer)** âœ… **32GB RAM suficiente con API externa**

| Componente | MÃ­nimo | Recomendado | Notas |
|------------|--------|-------------|-------|
| **CPU** | 4 cores | 8+ cores | Neo4j + Fuseki CPU-intensivo |
| **RAM** | **16 GB** âœ… | **32 GB** âœ… | - Neo4j: 4-8 GB<br/>- Fuseki: 2 GB<br/>- OS + Apps: 4-6 GB<br/>- **SIN Ollama local** (API externa) |
| **Storage** | 50 GB SSD | 200 GB NVMe | - Neo4j data: 10-50 GB<br/>- Fuseki TDB2: 10-50 GB<br/>- Python venv: 2 GB |
| **GPU** | N/A | N/A | **No necesario** (sin Ollama local) |
| **Internet** | 10 Mbps | 100 Mbps | Para API calls (OpenAI/Anthropic) |

**Comparativa vs Stack con Ollama local**:

| Stack | RAM Usada | GPU | Costo Mensual |
|-------|-----------|-----|---------------|
| **Actual (API externa)** | 10-16 GB | âŒ No | $5-10 (API calls) |
| Original (Ollama local) | 22-26 GB | âœ… 8GB VRAM | $0 (pero hardware caro) |

**Ventaja**: Con 32GB RAM cabe cÃ³modamente Fuseki + Neo4j + apps sin swap.

---

#### **ProducciÃ³n Cloud (AWS)** - Stack HÃ­brido

| Componente | EC2 Type | vCPU | RAM | Storage | Costo/mes* |
|------------|----------|------|-----|---------|-----------|
| **Fuseki** | r6i.large | 2 | 16 GB | 100 GB gp3 | $125 |
| **Neo4j** | r6i.xlarge | 4 | 32 GB | 200 GB gp3 | $250 |
| **API LLM** | OpenAI/Anthropic | - | - | - | $50-200 (uso) |
| **Total** | - | 6 | 48 GB | 300 GB | **~$425-575/mes** |

*Precios AWS us-east-1, On-Demand. Ahorro 40-60% con Reserved Instances.

**Alternativa Managed Services**:
- Neo4j Aura: $65-200/mes (3-5 GB, auto-scaling)
- Amazon Neptune: $0.10/hora (~$73/mes, serverless)
- OpenAI API: $10-50/mes tÃ­pico

**Stack mÃ­nimo producciÃ³n (managed)**:
- Neo4j Aura Professional: $150/mes
- Fuseki en EC2 r6i.large: $125/mes
- OpenAI GPT-4o-mini: $20/mes
- **Total**: ~$300/mes (vs $1050 con Ollama GPU)

---

### 6.1.3 Hardware Requirements (Stack Validado - 32GB RAM)

#### **ConfiguraciÃ³n de Desarrollo (32GB RAM) - VALIDADO âœ…**

| Componente | RAM | CPU | Storage | JustificaciÃ³n |
|------------|-----|-----|---------|---------------|
| **Fuseki** | 2GB | 2 cores | 20GB | JVM_ARGS `-Xmx2g`, almacena RDF/OWL ontology source |
| **Neo4j** | 12GB | 4 cores | 50GB | Heap 8GB + Page Cache 4GB, graph + vector index HNSW |
| **Ollama (embeddings)** | 500MB | 1 core | 5GB | **SOLO nomic-embed-text** (768d), NO LLM reasoning |
| **API Backend (FastAPI)** | 2GB | 2 cores | 10GB | Workers + LangChain/LlamaIndex + requests cache |
| **OS + Overhead** | 6GB | - | 20GB | Docker Desktop + sistema operativo + buffers |
| **Total** | **22.5GB** | **9 cores** | **105GB** | âœ… Dentro de constraint 32GB RAM |

**ValidaciÃ³n de RAM allocation**:
```bash
# Verificar uso de RAM
docker stats --no-stream

# Debe mostrar aproximadamente:
# aoem-fuseki:    ~1.8GB (JVM max 2GB)
# aoem-neo4j:     ~11GB (heap 8GB + page cache 3-4GB)
# aoem-ollama:    ~450MB (solo embedding model cargado)
# aoem-api:       ~1.5GB (FastAPI + workers)
# Total Docker:   ~15GB (REST es OS + overhead)
```

**Modelo embeddings (Ollama local)**:
```yaml
ollama:
  modelo: nomic-embed-text
  dimensiones: 768
  tamaÃ±o_descarga: 274MB
  ram_cargado: ~500MB
  latencia: <10ms (local)
  costo: $0 (vs OpenAI $0.020/1M tokens)
```

**LLM Reasoning (API externa)**:
```yaml
reasoning:
  provider: OpenAI GPT-4o-mini + Anthropic Claude 3.5
  ram_local: 0GB (API calls)
  costo_mensual: ~$5-10 (desarrollo)
  latencia: 100-300ms (API)
  context_window: 128k-200k tokens
```

---

#### **ConfiguraciÃ³n de ProducciÃ³n (Recomendada) - OpciÃ³n Cloud**

| Componente | RAM | CPU | Storage | Proveedor Sugerido |
|------------|-----|-----|---------|-------------------|
| **Fuseki** | 4GB | 2 cores | 50GB | AWS ECS Fargate (t3.medium equiv) |
| **Neo4j Aura** | Managed | Managed | 100GB | Neo4j Aura Professional (~$200/mes) |
| **Ollama** | 1GB | 1 core | 10GB | AWS ECS (embeddings only) |
| **API Backend** | 4GB | 4 cores | 20GB | AWS ECS + ALB |
| **Total** | N/A | N/A | 180GB | **~$300-400/mes** (vs $1050 con GPU) |

**Beneficios cloud vs local 32GB**:
- âœ… **Auto-scaling**: Neo4j Aura escala automÃ¡ticamente
- âœ… **Backup automÃ¡tico**: Snapshots diarios
- âœ… **Alta disponibilidad**: 99.9% SLA
- âœ… **Cero mantenimiento**: Parches y updates automÃ¡ticos
- âŒ **Costo**: $300-400/mes (vs hardware local amortizado)

---

#### **Comparativa Stack HÃ­brido vs API Total**

| MÃ©trica | Stack HÃ­brido (Ollama embeddings) | API Total (OpenAI embeddings) |
|---------|-----------------------------------|-------------------------------|
| **RAM local** | 22.5GB (Ollama 500MB) | 22GB (sin Ollama) |
| **Latencia embeddings** | <10ms (local) | ~100ms (API) |
| **Costo embeddings** | $0 | ~$0.50/mes (dev), $5-10/mes (prod) |
| **Privacy embeddings** | âœ… 100% local | âŒ Datos salen del servidor |
| **PrecisiÃ³n embeddings** | ğŸŸ¡ nomic 768d (buena) | ğŸŸ¢ OpenAI 1536d (excelente) |
| **Mantenimiento** | Actualizar Ollama model | Zero (API managed) |
| **RecomendaciÃ³n** | âœ… **DEV** (privacy + $0) | âš ï¸ **PROD** (mejor precisiÃ³n) |

---

#### **Bottleneck Analysis**

| OperaciÃ³n | Bottleneck | SoluciÃ³n |
|-----------|-----------|----------|
| **Vector search** | Neo4j RAM (page cache) | Aumentar a 16GB si corpus >100K docs |
| **SPARQL reasoning** | Fuseki CPU (HermiT) | Usar ELK reasoner (mÃ¡s rÃ¡pido) o pre-compute |
| **Embedding generation** | Ollama CPU | Batch requests, usar GPU (si disponible) |
| **Agent LLM calls** | API latency (100-300ms) | Cache responses, usar streaming SSE |

---

### 6.1.4 Dependencias de Software (Versiones Validadas con Context7)

```yaml
# Docker Images (Stack HÃ­brido Validado)
docker_images:
  - stain/jena-fuseki:5.0.0           # Triplestore RDF (Apache Jena)
  - neo4j:5.26                         # Graph DB + Vector Index HNSW (GenAI-Stack oficial)
  - ollama/ollama:latest               # Embeddings locales (nomic-embed-text)

# Python 3.11+ (Requirements con versiones validadas Context7)
python_packages:
  # Agent frameworks (dual architecture)
  - llama-index==0.14.6              # Ingestion ETL (Trust 8.7)
  - llama-index-vector-stores-neo4jvector==0.2.0
  - llama-index-graph-stores-neo4j==0.3.0
  - llama-index-embeddings-ollama==0.3.0  # Ollama local embeddings

  - langchain==0.3.7                 # Agent runtime
  - langchain-community==0.3.5
  - langgraph==0.2.45                # State machines

  # LLM integrations (API externa)
  - langchain-openai==0.2.8          # GPT-4o-mini
  - langchain-anthropic==0.2.4       # Claude 3.5 Sonnet
  - openai==1.54.0                   # Direct API
  - anthropic==0.39.0                # Direct API

  # Graph databases
  - langchain-neo4j==0.2.0           # Neo4j LangChain integration
  - neo4j==5.26.0                    # Python driver (GenAI-Stack)
  - neo4j-graphrag-python==0.4.0     # Neo4j GraphRAG retrievers

  # Ollama (embeddings local)
  - ollama==0.3.3                    # Ollama Python SDK

  # Ontology tools
  - rdflib==7.1.1                    # RDF parsing
  - SPARQLWrapper==2.0.0             # SPARQL queries
  - pyshacl==0.28.0                  # SHACL validation

  # Utils
  - pytest==8.3.4                    # Testing
  - pytest-asyncio==0.25.2           # Async tests
  - python-dotenv==1.0.1             # .env management (API keys)
  - black==24.10.0                   # Code formatting
  - ruff==0.8.4                      # Linting

# Java Tools (JDK 11+)
java_tools:
  - protege-5.6.4.jar                # Desktop authoring
  - hermit-1.4.5.jar                 # OWL reasoner
  - robot-1.9.6.jar                  # Quality automation

# Neo4j Plugins (instalados en container 5.26)
neo4j_plugins:
  - apoc-5.26.0.jar                  # Procedures library
  - neo4j-graph-data-science-2.12.1.jar  # GDS algorithms
  - neosemantics-5.26.0.jar          # n10s: RDF import/export
```

**InstalaciÃ³n completa (Python)**:
```bash
# Crear virtual environment
python3.11 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Instalar dependencias (stack validado)
pip install -U pip setuptools wheel
pip install \
  llama-index==0.14.6 \
  llama-index-vector-stores-neo4jvector==0.2.0 \
  llama-index-graph-stores-neo4j==0.3.0 \
  llama-index-embeddings-ollama==0.3.0 \
  langchain==0.3.7 \
  langchain-community==0.3.5 \
  langgraph==0.2.45 \
  langchain-openai==0.2.8 \
  langchain-anthropic==0.2.4 \
  langchain-neo4j==0.2.0 \
  neo4j==5.26.0 \
  neo4j-graphrag-python==0.4.0 \
  ollama==0.3.3 \
  rdflib==7.1.1 \
  SPARQLWrapper==2.0.0 \
  pyshacl==0.28.0 \
  pytest==8.3.4 \
  python-dotenv==1.0.1

# Verificar instalaciÃ³n
python -c "import langchain; import llama_index; import neo4j_graphrag; import ollama; print('Stack validado OK')"
```

**Variables de entorno requeridas**:
```bash
# .env file (stack hÃ­brido validado)

# LLM APIs (reasoning)
OPENAI_API_KEY=sk-proj-...
ANTHROPIC_API_KEY=sk-ant-...

# Ollama (embeddings local)
OLLAMA_BASE_URL=http://localhost:11434
EMBEDDING_MODEL=nomic-embed-text

# Neo4j 5.26
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=aoem2024

# Fuseki
FUSEKI_URL=http://localhost:3030
FUSEKI_DATASET=aoem
```

# Command-line tools
cli_tools:
  - docker >= 24.0                   # Container runtime
  - docker-compose >= 2.20           # Multi-container orchestration
  - git >= 2.40                      # Version control
  - curl >= 8.0                      # HTTP testing
  - jq >= 1.7                        # JSON parsing
```

---

### 6.1.5 ValidaciÃ³n del Stack Seleccionado

#### **ValidaciÃ³n AcadÃ©mica** (basada en papers ArXiv 2023-2025)

| Componente | Paper de Referencia | ValidaciÃ³n |
|------------|-------------------|------------|
| **Agent-driven KG** | "AI Agent-Driven Framework for Automated Product KG Construction" (2025) | âœ… Agentes LangChain logran 97%+ coverage |
| **Semantic Embeddings** | "Lattice-preserving ALC ontology embeddings" (2023) | âœ… PyKEEN + RDF2Vec preservan semÃ¡ntica DL |
| **Ontology Matching** | "GraphMatcher: Graph Representation Learning" (2024) | âœ… OAEI 2022 resultados (F1 > 0.85) |
| **Hybrid Reasoning** | "Knowledge Graph Reasoning with Logics and Embeddings" (2022) | âœ… Hybrid approach (SPARQL + Vector) Ã³ptimo |

#### **ValidaciÃ³n Industrial** (proyectos de producciÃ³n)

| Proyecto | Org | Stack Similar | Escala |
|---------|-----|--------------|--------|
| **BioPortal** | Stanford | Fuseki + Solr + Rails | 1000+ ontologÃ­as, 10M+ tÃ©rminos |
| **Wikidata** | Wikimedia | Blazegraph (similar Fuseki) | 100M+ entidades |
| **DBpedia** | Leipzig Uni | Virtuoso (similar Fuseki) | 3B+ triples |
| **Schema.org** | Google/MS/Yahoo | Flat files â†’ generated RDF | Web-scale (usado por 40M+ sitios) |

**ConclusiÃ³n**: Stack validado para AOEM con precedentes acadÃ©micos y de producciÃ³n.

---

## 6.2 Requerimientos del Sistema

### 6.2.1 Requerimientos Funcionales

#### **RF-001: AutorÃ­a de OntologÃ­as**
- **DescripciÃ³n**: El sistema DEBE permitir crear y editar ontologÃ­as OWL 2 DL usando ProtÃ©gÃ© Desktop
- **Criterio de aceptaciÃ³n**:
  - [ ] ProtÃ©gÃ© 5.6.4 instalado y ejecutable
  - [ ] Capacidad de crear clases, propiedades, individuos
  - [ ] Export a Turtle (.ttl), RDF/XML, JSON-LD
  - [ ] Guardar en repositorio Git (`ontologies/*.ttl`)
- **Prioridad**: ğŸ”´ CrÃ­tica
- **VerificaciÃ³n**: Manual (screenshot de ProtÃ©gÃ© con BIBO cargado)

---

#### **RF-002: Almacenamiento RDF (Triplestore)**
- **DescripciÃ³n**: El sistema DEBE almacenar triples RDF en Apache Jena Fuseki con persistencia
- **Criterio de aceptaciÃ³n**:
  - [ ] Fuseki corriendo en `localhost:3030`
  - [ ] Dataset `/aoem` creado con TDB2
  - [ ] SPARQL endpoint activo: `http://localhost:3030/aoem/sparql`
  - [ ] Upload de ontologÃ­a exitoso (>1000 triples)
- **Prioridad**: ğŸ”´ CrÃ­tica
- **VerificaciÃ³n**:
  ```bash
  curl -X POST http://localhost:3030/aoem/data \
    -H "Content-Type: text/turtle" \
    --data-binary @ontologies/bibo.ttl

  curl "http://localhost:3030/aoem/sparql?query=SELECT (COUNT(*) as ?count) WHERE { ?s ?p ?o }"
  # Expected: {"results": {"bindings": [{"count": {"value": "1523"}}]}}
  ```

---

#### **RF-003: ValidaciÃ³n SHACL**
- **DescripciÃ³n**: El sistema DEBE validar ontologÃ­as contra shapes SHACL antes de commit
- **Criterio de aceptaciÃ³n**:
  - [ ] Archivo `shapes/aoem-shapes.ttl` con 5+ constraints
  - [ ] Script `scripts/validate.py` ejecutable
  - [ ] Validation report en formato JSON
  - [ ] Exit code 1 si hay violaciones
- **Prioridad**: ğŸŸ  Alta
- **VerificaciÃ³n**:
  ```bash
  python scripts/validate.py ontologies/bibo.ttl shapes/aoem-shapes.ttl
  # Expected output:
  # âœ… Validation passed: 0 violations
  # Exit code: 0
  ```

---

#### **RF-004: Razonamiento OWL**
- **DescripciÃ³n**: El sistema DEBE ejecutar reasoning HermiT para inferir nuevos triples
- **Criterio de aceptaciÃ³n**:
  - [ ] HermiT 1.4.5 JAR descargado
  - [ ] Script `scripts/reason.sh` funcional
  - [ ] Inferencias exportadas a `ontologies/bibo-inferred.ttl`
  - [ ] Log de reasoning sin errores
- **Prioridad**: ğŸŸ  Alta
- **VerificaciÃ³n**:
  ```bash
  ./scripts/reason.sh ontologies/bibo.ttl
  # Expected: bibo-inferred.ttl created
  # Check: grep "owl:equivalentClass" ontologies/bibo-inferred.ttl
  ```

---

#### **RF-005: Embeddings de Triples**
- **DescripciÃ³n**: El sistema DEBE generar embeddings para triples usando nomic-embed-text
- **Criterio de aceptaciÃ³n**:
  - [ ] Ollama con `nomic-embed-text` descargado
  - [ ] Script Python que convierte triples â†’ texto â†’ embeddings
  - [ ] Embeddings de 768 dimensiones
  - [ ] Almacenados en Milvus collection `aoem_embeddings`
- **Prioridad**: ğŸŸ  Alta
- **VerificaciÃ³n**:
  ```python
  from pymilvus import Collection
  coll = Collection("aoem_embeddings")
  print(coll.num_entities)  # Expected: > 1000
  print(coll.schema)        # Expected: dim=768
  ```

---

#### **RF-006: Vector Search**
- **DescripciÃ³n**: El sistema DEBE realizar bÃºsquedas de similitud en Milvus con HNSW index
- **Criterio de aceptaciÃ³n**:
  - [ ] Index HNSW creado (`M=16, efConstruction=200`)
  - [ ] Query: "libros de Tolkien" â†’ top 5 resultados
  - [ ] Latencia < 100ms para 10k vectors
  - [ ] Recall@5 > 0.9
- **Prioridad**: ğŸŸ  Alta
- **VerificaciÃ³n**:
  ```python
  results = coll.search(
      data=[query_embedding],
      anns_field="embedding",
      param={"metric_type": "COSINE", "params": {"ef": 64}},
      limit=5
  )
  print(results[0].distances)  # Expected: [0.95, 0.89, 0.87, 0.82, 0.79]
  ```

---

#### **RF-007: Agente SPARQL**
- **DescripciÃ³n**: El sistema DEBE proveer agente LangChain con SPARQL tool para queries
- **Criterio de aceptaciÃ³n**:
  - [ ] `SPARQLGraphChain` configurado con endpoint Fuseki
  - [ ] Agente responde: "Â¿CuÃ¡ntos autores hay?" â†’ ejecuta COUNT query
  - [ ] Respuesta en lenguaje natural con resultado numÃ©rico
  - [ ] Logs con query SPARQL ejecutado
- **Prioridad**: ğŸ”´ CrÃ­tica
- **VerificaciÃ³n**:
  ```python
  from agents.sparql_agent import create_agent
  agent = create_agent()
  response = agent.invoke("Â¿CuÃ¡ntos autores hay en la ontologÃ­a?")
  assert "autores" in response.lower()
  assert any(char.isdigit() for char in response)
  ```

---

#### **RF-008: Agente HÃ­brido (SPARQL + Vector)**
- **DescripciÃ³n**: El sistema DEBE combinar bÃºsqueda estructurada (SPARQL) y semÃ¡ntica (vector)
- **Criterio de aceptaciÃ³n**:
  - [ ] Query complejo: "Libros de fantasÃ­a publicados despuÃ©s de 1950"
  - [ ] Ejecuta SPARQL (filtro fecha) + vector search ("fantasÃ­a")
  - [ ] Fusiona resultados con ranking hÃ­brido (0.6*SPARQL + 0.4*vector)
  - [ ] Top 10 resultados con scores
- **Prioridad**: ğŸŸ  Alta
- **VerificaciÃ³n**: Test E2E con dataset BIBO (ver secciÃ³n 6.8.3)

---

#### **RF-009: Version Control (Git)**
- **DescripciÃ³n**: El sistema DEBE versionar ontologÃ­as en Git con commits atÃ³micos
- **Criterio de aceptaciÃ³n**:
  - [ ] Repo Git inicializado en `ontologies/`
  - [ ] `.gitignore` para archivos temporales (.catalog-v001.xml)
  - [ ] Commit hooks con validation pre-commit
  - [ ] GitHub Actions CI con ROBOT quality checks
- **Prioridad**: ğŸŸ¡ Media
- **VerificaciÃ³n**:
  ```bash
  cd ontologies/
  git log --oneline | head -5
  # Expected: commits con mensajes descriptivos
  ```

---

#### **RF-010: CI/CD Pipeline**
- **DescripciÃ³n**: El sistema DEBE ejecutar validaciÃ³n automÃ¡tica en cada pull request
- **Criterio de aceptaciÃ³n**:
  - [ ] GitHub Actions workflow `.github/workflows/ontology-ci.yml`
  - [ ] Steps: pySHACL validation, HermiT reasoning, ROBOT report
  - [ ] Falla si hay violations o errores
  - [ ] Badge en README con status
- **Prioridad**: ğŸŸ¡ Media
- **VerificaciÃ³n**: Ver logs de Ãºltima ejecuciÃ³n en GitHub Actions

---

### 6.2.2 Requerimientos No Funcionales

#### **RNF-001: Performance - SPARQL Queries**
- **DescripciÃ³n**: Queries SPARQL complejos DEBEN ejecutarse en < 2 segundos
- **MÃ©trica**: P95 latency < 2s para queries con 3+ JOINs
- **Test**:
  ```bash
  time curl "http://localhost:3030/aoem/sparql" \
    --data-urlencode "query=SELECT ?book ?author WHERE {
      ?book a bibo:Book .
      ?book bibo:author ?author .
      ?book bibo:publicationDate ?date .
      FILTER(?date > '1950-01-01'^^xsd:date)
    }"
  # Expected: real time < 2.0s
  ```

---

#### **RNF-002: Performance - Vector Search**
- **DescripciÃ³n**: Vector similarity search DEBE ejecutarse en < 100ms
- **MÃ©trica**: P95 latency < 100ms para 10k vectors
- **Test**:
  ```python
  import time
  start = time.time()
  results = coll.search(data=[query_vec], anns_field="embedding", limit=10)
  latency_ms = (time.time() - start) * 1000
  assert latency_ms < 100, f"Latency {latency_ms}ms exceeds 100ms threshold"
  ```

---

#### **RNF-003: Escalabilidad - Triples**
- **DescripciÃ³n**: Fuseki DEBE manejar 1M+ triples con degradaciÃ³n < 20%
- **MÃ©trica**: Query latency con 1M triples < 1.2x latency con 100k triples
- **Test**: Load test con Apache JMeter (ver secciÃ³n 6.8.4)

---

#### **RNF-004: Escalabilidad - Embeddings**
- **DescripciÃ³n**: Milvus DEBE manejar 100k+ vectors sin degradaciÃ³n
- **MÃ©trica**: Search latency con 100k vectors < 200ms
- **Test**: Benchmark con `pymilvus` (ver secciÃ³n 6.8.4)

---

#### **RNF-005: Disponibilidad**
- **DescripciÃ³n**: Servicios core (Fuseki, Milvus) DEBEN tener uptime > 99.5%
- **MÃ©trica**: Downtime < 3.6 horas/mes
- **Monitoreo**: Prometheus + Grafana dashboard

---

#### **RNF-006: RecuperaciÃ³n ante Fallos**
- **DescripciÃ³n**: Sistema DEBE recuperarse automÃ¡ticamente de crashes de servicios
- **Test**:
  ```bash
  docker kill fuseki
  sleep 30
  docker ps | grep fuseki  # Expected: container restarted by Docker
  curl http://localhost:3030/$/ping  # Expected: {"status":"ok"}
  ```

---

#### **RNF-007: Backup y Restore**
- **DescripciÃ³n**: Backup completo (Fuseki + Milvus) DEBE ejecutarse diariamente
- **Objetivo**: RPO < 24 horas, RTO < 1 hora
- **Script**: `scripts/backup.sh` con `tdb2.tdbbackup` + `milvus-backup`

---

#### **RNF-008: Seguridad - AutenticaciÃ³n**
- **DescripciÃ³n**: Fuseki DEBE requerir autenticaciÃ³n bÃ¡sica en producciÃ³n
- **ConfiguraciÃ³n**:
  ```bash
  # shiro.ini
  [users]
  admin = $shiro1$SHA-256$500000$...

  [urls]
  /aoem/** = authcBasic
  ```

---

#### **RNF-009: Observabilidad**
- **DescripciÃ³n**: Sistema DEBE exponer mÃ©tricas Prometheus
- **MÃ©tricas requeridas**:
  - `fuseki_query_duration_seconds` (histogram)
  - `milvus_search_latency_ms` (histogram)
  - `ollama_tokens_per_second` (gauge)
  - `agent_requests_total` (counter)

---

#### **RNF-010: Mantenibilidad**
- **DescripciÃ³n**: CÃ³digo DEBE cumplir > 80% test coverage y linting limpio
- **Tools**: `pytest-cov`, `black`, `ruff`
- **Enforcement**: GitHub Actions CI falla si coverage < 80%

---

## 6.3 Arquitectura del Sistema

### 6.3.1 Vista de Capas (5 Layers)

```mermaid
graph TB
    subgraph "LAYER 5: Presentation / Authoring"
        L5A[ProtÃ©gÃ© Desktop]
        L5B[WebProtÃ©gÃ©]
        L5C[Custom Web UI]
    end

    subgraph "LAYER 4: Application / Agents"
        L4A[LangChain Agents]
        L4B[SPARQL Agent]
        L4C[Hybrid Agent]
        L4D[Suggestion Agent]
    end

    subgraph "LAYER 3: Business Logic / Orchestration"
        L3A[Agent Orchestrator]
        L3B[Query Router]
        L3C[Embedding Pipeline]
        L3D[Validation Service]
    end

    subgraph "LAYER 2: Data Access / Storage"
        L2A[Fuseki Adapter]
        L2B[Milvus Adapter]
        L2C[Git Adapter]
        L2D[File System]
    end

    subgraph "LAYER 1: Infrastructure"
        L1A[Apache Jena Fuseki]
        L1B[Milvus Vector DB]
        L1C[Ollama LLM]
        L1D[Docker Network]
    end

    L5A --> L3D
    L5A --> L2D
    L5B --> L2A
    L5C --> L4A

    L4A --> L3A
    L4B --> L3B
    L4C --> L3B
    L4C --> L3C

    L3A --> L2A
    L3A --> L2B
    L3B --> L2A
    L3C --> L2B
    L3C --> L1C
    L3D --> L2A

    L2A --> L1A
    L2B --> L1B
    L2C --> L2D

    style L1A fill:#e1f5ff
    style L1B fill:#e1f5ff
    style L1C fill:#e1f5ff
    style L2A fill:#fff3cd
    style L2B fill:#fff3cd
    style L3A fill:#d4edda
    style L3B fill:#d4edda
    style L4A fill:#f8d7da
    style L4B fill:#f8d7da
```

---

### 6.3.2 Arquitectura de Componentes Detallada

```mermaid
C4Component
    title Arquitectura de Componentes - AOEM Stack

    Container_Boundary(c1, "Authoring Layer") {
        Component(protege, "ProtÃ©gÃ© Desktop", "Java Swing", "Ontology authoring with reasoning")
        Component(webprotege, "WebProtÃ©gÃ©", "Angular + Spring", "Collaborative editing")
    }

    Container_Boundary(c2, "Agent Layer") {
        Component(langchain, "LangChain", "Python", "Agent orchestration framework")
        Component(sparql_tool, "SPARQL Tool", "Python", "Executes SPARQL queries")
        Component(vector_tool, "Vector Tool", "Python", "Similarity search")
        Component(llm, "Ollama Client", "Python", "LLM invocations")
    }

    Container_Boundary(c3, "Data Layer") {
        ComponentDb(fuseki, "Apache Jena Fuseki", "Java + TDB2", "SPARQL endpoint, stores RDF triples")
        ComponentDb(milvus, "Milvus", "Go + C++", "Vector database with HNSW index")
        ComponentDb(git, "Git Repository", "Git", "Version control for ontologies")
    }

    Container_Boundary(c4, "Validation Layer") {
        Component(pyshacl, "pySHACL", "Python", "SHACL constraint validation")
        Component(hermit, "HermiT", "Java", "OWL 2 DL reasoner")
        Component(robot, "ROBOT", "Java", "Ontology quality reports")
    }

    Rel(protege, fuseki, "Uploads ontology", "HTTP POST Turtle")
    Rel(protege, hermit, "Triggers reasoning", "Java API")
    Rel(langchain, sparql_tool, "Invokes", "Python call")
    Rel(sparql_tool, fuseki, "Executes SPARQL", "HTTP GET/POST")
    Rel(langchain, vector_tool, "Invokes", "Python call")
    Rel(vector_tool, milvus, "Similarity search", "gRPC")
    Rel(langchain, llm, "Generates text", "HTTP POST")
    Rel(pyshacl, fuseki, "Validates triples", "HTTP GET")
    Rel(robot, git, "Reads ontologies", "File I/O")
```

---

### 6.3.3 Flujos de Datos Principales

#### **Flujo 1: CreaciÃ³n de OntologÃ­a**

```mermaid
sequenceDiagram
    actor User
    participant Protege
    participant Git
    participant CI
    participant pySHACL
    participant HermiT
    participant Fuseki

    User->>Protege: 1. Crear/editar ontologÃ­a
    Protege->>Protege: 2. Validate syntax
    Protege->>Git: 3. Save to file (.ttl)
    User->>Git: 4. git add + commit + push
    Git->>CI: 5. Trigger GitHub Actions
    CI->>pySHACL: 6. Validate SHACL shapes
    pySHACL-->>CI: 7. Validation report
    CI->>HermiT: 8. Run reasoner
    HermiT-->>CI: 9. Inferred triples
    CI->>ROBOT: 10. Generate quality report
    ROBOT-->>CI: 11. Quality metrics
    alt Validation Passed
        CI->>Fuseki: 12. Upload to triplestore
        Fuseki-->>CI: 13. Success (201 Created)
        CI->>User: 14. âœ… Deployment successful
    else Validation Failed
        CI->>User: 15. âŒ Fix violations
    end
```

---

#### **Flujo 2: Query HÃ­brido (SPARQL + Vector)**

```mermaid
sequenceDiagram
    actor User
    participant Agent
    participant Router
    participant SPARQL_Tool
    participant Vector_Tool
    participant Fuseki
    participant Milvus
    participant Ollama
    participant Ranker

    User->>Agent: "Libros de fantasÃ­a publicados despuÃ©s de 1950"
    Agent->>Router: 1. Parse intent
    Router->>Router: 2. Detect hybrid query

    par SPARQL Path
        Router->>SPARQL_Tool: 3a. Extract structured filters
        SPARQL_Tool->>Fuseki: 4a. SPARQL: publicationDate > 1950
        Fuseki-->>SPARQL_Tool: 5a. Candidate URIs (n=50)
    and Vector Path
        Router->>Ollama: 3b. Embed "fantasÃ­a"
        Ollama-->>Router: 4b. embedding_vector
        Router->>Vector_Tool: 5b. Search similar
        Vector_Tool->>Milvus: 6b. HNSW search (top 20)
        Milvus-->>Vector_Tool: 7b. Similar URIs + scores
    end

    SPARQL_Tool->>Ranker: 8a. Structured results
    Vector_Tool->>Ranker: 8b. Semantic results
    Ranker->>Ranker: 9. Merge (0.6*SPARQL + 0.4*Vector)
    Ranker->>Agent: 10. Ranked top 10 results
    Agent->>Ollama: 11. Generate natural language response
    Ollama-->>Agent: 12. "EncontrÃ© 10 libros..."
    Agent->>User: 13. Present results
```

---

#### **Flujo 3: GeneraciÃ³n de Embeddings**

```mermaid
sequenceDiagram
    participant Trigger as Cron Job / CI
    participant Pipeline
    participant Fuseki
    participant Ollama
    participant Milvus

    Trigger->>Pipeline: 1. Run daily (2 AM)
    Pipeline->>Fuseki: 2. SPARQL: SELECT new triples
    Fuseki-->>Pipeline: 3. Triples RDF (n=1000)

    loop For each triple
        Pipeline->>Pipeline: 4. Convert to text
        Note right of Pipeline: "(Book1, author, Tolkien)" â†’<br/>"Book1 has author Tolkien"
        Pipeline->>Ollama: 5. POST /api/embeddings
        Ollama-->>Pipeline: 6. embedding_768d
        Pipeline->>Milvus: 7. Insert (URI, embedding)
    end

    Milvus-->>Pipeline: 8. Insert count
    Pipeline->>Pipeline: 9. Log completion
    Pipeline->>Trigger: 10. Exit 0 (success)
```

---

### 6.3.4 Contratos Entre Capas

#### **Contrato 1: Authoring â†’ Storage**

| MÃ©todo | Endpoint | Request | Response | Status Codes |
|--------|----------|---------|----------|--------------|
| **Upload Ontology** | `POST /aoem/data` | `Content-Type: text/turtle`<br/>Body: RDF Turtle | `{"count": 1523}` | 201: Created<br/>400: Invalid RDF<br/>500: Server error |
| **Query Triples** | `GET /aoem/sparql` | `?query=SELECT * WHERE {?s ?p ?o}` | `application/sparql-results+json` | 200: OK<br/>400: Invalid SPARQL |

**Ejemplo Python**:
```python
import requests

# Upload ontology
with open('bibo.ttl', 'rb') as f:
    response = requests.post(
        'http://localhost:3030/aoem/data',
        headers={'Content-Type': 'text/turtle'},
        data=f
    )
    assert response.status_code == 201
```

---

#### **Contrato 2: Agent â†’ SPARQL Endpoint**

**Interface**:
```python
from typing import Protocol

class SPARQLEndpoint(Protocol):
    def query(self, sparql: str) -> dict:
        """Execute SELECT/ASK/DESCRIBE query.

        Args:
            sparql: Valid SPARQL query string

        Returns:
            {
                "results": {
                    "bindings": [
                        {"var1": {"type": "uri", "value": "..."}}
                    ]
                }
            }

        Raises:
            SPARQLSyntaxError: Invalid query
            SPARQLExecutionError: Execution failed
        """
        ...

    def update(self, sparql: str) -> None:
        """Execute INSERT/DELETE query."""
        ...
```

**ImplementaciÃ³n**:
```python
from SPARQLWrapper import SPARQLWrapper, JSON

class FusekiAdapter:
    def __init__(self, endpoint: str):
        self.sparql = SPARQLWrapper(endpoint)
        self.sparql.setReturnFormat(JSON)

    def query(self, sparql: str) -> dict:
        self.sparql.setQuery(sparql)
        return self.sparql.query().convert()
```

---

#### **Contrato 3: Embedding Pipeline â†’ Vector DB**

**Interface**:
```python
from typing import Protocol, List
import numpy as np

class VectorStore(Protocol):
    def insert(self,
               ids: List[str],
               embeddings: np.ndarray,
               metadata: List[dict]) -> int:
        """Insert embeddings with metadata.

        Args:
            ids: Unique identifiers (URIs)
            embeddings: numpy array shape (n, 768)
            metadata: List of dicts with triple info

        Returns:
            Number of inserted vectors
        """
        ...

    def search(self,
               query_embedding: np.ndarray,
               top_k: int = 10) -> List[dict]:
        """Similarity search.

        Returns:
            [
                {"id": "uri", "score": 0.95, "metadata": {...}},
                ...
            ]
        """
        ...
```

**ImplementaciÃ³n Milvus**:
```python
from pymilvus import Collection, connections

class MilvusAdapter:
    def __init__(self, collection_name: str):
        connections.connect("default", host="localhost", port="19530")
        self.collection = Collection(collection_name)
        self.collection.load()

    def insert(self, ids, embeddings, metadata):
        entities = [ids, embeddings.tolist(), metadata]
        insert_result = self.collection.insert(entities)
        self.collection.flush()
        return len(insert_result.primary_keys)

    def search(self, query_embedding, top_k=10):
        results = self.collection.search(
            data=[query_embedding.tolist()],
            anns_field="embedding",
            param={"metric_type": "COSINE", "params": {"ef": 64}},
            limit=top_k,
            output_fields=["metadata"]
        )
        return [
            {"id": hit.id, "score": hit.distance, "metadata": hit.entity.get("metadata")}
            for hit in results[0]
        ]
```

---

### 6.3.5 Ports & Adapters (Hexagonal Architecture)

```python
# domain/ports.py
from abc import ABC, abstractmethod
from typing import List, Dict

class TriplestorePort(ABC):
    """Port para acceso a triplestore"""

    @abstractmethod
    def query(self, sparql: str) -> List[Dict]:
        pass

    @abstractmethod
    def insert_triples(self, rdf_data: str, format: str) -> int:
        pass

class VectorStorePort(ABC):
    """Port para bÃºsqueda vectorial"""

    @abstractmethod
    def search_similar(self, embedding: List[float], top_k: int) -> List[Dict]:
        pass

    @abstractmethod
    def upsert_embeddings(self, vectors: List[Dict]) -> int:
        pass

class LLMPort(ABC):
    """Port para interacciÃ³n con LLM"""

    @abstractmethod
    def generate(self, prompt: str, max_tokens: int) -> str:
        pass

    @abstractmethod
    def embed_text(self, text: str) -> List[float]:
        pass

# adapters/fuseki_adapter.py
class FusekiAdapter(TriplestorePort):
    def __init__(self, endpoint: str):
        self.endpoint = endpoint
        # Implementation details...

# adapters/milvus_adapter.py
class MilvusAdapter(VectorStorePort):
    def __init__(self, host: str, port: int):
        # Implementation details...

# adapters/ollama_adapter.py
class OllamaAdapter(LLMPort):
    def __init__(self, base_url: str):
        # Implementation details...
```

---

## 6.4 Contratos y APIs

### 6.4.1 CatÃ¡logo de Queries SPARQL Comunes

Esta secciÃ³n documenta las 15 queries SPARQL mÃ¡s utilizadas en AOEM, con ejemplos ejecutables y resultados esperados.

#### **Query 1: Contar Todos los Triples**

**PropÃ³sito**: Verificar que la ontologÃ­a se cargÃ³ correctamente.

```sparql
# count-all-triples.rq
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>

SELECT (COUNT(*) as ?totalTriples)
WHERE {
  ?subject ?predicate ?object
}
```

**EjecuciÃ³n**:
```bash
curl -X POST http://localhost:3030/aoem/sparql \
  -H "Accept: application/sparql-results+json" \
  --data-urlencode "query@queries/count-all-triples.rq"
```

**Resultado Esperado**:
```json
{
  "head": {"vars": ["totalTriples"]},
  "results": {
    "bindings": [
      {
        "totalTriples": {
          "type": "literal",
          "datatype": "http://www.w3.org/2001/XMLSchema#integer",
          "value": "1523"
        }
      }
    ]
  }
}
```

**Criterio de ValidaciÃ³n**: `totalTriples > 1000` para BIBO completo.

---

#### **Query 2: Listar Todas las Clases**

**PropÃ³sito**: Explorar la jerarquÃ­a de clases de la ontologÃ­a.

```sparql
# list-classes.rq
PREFIX owl: <http://www.w3.org/2002/07/owl#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT DISTINCT ?class ?label
WHERE {
  ?class a owl:Class .
  OPTIONAL { ?class rdfs:label ?label }
}
ORDER BY ?class
LIMIT 50
```

**Resultado Esperado** (BIBO):
```json
{
  "results": {
    "bindings": [
      {
        "class": {"type": "uri", "value": "http://purl.org/ontology/bibo/AcademicArticle"},
        "label": {"type": "literal", "value": "Academic Article"}
      },
      {
        "class": {"type": "uri", "value": "http://purl.org/ontology/bibo/Book"},
        "label": {"type": "literal", "value": "Book"}
      },
      {
        "class": {"type": "uri", "value": "http://purl.org/ontology/bibo/Collection"},
        "label": {"type": "literal", "value": "Collection"}
      }
    ]
  }
}
```

**Python Helper**:
```python
def list_classes(endpoint: str) -> List[str]:
    """Lista todas las clases OWL de la ontologÃ­a."""
    query = """
    PREFIX owl: <http://www.w3.org/2002/07/owl#>
    SELECT DISTINCT ?class WHERE { ?class a owl:Class }
    """

    sparql = SPARQLWrapper(endpoint)
    sparql.setQuery(query)
    sparql.setReturnFormat(JSON)
    results = sparql.query().convert()

    return [
        binding["class"]["value"]
        for binding in results["results"]["bindings"]
    ]
```

---

#### **Query 3: JerarquÃ­a de Subclases**

**PropÃ³sito**: Entender herencia de clases (Ã¡rbol taxonÃ³mico).

```sparql
# class-hierarchy.rq
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX bibo: <http://purl.org/ontology/bibo/>

SELECT ?subclass ?superclass ?subLabel ?superLabel
WHERE {
  ?subclass rdfs:subClassOf ?superclass .
  FILTER(?superclass != <http://www.w3.org/2002/07/owl#Thing>)

  OPTIONAL { ?subclass rdfs:label ?subLabel }
  OPTIONAL { ?superclass rdfs:label ?superLabel }
}
ORDER BY ?superclass ?subclass
```

**Resultado Esperado** (fragmento BIBO):
```
subclass                      | superclass                | subLabel              | superLabel
------------------------------|---------------------------|-----------------------|-------------------
bibo:AcademicArticle          | bibo:Article              | "Academic Article"    | "Article"
bibo:Article                  | bibo:Document             | "Article"             | "Document"
bibo:Book                     | bibo:Document             | "Book"                | "Document"
bibo:Thesis                   | bibo:Document             | "Thesis"              | "Document"
```

**VisualizaciÃ³n Mermaid** (generada a partir de resultados):
```mermaid
graph TD
    Document["Document"]
    Article["Article"]
    AcademicArticle["Academic Article"]
    Book["Book"]
    Thesis["Thesis"]

    Document --> Article
    Document --> Book
    Document --> Thesis
    Article --> AcademicArticle
```

---

#### **Query 4: Propiedades de una Clase**

**PropÃ³sito**: Descubrir quÃ© propiedades tiene una clase especÃ­fica.

```sparql
# class-properties.rq
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX owl: <http://www.w3.org/2002/07/owl#>
PREFIX bibo: <http://purl.org/ontology/bibo/>

SELECT DISTINCT ?property ?propertyLabel ?range ?rangeLabel
WHERE {
  # Propiedades cuyo dominio es bibo:Book
  {
    ?property rdfs:domain bibo:Book .
  }
  UNION
  {
    # O subclase de Book
    ?property rdfs:domain ?domain .
    ?domain rdfs:subClassOf* bibo:Book .
  }

  OPTIONAL { ?property rdfs:label ?propertyLabel }
  OPTIONAL {
    ?property rdfs:range ?range .
    ?range rdfs:label ?rangeLabel
  }
}
ORDER BY ?property
```

**Resultado Esperado** (BIBO Book):
```json
[
  {
    "property": "http://purl.org/ontology/bibo/isbn",
    "propertyLabel": "ISBN",
    "range": "http://www.w3.org/2001/XMLSchema#string"
  },
  {
    "property": "http://purl.org/dc/terms/creator",
    "propertyLabel": "creator",
    "range": "http://xmlns.com/foaf/0.1/Person"
  },
  {
    "property": "http://purl.org/ontology/bibo/numPages",
    "propertyLabel": "number of pages",
    "range": "http://www.w3.org/2001/XMLSchema#integer"
  }
]
```

---

#### **Query 5: Buscar Instancias por Clase**

**PropÃ³sito**: Encontrar todos los individuos de un tipo especÃ­fico.

```sparql
# find-instances-by-type.rq
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX bibo: <http://purl.org/ontology/bibo/>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?book ?title ?author
WHERE {
  ?book rdf:type bibo:Book .
  OPTIONAL { ?book rdfs:label ?title }
  OPTIONAL { ?book bibo:author ?author }
}
LIMIT 20
```

**Python Parametrizado**:
```python
def find_instances_by_type(endpoint: str, class_uri: str, limit: int = 20) -> List[Dict]:
    """Encuentra instancias de una clase especÃ­fica."""
    query = f"""
    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

    SELECT ?instance ?label
    WHERE {{
      ?instance rdf:type <{class_uri}> .
      OPTIONAL {{ ?instance rdfs:label ?label }}
    }}
    LIMIT {limit}
    """

    sparql = SPARQLWrapper(endpoint)
    sparql.setQuery(query)
    sparql.setReturnFormat(JSON)
    results = sparql.query().convert()

    return [
        {
            "uri": b["instance"]["value"],
            "label": b.get("label", {}).get("value", "N/A")
        }
        for b in results["results"]["bindings"]
    ]

# Uso
books = find_instances_by_type(
    "http://localhost:3030/aoem/sparql",
    "http://purl.org/ontology/bibo/Book"
)
```

---

#### **Query 6: BÃºsqueda por Texto (Full-Text Search)**

**PropÃ³sito**: Buscar entidades que contengan una palabra clave (requiere Lucene/Solr index en Fuseki).

```sparql
# text-search.rq
PREFIX text: <http://jena.apache.org/text#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?entity ?label ?score
WHERE {
  (?entity ?score) text:query (rdfs:label "Tolkien*") .
  ?entity rdfs:label ?label .
}
ORDER BY DESC(?score)
LIMIT 10
```

**ConfiguraciÃ³n Fuseki** (requerida):
```turtle
# fuseki-config.ttl
@prefix fuseki: <http://jena.apache.org/fuseki#> .
@prefix text: <http://jena.apache.org/text#> .

<#service> a fuseki:Service ;
    fuseki:name "aoem" ;
    fuseki:endpoint [ fuseki:operation fuseki:query ] ;
    fuseki:dataset <#text_dataset> .

<#text_dataset> a text:TextDataset ;
    text:dataset <#dataset> ;
    text:index <#indexLucene> .

<#indexLucene> a text:TextIndexLucene ;
    text:directory <file:lucene> ;
    text:entityMap <#entMap> .

<#entMap> a text:EntityMap ;
    text:defaultField "label" ;
    text:entityField "uri" ;
    text:map (
        [ text:field "label" ; text:predicate rdfs:label ]
    ) .
```

---

#### **Query 7: Filtros por Fecha**

**PropÃ³sito**: Buscar recursos publicados en un rango de fechas.

```sparql
# filter-by-date.rq
PREFIX bibo: <http://purl.org/ontology/bibo/>
PREFIX dc: <http://purl.org/dc/terms/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>

SELECT ?book ?title ?date
WHERE {
  ?book a bibo:Book ;
        dc:title ?title ;
        dc:date ?date .

  FILTER(?date >= "1950-01-01"^^xsd:date && ?date <= "2000-12-31"^^xsd:date)
}
ORDER BY ?date
```

**Python Helper con ParÃ¡metros**:
```python
from datetime import date

def filter_by_date_range(
    endpoint: str,
    class_uri: str,
    start_date: date,
    end_date: date
) -> List[Dict]:
    """Filtra recursos por rango de fechas."""
    query = f"""
    PREFIX dc: <http://purl.org/dc/terms/>
    PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>

    SELECT ?resource ?date
    WHERE {{
      ?resource a <{class_uri}> ;
                dc:date ?date .

      FILTER(?date >= "{start_date.isoformat()}"^^xsd:date &&
             ?date <= "{end_date.isoformat()}"^^xsd:date)
    }}
    ORDER BY ?date
    """

    sparql = SPARQLWrapper(endpoint)
    sparql.setQuery(query)
    sparql.setReturnFormat(JSON)
    return sparql.query().convert()["results"]["bindings"]
```

---

#### **Query 8: Agregaciones (COUNT, AVG, MIN, MAX)**

**PropÃ³sito**: EstadÃ­sticas sobre propiedades numÃ©ricas.

```sparql
# aggregations.rq
PREFIX bibo: <http://purl.org/ontology/bibo/>

SELECT
  (COUNT(?book) as ?totalBooks)
  (AVG(?pages) as ?avgPages)
  (MIN(?pages) as ?minPages)
  (MAX(?pages) as ?maxPages)
WHERE {
  ?book a bibo:Book .
  OPTIONAL { ?book bibo:numPages ?pages }
}
```

**Resultado Esperado**:
```json
{
  "totalBooks": {"value": "1523"},
  "avgPages": {"value": "342.5"},
  "minPages": {"value": "50"},
  "maxPages": {"value": "1200"}
}
```

---

#### **Query 9: Graph Traversal (Rutas de N saltos)**

**PropÃ³sito**: Encontrar relaciones transitivas (ej: co-autores de co-autores).

```sparql
# graph-traversal.rq
PREFIX bibo: <http://purl.org/ontology/bibo/>
PREFIX foaf: <http://xmlns.com/foaf/0.1/>

SELECT DISTINCT ?coAuthor2 ?name
WHERE {
  # Libro de "J.R.R. Tolkien"
  ?book1 bibo:author ?tolkien .
  ?tolkien foaf:name "J.R.R. Tolkien" .

  # Co-autores de ese libro
  ?book1 bibo:author ?coAuthor1 .
  FILTER(?coAuthor1 != ?tolkien)

  # Otros libros de esos co-autores
  ?book2 bibo:author ?coAuthor1 .

  # Co-autores de esos libros (2 saltos)
  ?book2 bibo:author ?coAuthor2 .
  ?coAuthor2 foaf:name ?name .

  FILTER(?coAuthor2 != ?tolkien && ?coAuthor2 != ?coAuthor1)
}
LIMIT 50
```

**Nota**: Para grafos grandes, usar **Property Paths**:
```sparql
?start (bibo:author / ^bibo:author){1,3} ?related .
```

---

#### **Query 10: CONSTRUCT (Generar Nuevos Triples)**

**PropÃ³sito**: Crear vistas simplificadas de la ontologÃ­a.

```sparql
# construct-simplified-view.rq
PREFIX bibo: <http://purl.org/ontology/bibo/>
PREFIX dc: <http://purl.org/dc/terms/>
PREFIX foaf: <http://xmlns.com/foaf/0.1/>

CONSTRUCT {
  ?book a <http://example.org/SimpleBook> ;
        <http://example.org/title> ?title ;
        <http://example.org/authorName> ?authorName .
}
WHERE {
  ?book a bibo:Book ;
        dc:title ?title ;
        bibo:author ?author .
  ?author foaf:name ?authorName .
}
```

**Uso**: Exportar a nuevo grafo RDF simplificado para consumo externo.

---

#### **Query 11: OPTIONAL (Propiedades Opcionales)**

**PropÃ³sito**: Manejar datos incompletos sin perder resultados.

```sparql
# optional-properties.rq
PREFIX bibo: <http://purl.org/ontology/bibo/>
PREFIX dc: <http://purl.org/dc/terms/>

SELECT ?book ?title ?isbn ?pages
WHERE {
  ?book a bibo:Book ;
        dc:title ?title .

  # ISBN puede no existir
  OPTIONAL { ?book bibo:isbn ?isbn }

  # PÃ¡ginas pueden no estar especificadas
  OPTIONAL { ?book bibo:numPages ?pages }
}
LIMIT 20
```

**Resultado Esperado** (con NULL en campos opcionales):
```json
[
  {"book": "uri1", "title": "Book A", "isbn": "978-0-123", "pages": "320"},
  {"book": "uri2", "title": "Book B", "isbn": null, "pages": "250"},
  {"book": "uri3", "title": "Book C", "isbn": "978-0-456", "pages": null}
]
```

---

#### **Query 12: UNION (Combinar Patrones)**

**PropÃ³sito**: Buscar en mÃºltiples propiedades alternativas.

```sparql
# union-properties.rq
PREFIX dc: <http://purl.org/dc/terms/>
PREFIX dcterms: <http://purl.org/dc/terms/>
PREFIX bibo: <http://purl.org/ontology/bibo/>

SELECT ?resource ?identifier
WHERE {
  ?resource a bibo:Document .

  {
    ?resource dc:identifier ?identifier .
  }
  UNION
  {
    ?resource bibo:doi ?identifier .
  }
  UNION
  {
    ?resource bibo:isbn ?identifier .
  }
}
```

---

#### **Query 13: FILTER Avanzado (Regex, Lang)**

**PropÃ³sito**: Filtros complejos con expresiones regulares e idiomas.

```sparql
# advanced-filters.rq
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX bibo: <http://purl.org/ontology/bibo/>

SELECT ?book ?title ?abstract
WHERE {
  ?book a bibo:Book ;
        rdfs:label ?title .

  OPTIONAL { ?book bibo:abstract ?abstract }

  # Solo tÃ­tulos en espaÃ±ol
  FILTER(LANG(?title) = "es")

  # TÃ­tulos que contengan "ontologÃ­a" (case-insensitive)
  FILTER(REGEX(?title, "ontolog[iÃ­]a", "i"))

  # Solo si el abstract existe y tiene mÃ¡s de 100 caracteres
  FILTER(BOUND(?abstract) && STRLEN(?abstract) > 100)
}
```

---

#### **Query 14: BIND (Variables Calculadas)**

**PropÃ³sito**: Crear nuevas variables derivadas.

```sparql
# bind-computed-vars.rq
PREFIX bibo: <http://purl.org/ontology/bibo/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>

SELECT ?book ?title ?pages ?category
WHERE {
  ?book a bibo:Book ;
        dc:title ?title ;
        bibo:numPages ?pages .

  # Categorizar por tamaÃ±o
  BIND(
    IF(?pages < 100, "Folleto",
    IF(?pages < 300, "Libro Corto",
    IF(?pages < 600, "Libro Medio",
    "Libro Largo"))) AS ?category
  )
}
ORDER BY DESC(?pages)
```

---

#### **Query 15: Federated Query (Consultar MÃºltiples Endpoints)**

**PropÃ³sito**: Combinar datos de DBpedia + ontologÃ­a local.

```sparql
# federated-query.rq
PREFIX dbo: <http://dbpedia.org/ontology/>
PREFIX bibo: <http://purl.org/ontology/bibo/>

SELECT ?localBook ?dbpediaAbstract
WHERE {
  # Query local
  ?localBook a bibo:Book ;
             dc:title ?title .

  # Query remoto a DBpedia
  SERVICE <http://dbpedia.org/sparql> {
    ?dbpediaBook rdfs:label ?title ;
                 dbo:abstract ?dbpediaAbstract .
    FILTER(LANG(?dbpediaAbstract) = "en")
  }
}
LIMIT 10
```

**Advertencia**: Queries federados pueden ser lentos (dependen de servicio externo).

---

### 6.4.2 API REST para Agentes

#### **EspecificaciÃ³n OpenAPI 3.0**

```yaml
# openapi.yaml
openapi: 3.0.3
info:
  title: AOEM Agent API
  version: 1.0.0
  description: |
    RESTful API para interactuar con el stack AOEM (Fuseki + Milvus + Ollama).
    Provee endpoints para queries hÃ­bridos, embeddings y validaciÃ³n.

servers:
  - url: http://localhost:8000/api/v1
    description: Development server
  - url: https://aoem.example.com/api/v1
    description: Production server

paths:
  /query/sparql:
    post:
      summary: Ejecutar query SPARQL
      operationId: executeSparqlQuery
      tags: [Query]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              required: [query]
              properties:
                query:
                  type: string
                  description: Query SPARQL vÃ¡lido
                  example: "SELECT ?s ?p ?o WHERE { ?s ?p ?o } LIMIT 10"
                timeout:
                  type: integer
                  description: Timeout en segundos
                  default: 30
      responses:
        '200':
          description: Query ejecutado exitosamente
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/SparqlResults'
        '400':
          $ref: '#/components/responses/BadRequest'
        '500':
          $ref: '#/components/responses/InternalError'

  /query/hybrid:
    post:
      summary: Query hÃ­brido (SPARQL + Vector Search)
      operationId: executeHybridQuery
      tags: [Query]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              required: [question]
              properties:
                question:
                  type: string
                  description: Pregunta en lenguaje natural
                  example: "Libros de fantasÃ­a publicados despuÃ©s de 1950"
                filters:
                  type: object
                  description: Filtros estructurados opcionales
                  properties:
                    type:
                      type: string
                      example: "http://purl.org/ontology/bibo/Book"
                    dateFrom:
                      type: string
                      format: date
                    dateTo:
                      type: string
                      format: date
                topK:
                  type: integer
                  description: NÃºmero de resultados a retornar
                  default: 10
                  minimum: 1
                  maximum: 100
                sparqlWeight:
                  type: number
                  description: Peso de resultados SPARQL (0.0-1.0)
                  default: 0.6
                  minimum: 0
                  maximum: 1
      responses:
        '200':
          description: Resultados hÃ­bridos
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/HybridResults'

  /embeddings/generate:
    post:
      summary: Generar embeddings para texto
      operationId: generateEmbeddings
      tags: [Embeddings]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              required: [texts]
              properties:
                texts:
                  type: array
                  items:
                    type: string
                  description: Lista de textos a embedear
                  maxItems: 1000
                  example: ["Tolkien wrote Lord of the Rings", "Ontology engineering"]
                model:
                  type: string
                  description: Modelo de embeddings
                  default: "nomic-embed-text"
                  enum: [nomic-embed-text, sentence-transformers]
      responses:
        '200':
          description: Embeddings generados
          content:
            application/json:
              schema:
                type: object
                properties:
                  embeddings:
                    type: array
                    items:
                      type: array
                      items:
                        type: number
                      minItems: 768
                      maxItems: 768
                  model:
                    type: string
                  dimensionality:
                    type: integer

  /embeddings/search:
    post:
      summary: BÃºsqueda por similitud vectorial
      operationId: vectorSearch
      tags: [Embeddings]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              required: [query]
              properties:
                query:
                  type: string
                  description: Texto de bÃºsqueda
                  example: "fantasy books"
                topK:
                  type: integer
                  default: 10
                metadataFilters:
                  type: object
                  description: Filtros adicionales sobre metadata
                  properties:
                    type:
                      type: string
                    language:
                      type: string
      responses:
        '200':
          description: Resultados de similitud
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/VectorSearchResults'

  /validation/shacl:
    post:
      summary: Validar ontologÃ­a contra shapes SHACL
      operationId: validateShacl
      tags: [Validation]
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              type: object
              required: [ontology, shapes]
              properties:
                ontology:
                  type: string
                  format: binary
                  description: Archivo .ttl de la ontologÃ­a
                shapes:
                  type: string
                  format: binary
                  description: Archivo .ttl con shapes SHACL
                inference:
                  type: string
                  enum: [none, rdfs, owlrl]
                  default: rdfs
      responses:
        '200':
          description: Reporte de validaciÃ³n
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ValidationReport'

  /reasoning/infer:
    post:
      summary: Ejecutar reasoning OWL sobre ontologÃ­a
      operationId: runReasoning
      tags: [Reasoning]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              required: [ontologyUri]
              properties:
                ontologyUri:
                  type: string
                  format: uri
                  description: URI del grafo a razonar
                reasoner:
                  type: string
                  enum: [hermit, elk, pellet]
                  default: hermit
                exportInferred:
                  type: boolean
                  description: Si exportar triples inferidos
                  default: true
      responses:
        '200':
          description: Reasoning completado
          content:
            application/json:
              schema:
                type: object
                properties:
                  status:
                    type: string
                    enum: [success, error]
                  inferredTriples:
                    type: integer
                  duration:
                    type: number
                    description: Tiempo en segundos
                  exportUri:
                    type: string
                    format: uri

components:
  schemas:
    SparqlResults:
      type: object
      properties:
        head:
          type: object
          properties:
            vars:
              type: array
              items:
                type: string
        results:
          type: object
          properties:
            bindings:
              type: array
              items:
                type: object
                additionalProperties:
                  type: object
                  properties:
                    type:
                      type: string
                    value:
                      type: string
                    datatype:
                      type: string
                    xml:lang:
                      type: string

    HybridResults:
      type: object
      properties:
        results:
          type: array
          items:
            type: object
            properties:
              uri:
                type: string
                format: uri
              label:
                type: string
              score:
                type: number
                description: Score hÃ­brido (0.0-1.0)
              sparqlScore:
                type: number
              vectorScore:
                type: number
              metadata:
                type: object
        executionTime:
          type: number
          description: Tiempo total en ms
        sparqlTime:
          type: number
        vectorTime:
          type: number

    VectorSearchResults:
      type: object
      properties:
        results:
          type: array
          items:
            type: object
            properties:
              id:
                type: string
              score:
                type: number
              metadata:
                type: object
        total:
          type: integer

    ValidationReport:
      type: object
      properties:
        conforms:
          type: boolean
          description: True si no hay violaciones
        violations:
          type: array
          items:
            type: object
            properties:
              focusNode:
                type: string
              resultPath:
                type: string
              value:
                type: string
              message:
                type: string
              severity:
                type: string
                enum: [Violation, Warning, Info]
        statistics:
          type: object
          properties:
            totalTriples:
              type: integer
            totalViolations:
              type: integer
            validationTime:
              type: number

  responses:
    BadRequest:
      description: Request invÃ¡lido
      content:
        application/json:
          schema:
            type: object
            properties:
              error:
                type: string
              details:
                type: string

    InternalError:
      description: Error interno del servidor
      content:
        application/json:
          schema:
            type: object
            properties:
              error:
                type: string
              traceback:
                type: string

  securitySchemes:
    BearerAuth:
      type: http
      scheme: bearer
      bearerFormat: JWT

security:
  - BearerAuth: []
```

---

#### **ImplementaciÃ³n FastAPI**

```python
# api/main.py
from fastapi import FastAPI, HTTPException, File, UploadFile
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import logging

from adapters.fuseki_adapter import FusekiAdapter
from adapters.milvus_adapter import MilvusAdapter
from adapters.ollama_adapter import OllamaAdapter
from services.hybrid_query_service import HybridQueryService
from services.validation_service import ValidationService

# ConfiguraciÃ³n
app = FastAPI(
    title="AOEM Agent API",
    version="1.0.0",
    description="RESTful API for AOEM stack"
)

# Inicializar adapters
fuseki = FusekiAdapter("http://localhost:3030/aoem/sparql")
milvus = MilvusAdapter("localhost", 19530)
ollama = OllamaAdapter("http://localhost:11434")

# Servicios
hybrid_service = HybridQueryService(fuseki, milvus, ollama)
validation_service = ValidationService(fuseki)

# Request/Response Models
class SparqlQueryRequest(BaseModel):
    query: str = Field(..., description="SPARQL query vÃ¡lido")
    timeout: int = Field(30, ge=1, le=300)

class HybridQueryRequest(BaseModel):
    question: str = Field(..., min_length=3, max_length=500)
    filters: Optional[Dict[str, Any]] = None
    topK: int = Field(10, ge=1, le=100)
    sparqlWeight: float = Field(0.6, ge=0.0, le=1.0)

class EmbeddingsRequest(BaseModel):
    texts: List[str] = Field(..., max_items=1000)
    model: str = Field("nomic-embed-text")

# Endpoints
@app.post("/api/v1/query/sparql")
async def execute_sparql(request: SparqlQueryRequest):
    """Ejecuta query SPARQL contra Fuseki."""
    try:
        results = fuseki.query(request.query)
        return JSONResponse(content=results)
    except Exception as e:
        logging.error(f"SPARQL error: {e}")
        raise HTTPException(status_code=400, detail=str(e))

@app.post("/api/v1/query/hybrid")
async def execute_hybrid(request: HybridQueryRequest):
    """Ejecuta query hÃ­brido (SPARQL + Vector)."""
    try:
        results = await hybrid_service.query(
            question=request.question,
            filters=request.filters,
            top_k=request.topK,
            sparql_weight=request.sparqlWeight
        )
        return JSONResponse(content=results)
    except Exception as e:
        logging.error(f"Hybrid query error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/embeddings/generate")
async def generate_embeddings(request: EmbeddingsRequest):
    """Genera embeddings usando Ollama."""
    try:
        embeddings = []
        for text in request.texts:
            emb = ollama.embed_text(text)
            embeddings.append(emb)

        return {
            "embeddings": embeddings,
            "model": request.model,
            "dimensionality": len(embeddings[0]) if embeddings else 0
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/embeddings/search")
async def vector_search(request: dict):
    """BÃºsqueda por similitud vectorial."""
    try:
        query_text = request["query"]
        top_k = request.get("topK", 10)

        # Generar embedding
        query_emb = ollama.embed_text(query_text)

        # Buscar en Milvus
        results = milvus.search_similar(query_emb, top_k)

        return {"results": results, "total": len(results)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/validation/shacl")
async def validate_shacl(
    ontology: UploadFile = File(...),
    shapes: UploadFile = File(...),
    inference: str = "rdfs"
):
    """Valida ontologÃ­a contra shapes SHACL."""
    try:
        ontology_content = await ontology.read()
        shapes_content = await shapes.read()

        report = validation_service.validate(
            ontology_content.decode('utf-8'),
            shapes_content.decode('utf-8'),
            inference=inference
        )

        return JSONResponse(content=report)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """Health check endpoint."""
    status = {
        "fuseki": fuseki.is_alive(),
        "milvus": milvus.is_alive(),
        "ollama": ollama.is_alive()
    }

    all_healthy = all(status.values())

    return JSONResponse(
        content={"status": "healthy" if all_healthy else "degraded", "services": status},
        status_code=200 if all_healthy else 503
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
```

**Testing con curl**:
```bash
# 1. Health check
curl http://localhost:8000/health

# 2. SPARQL query
curl -X POST http://localhost:8000/api/v1/query/sparql \
  -H "Content-Type: application/json" \
  -d '{
    "query": "SELECT (COUNT(*) as ?count) WHERE { ?s ?p ?o }"
  }'

# 3. Hybrid query
curl -X POST http://localhost:8000/api/v1/query/hybrid \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Libros de fantasÃ­a",
    "topK": 5,
    "sparqlWeight": 0.7
  }'

# 4. Generate embeddings
curl -X POST http://localhost:8000/api/v1/embeddings/generate \
  -H "Content-Type: application/json" \
  -d '{
    "texts": ["Knowledge graph", "Ontology engineering"]
  }'
```

---

### 6.4.3 Esquemas de ValidaciÃ³n JSON

#### **JSON Schema para Metadatos de OntologÃ­a**

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://aoem.example.com/schemas/ontology-metadata.json",
  "title": "AOEM Ontology Metadata",
  "description": "Esquema de validaciÃ³n para metadatos de ontologÃ­as en AOEM",
  "type": "object",
  "required": ["uri", "title", "version", "authors"],
  "properties": {
    "uri": {
      "type": "string",
      "format": "uri",
      "description": "URI Ãºnico de la ontologÃ­a",
      "pattern": "^https?://",
      "examples": ["http://purl.org/ontology/bibo/"]
    },
    "title": {
      "type": "string",
      "minLength": 3,
      "maxLength": 200,
      "description": "TÃ­tulo descriptivo",
      "examples": ["Bibliographic Ontology"]
    },
    "version": {
      "type": "string",
      "pattern": "^\\d+\\.\\d+\\.\\d+$",
      "description": "Semantic versioning",
      "examples": ["1.0.0", "2.1.3"]
    },
    "authors": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["name", "email"],
        "properties": {
          "name": {"type": "string", "minLength": 2},
          "email": {"type": "string", "format": "email"},
          "orcid": {
            "type": "string",
            "pattern": "^\\d{4}-\\d{4}-\\d{4}-\\d{3}[0-9X]$"
          }
        }
      }
    },
    "license": {
      "type": "string",
      "enum": ["CC-BY-4.0", "MIT", "Apache-2.0", "GPL-3.0"],
      "description": "Licencia de uso"
    },
    "description": {
      "type": "string",
      "minLength": 20,
      "maxLength": 5000,
      "description": "DescripciÃ³n detallada en Markdown"
    },
    "keywords": {
      "type": "array",
      "items": {"type": "string"},
      "minItems": 3,
      "maxItems": 20,
      "uniqueItems": true
    },
    "domains": {
      "type": "array",
      "items": {
        "type": "string",
        "enum": [
          "biomedical",
          "library-science",
          "finance",
          "agriculture",
          "education",
          "government",
          "general"
        ]
      }
    },
    "imports": {
      "type": "array",
      "items": {
        "type": "string",
        "format": "uri"
      },
      "description": "OntologÃ­as importadas"
    },
    "status": {
      "type": "string",
      "enum": ["draft", "testing", "stable", "deprecated"],
      "default": "draft"
    },
    "createdDate": {
      "type": "string",
      "format": "date"
    },
    "modifiedDate": {
      "type": "string",
      "format": "date-time"
    },
    "statistics": {
      "type": "object",
      "properties": {
        "classCount": {"type": "integer", "minimum": 0},
        "propertyCount": {"type": "integer", "minimum": 0},
        "individualCount": {"type": "integer", "minimum": 0},
        "axiomCount": {"type": "integer", "minimum": 0}
      }
    }
  }
}
```

**Ejemplo vÃ¡lido**:
```json
{
  "uri": "http://purl.org/ontology/bibo/",
  "title": "Bibliographic Ontology",
  "version": "1.3.0",
  "authors": [
    {
      "name": "Bruce D'Arcus",
      "email": "bdarcus@gmail.com",
      "orcid": "0000-0002-1234-5678"
    }
  ],
  "license": "CC-BY-4.0",
  "description": "The Bibliographic Ontology provides main concepts and properties for describing citations and bibliographic references.",
  "keywords": ["bibliography", "citation", "publication", "rdf", "ontology"],
  "domains": ["library-science", "general"],
  "imports": [
    "http://xmlns.com/foaf/0.1/",
    "http://purl.org/dc/terms/"
  ],
  "status": "stable",
  "createdDate": "2009-04-01",
  "modifiedDate": "2020-11-15T10:30:00Z",
  "statistics": {
    "classCount": 45,
    "propertyCount": 87,
    "individualCount": 0,
    "axiomCount": 523
  }
}
```

**ValidaciÃ³n en Python**:
```python
import json
import jsonschema

def validate_ontology_metadata(metadata: dict) -> tuple[bool, list]:
    """Valida metadatos de ontologÃ­a contra JSON Schema.

    Returns:
        (is_valid, errors)
    """
    with open('schemas/ontology-metadata.json') as f:
        schema = json.load(f)

    try:
        jsonschema.validate(instance=metadata, schema=schema)
        return (True, [])
    except jsonschema.ValidationError as e:
        return (False, [e.message])
```

---

#### **JSON Schema para ConfiguraciÃ³n de Agentes**

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://aoem.example.com/schemas/agent-config.json",
  "title": "AOEM Agent Configuration",
  "type": "object",
  "required": ["agentId", "sparqlEndpoint", "vectorStore", "llm"],
  "properties": {
    "agentId": {
      "type": "string",
      "pattern": "^[a-z0-9-]+$",
      "description": "Identificador Ãºnico del agente"
    },
    "sparqlEndpoint": {
      "type": "object",
      "required": ["url"],
      "properties": {
        "url": {"type": "string", "format": "uri"},
        "dataset": {"type": "string", "default": "aoem"},
        "timeout": {"type": "integer", "minimum": 5, "maximum": 300, "default": 30},
        "auth": {
          "type": "object",
          "properties": {
            "username": {"type": "string"},
            "password": {"type": "string", "minLength": 8}
          }
        }
      }
    },
    "vectorStore": {
      "type": "object",
      "required": ["type", "host"],
      "properties": {
        "type": {"type": "string", "enum": ["milvus", "faiss", "weaviate"]},
        "host": {"type": "string"},
        "port": {"type": "integer", "minimum": 1, "maximum": 65535},
        "collection": {"type": "string", "default": "aoem_embeddings"},
        "metric": {"type": "string", "enum": ["COSINE", "L2", "IP"], "default": "COSINE"}
      }
    },
    "llm": {
      "type": "object",
      "required": ["provider", "model"],
      "properties": {
        "provider": {"type": "string", "enum": ["ollama", "openai", "anthropic"]},
        "model": {"type": "string"},
        "baseUrl": {"type": "string", "format": "uri"},
        "apiKey": {"type": "string", "minLength": 20},
        "temperature": {"type": "number", "minimum": 0.0, "maximum": 2.0, "default": 0.7},
        "maxTokens": {"type": "integer", "minimum": 100, "maximum": 32000, "default": 2000}
      }
    },
    "tools": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["name", "type"],
        "properties": {
          "name": {"type": "string"},
          "type": {"type": "string", "enum": ["sparql", "vector_search", "custom"]},
          "enabled": {"type": "boolean", "default": true},
          "config": {"type": "object"}
        }
      }
    },
    "hybridSearch": {
      "type": "object",
      "properties": {
        "enabled": {"type": "boolean", "default": true},
        "sparqlWeight": {"type": "number", "minimum": 0.0, "maximum": 1.0, "default": 0.6},
        "vectorWeight": {"type": "number", "minimum": 0.0, "maximum": 1.0, "default": 0.4},
        "topK": {"type": "integer", "minimum": 1, "maximum": 100, "default": 10}
      }
    }
  }
}
```

**Ejemplo de configuraciÃ³n**:
```json
{
  "agentId": "aoem-hybrid-agent-v1",
  "sparqlEndpoint": {
    "url": "http://localhost:3030/aoem/sparql",
    "dataset": "aoem",
    "timeout": 30
  },
  "vectorStore": {
    "type": "milvus",
    "host": "localhost",
    "port": 19530,
    "collection": "aoem_embeddings",
    "metric": "COSINE"
  },
  "llm": {
    "provider": "ollama",
    "model": "qwen2.5:7b",
    "baseUrl": "http://localhost:11434",
    "temperature": 0.3,
    "maxTokens": 2000
  },
  "tools": [
    {
      "name": "sparql_query",
      "type": "sparql",
      "enabled": true
    },
    {
      "name": "vector_search",
      "type": "vector_search",
      "enabled": true
    }
  ],
  "hybridSearch": {
    "enabled": true,
    "sparqlWeight": 0.6,
    "vectorWeight": 0.4,
    "topK": 10
  }
}
```

---

### 6.4.4 Contratos de Testing

#### **Pytest Fixtures para Contratos**

```python
# tests/conftest.py
import pytest
from typing import Generator
from adapters.fuseki_adapter import FusekiAdapter
from adapters.milvus_adapter import MilvusAdapter

@pytest.fixture(scope="session")
def fuseki_endpoint() -> str:
    """Endpoint de Fuseki para testing."""
    return "http://localhost:3030/aoem-test/sparql"

@pytest.fixture(scope="session")
def fuseki_client(fuseki_endpoint: str) -> Generator[FusekiAdapter, None, None]:
    """Cliente Fuseki configurado."""
    client = FusekiAdapter(fuseki_endpoint)

    # Setup: cargar datos de prueba
    with open('tests/fixtures/bibo-sample.ttl', 'r') as f:
        client.insert_triples(f.read(), format='turtle')

    yield client

    # Teardown: limpiar dataset
    client.clear_dataset()

@pytest.fixture(scope="session")
def milvus_client() -> Generator[MilvusAdapter, None, None]:
    """Cliente Milvus configurado."""
    client = MilvusAdapter("localhost", 19530)

    # Setup: crear colecciÃ³n de prueba
    client.create_collection("test_embeddings", dim=768)

    yield client

    # Teardown: eliminar colecciÃ³n
    client.drop_collection("test_embeddings")

# Contract tests
def test_fuseki_query_contract(fuseki_client):
    """Verifica que FusekiAdapter cumple contrato SPARQLEndpoint."""
    # ACT
    results = fuseki_client.query("SELECT (COUNT(*) as ?count) WHERE { ?s ?p ?o }")

    # ASSERT
    assert "results" in results
    assert "bindings" in results["results"]
    assert len(results["results"]["bindings"]) > 0
    assert "count" in results["results"]["bindings"][0]

def test_milvus_search_contract(milvus_client):
    """Verifica que MilvusAdapter cumple contrato VectorStore."""
    import numpy as np

    # ARRANGE
    test_vectors = np.random.rand(10, 768).astype(np.float32)
    ids = [f"uri{i}" for i in range(10)]
    metadata = [{"type": "Book"} for _ in range(10)]

    # ACT
    insert_count = milvus_client.insert(ids, test_vectors, metadata)
    query_vec = np.random.rand(768).astype(np.float32)
    results = milvus_client.search_similar(query_vec, top_k=5)

    # ASSERT
    assert insert_count == 10
    assert len(results) == 5
    assert all("id" in r and "score" in r for r in results)
```

---

**Estado**: SecciÃ³n 6.4 completada con 15 queries SPARQL, API REST completa (OpenAPI + FastAPI), y esquemas de validaciÃ³n JSON.

---

## 6.5 InstalaciÃ³n Fase 1: Infraestructura Base

**Objetivo**: Desplegar y verificar los 3 contenedores principales del stack validado (Fuseki 5.0.0 + Neo4j 5.26 + Ollama nomic-embed-text).

**Tiempo estimado**: 30-45 minutos (primera vez, incluye downloads).

**RAM requerida**: ~14.5GB de 32GB disponibles (Fuseki 2GB + Neo4j 12GB + Ollama 500MB).

---

### 6.5.1 Docker Compose Completo

Crea el archivo `docker-compose.yml` en el directorio raÃ­z de tu proyecto:

```yaml
version: '3.8'

services:
  # ==========================================
  # FUSEKI 5.0.0 - RDF Triple Store (Source of Truth)
  # ==========================================
  fuseki:
    image: stain/jena-fuseki:5.0.0
    container_name: aoem-fuseki
    ports:
      - "3030:3030"
    environment:
      ADMIN_PASSWORD: aoem2024
      JVM_ARGS: "-Xmx2g -Xms512m"
    volumes:
      - fuseki-data:/fuseki
      - ./ontologies:/ontologies:ro  # Mount read-only ontologies
    networks:
      - aoem-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3030/$/ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  # ==========================================
  # NEO4J 5.26 - Graph DB + Vector Store (GenAI-Stack)
  # ==========================================
  neo4j:
    image: neo4j:5.26
    container_name: aoem-neo4j
    ports:
      - "7474:7474"  # Browser UI
      - "7687:7687"  # Bolt protocol
    environment:
      NEO4J_AUTH: neo4j/aoem2024
      # Plugins (APOC, GDS, n10s)
      NEO4J_PLUGINS: '["apoc", "graph-data-science", "n10s"]'
      NEO4J_dbms_security_procedures_unrestricted: apoc.*,gds.*,n10s.*

      # Memory allocation (12GB heap for vector search)
      NEO4J_dbms_memory_heap_max__size: 8G
      NEO4J_dbms_memory_heap_initial__size: 2G
      NEO4J_dbms_memory_pagecache_size: 4G

      # Vector search configuration (HNSW)
      NEO4J_db_index_vector_enabled: "true"
      NEO4J_db_index_vector_dimensions: 768  # nomic-embed-text
      NEO4J_db_index_vector_similarity__function: cosine

      # Performance tuning
      NEO4J_dbms_transaction_timeout: 30s
      NEO4J_dbms_locks_acquisition__timeout: 15s
    volumes:
      - neo4j-data:/data
      - neo4j-logs:/logs
      - neo4j-plugins:/plugins
      - neo4j-import:/var/lib/neo4j/import
    networks:
      - aoem-network
    healthcheck:
      test: ["CMD-SHELL", "cypher-shell -u neo4j -p aoem2024 'RETURN 1' || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # ==========================================
  # OLLAMA - Local Embeddings (nomic-embed-text 768d)
  # ==========================================
  ollama:
    image: ollama/ollama:latest
    container_name: aoem-ollama
    ports:
      - "11434:11434"
    environment:
      OLLAMA_HOST: "0.0.0.0"
      OLLAMA_ORIGINS: "*"
      OLLAMA_MAX_LOADED_MODELS: 1  # Solo embeddings
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - aoem-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 20s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G  # 500MB model + 1.5GB buffer
        reservations:
          memory: 512M

# ==========================================
# NETWORKS
# ==========================================
networks:
  aoem-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16

# ==========================================
# VOLUMES (Persistent Data)
# ==========================================
volumes:
  fuseki-data:
    name: aoem-fuseki-data
  neo4j-data:
    name: aoem-neo4j-data
  neo4j-logs:
    name: aoem-neo4j-logs
  neo4j-plugins:
    name: aoem-neo4j-plugins
  neo4j-import:
    name: aoem-neo4j-import
  ollama-data:
    name: aoem-ollama-data
```

---

### 6.5.2 Archivo de Variables de Entorno

Crea `.env` en el mismo directorio:

```bash
# ==================================================
# AOEM 2.0 - Stack HÃ­brido Validado
# Fuseki 5.0.0 + Neo4j 5.26 + Ollama + API Externa
# ==================================================

# ===== LLM APIs (Reasoning - Externa) =====
OPENAI_API_KEY=sk-proj-XXXXXXXXXXXXXXXXXXXXXXXX
ANTHROPIC_API_KEY=sk-ant-XXXXXXXXXXXXXXXXXXXXXXXX
OPENAI_MODEL=gpt-4o-mini
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# ===== Ollama (Embeddings - Local) =====
OLLAMA_BASE_URL=http://localhost:11434
EMBEDDING_MODEL=nomic-embed-text
EMBEDDING_DIMENSIONS=768

# ===== Neo4j 5.26 (Graph + Vector) =====
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=aoem2024
NEO4J_DATABASE=neo4j

# ===== Fuseki 5.0.0 (RDF Source of Truth) =====
FUSEKI_URL=http://localhost:3030
FUSEKI_DATASET=aoem
FUSEKI_ADMIN_PASSWORD=aoem2024

# ===== Application Settings =====
LOG_LEVEL=INFO
PYTHONUNBUFFERED=1
```

**âš ï¸ Seguridad**: AÃ±ade `.env` a tu `.gitignore`:

```bash
echo ".env" >> .gitignore
```

---

### 6.5.3 Scripts de InicializaciÃ³n

#### Script 1: Levantar Infraestructura

Crea `scripts/start-infrastructure.sh` (Linux/Mac) o `scripts/start-infrastructure.ps1` (Windows):

**Windows PowerShell** (`scripts/start-infrastructure.ps1`):

```powershell
# start-infrastructure.ps1
# Inicia el stack completo AOEM 2.0

Write-Host "ğŸš€ Iniciando AOEM 2.0 Stack (Fuseki + Neo4j 5.26 + Ollama)..." -ForegroundColor Cyan

# 1. Verificar Docker
if (-not (Get-Command docker -ErrorAction SilentlyContinue)) {
    Write-Host "âŒ Docker no estÃ¡ instalado. Instala Docker Desktop." -ForegroundColor Red
    exit 1
}

# 2. Verificar .env
if (-not (Test-Path .env)) {
    Write-Host "âš ï¸  Archivo .env no encontrado. Creando desde plantilla..." -ForegroundColor Yellow
    Copy-Item .env.example .env
    Write-Host "ğŸ“ Edita .env con tus API keys antes de continuar." -ForegroundColor Yellow
    exit 1
}

# 3. Levantar containers
Write-Host "ğŸ“¦ Levantando contenedores..." -ForegroundColor Green
docker-compose up -d

# 4. Esperar health checks
Write-Host "â³ Esperando health checks (60 segundos)..." -ForegroundColor Yellow
Start-Sleep -Seconds 60

# 5. Verificar estado
Write-Host "`nğŸ“Š Estado de los contenedores:" -ForegroundColor Cyan
docker-compose ps

# 6. Descargar modelo Ollama
Write-Host "`nğŸ¤– Descargando modelo nomic-embed-text (768d, ~500MB)..." -ForegroundColor Green
docker exec aoem-ollama ollama pull nomic-embed-text

# 7. Verificar conectividad
Write-Host "`nâœ… Verificando conectividad..." -ForegroundColor Cyan

# Fuseki
$fusekiStatus = (Invoke-WebRequest -Uri http://localhost:3030/$/ping -UseBasicParsing -ErrorAction SilentlyContinue).StatusCode
if ($fusekiStatus -eq 200) {
    Write-Host "âœ… Fuseki 5.0.0: http://localhost:3030" -ForegroundColor Green
} else {
    Write-Host "âŒ Fuseki no responde" -ForegroundColor Red
}

# Neo4j
try {
    $neo4jStatus = (Invoke-WebRequest -Uri http://localhost:7474 -UseBasicParsing -ErrorAction SilentlyContinue).StatusCode
    if ($neo4jStatus -eq 200) {
        Write-Host "âœ… Neo4j 5.26 Browser: http://localhost:7474" -ForegroundColor Green
    }
} catch {
    Write-Host "âŒ Neo4j Browser no responde" -ForegroundColor Red
}

# Ollama
$ollamaStatus = (Invoke-WebRequest -Uri http://localhost:11434/api/tags -UseBasicParsing -ErrorAction SilentlyContinue).StatusCode
if ($ollamaStatus -eq 200) {
    Write-Host "âœ… Ollama API: http://localhost:11434" -ForegroundColor Green
} else {
    Write-Host "âŒ Ollama no responde" -ForegroundColor Red
}

Write-Host "`nğŸ‰ Stack listo. ContinÃºa con la Fase 2 (PoblaciÃ³n de datos)." -ForegroundColor Cyan
```

**Linux/Mac** (`scripts/start-infrastructure.sh`):

```bash
#!/bin/bash
# start-infrastructure.sh

set -e

echo "ğŸš€ Iniciando AOEM 2.0 Stack (Fuseki + Neo4j 5.26 + Ollama)..."

# 1. Verificar Docker
if ! command -v docker &> /dev/null; then
    echo "âŒ Docker no estÃ¡ instalado."
    exit 1
fi

# 2. Verificar .env
if [ ! -f .env ]; then
    echo "âš ï¸  Archivo .env no encontrado."
    cp .env.example .env
    echo "ğŸ“ Edita .env con tus API keys."
    exit 1
fi

# 3. Levantar containers
echo "ğŸ“¦ Levantando contenedores..."
docker-compose up -d

# 4. Esperar health checks
echo "â³ Esperando health checks (60 segundos)..."
sleep 60

# 5. Verificar estado
echo -e "\nğŸ“Š Estado de los contenedores:"
docker-compose ps

# 6. Descargar modelo Ollama
echo -e "\nğŸ¤– Descargando modelo nomic-embed-text..."
docker exec aoem-ollama ollama pull nomic-embed-text

# 7. Verificar conectividad
echo -e "\nâœ… Verificando conectividad..."

if curl -s http://localhost:3030/$/ping > /dev/null; then
    echo "âœ… Fuseki 5.0.0: http://localhost:3030"
else
    echo "âŒ Fuseki no responde"
fi

if curl -s http://localhost:7474 > /dev/null; then
    echo "âœ… Neo4j 5.26 Browser: http://localhost:7474"
else
    echo "âŒ Neo4j Browser no responde"
fi

if curl -s http://localhost:11434/api/tags > /dev/null; then
    echo "âœ… Ollama API: http://localhost:11434"
else
    echo "âŒ Ollama no responde"
fi

echo -e "\nğŸ‰ Stack listo."
```

**Permisos** (Linux/Mac):

```bash
chmod +x scripts/start-infrastructure.sh
```

---

### 6.5.4 VerificaciÃ³n Manual de Servicios

#### A. Fuseki 5.0.0

**1. Browser UI**: http://localhost:3030

**2. Crear dataset** (si no existe):

```bash
curl -X POST http://localhost:3030/$/datasets \
  -u admin:aoem2024 \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "dbName=aoem&dbType=tdb2"
```

**3. Verificar con Python**:

```python
from SPARQLWrapper import SPARQLWrapper

sparql = SPARQLWrapper("http://localhost:3030/aoem/query")
sparql.setQuery("SELECT (COUNT(*) AS ?count) WHERE { ?s ?p ?o }")
sparql.setReturnFormat("json")

results = sparql.query().convert()
print(f"Triples en Fuseki: {results['results']['bindings'][0]['count']['value']}")
# Expected: 0 (vacÃ­o en primera instalaciÃ³n)
```

---

#### B. Neo4j 5.26

**1. Browser UI**: http://localhost:7474

- **Usuario**: neo4j
- **Password**: aoem2024

**2. Verificar plugins** (ejecutar en Neo4j Browser):

```cypher
// Verificar versiÃ³n
CALL dbms.components() YIELD name, versions, edition
UNWIND versions AS version
RETURN name, version, edition;

// Expected: Neo4j Kernel 5.26.0 community

// Verificar plugins instalados
CALL dbms.procedures()
WHERE name STARTS WITH 'apoc' OR name STARTS WITH 'gds' OR name STARTS WITH 'n10s'
RETURN count(*) AS plugin_count;

// Expected: >300 procedures (APOC + GDS + n10s)
```

**3. Crear Ã­ndice vectorial** (HNSW para embeddings):

```cypher
// Crear constraint unique para URIs
CREATE CONSTRAINT uri_unique IF NOT EXISTS
FOR (n:Resource) REQUIRE n.uri IS UNIQUE;

// Crear Ã­ndice vectorial (768 dimensiones, nomic-embed-text)
CREATE VECTOR INDEX resource_embeddings IF NOT EXISTS
FOR (n:Resource)
ON n.embedding
OPTIONS {
  indexConfig: {
    `vector.dimensions`: 768,
    `vector.similarity_function`: 'cosine',
    `vector.quantization.enabled`: true
  }
};

// Verificar Ã­ndice
SHOW INDEXES YIELD name, type, state
WHERE type = 'VECTOR'
RETURN name, type, state;

// Expected: resource_embeddings | VECTOR | ONLINE
```

**4. Verificar con Python**:

```python
from neo4j import GraphDatabase

driver = GraphDatabase.driver(
    "bolt://localhost:7687",
    auth=("neo4j", "aoem2024")
)

with driver.session() as session:
    # Verificar versiÃ³n
    result = session.run("""
        CALL dbms.components() YIELD name, versions
        UNWIND versions AS version
        RETURN name, version
    """)
    for record in result:
        print(f"{record['name']}: {record['version']}")

    # Verificar Ã­ndices vectoriales
    result = session.run("""
        SHOW INDEXES YIELD name, type
        WHERE type = 'VECTOR'
        RETURN count(*) AS vector_indexes
    """)
    count = result.single()["vector_indexes"]
    print(f"Ãndices vectoriales: {count}")

driver.close()
```

---

#### C. Ollama (nomic-embed-text)

**1. Verificar modelos instalados**:

```bash
curl http://localhost:11434/api/tags | jq .
```

**Expected output**:

```json
{
  "models": [
    {
      "name": "nomic-embed-text:latest",
      "size": 274290688,
      "digest": "...",
      "details": {
        "format": "gguf",
        "family": "nomic-bert",
        "parameter_size": "137M",
        "quantization_level": "Q4_0"
      }
    }
  ]
}
```

**2. Test de embeddings**:

```bash
curl http://localhost:11434/api/embeddings -d '{
  "model": "nomic-embed-text",
  "prompt": "The quick brown fox jumps over the lazy dog"
}'
```

**Expected**: JSON con array de 768 floats.

**3. Verificar con Python**:

```python
import ollama

# Test embedding generation
response = ollama.embeddings(
    model="nomic-embed-text",
    prompt="Ontology engineering best practices"
)

embedding = response["embedding"]
print(f"Embedding dimensions: {len(embedding)}")  # Expected: 768
print(f"Sample values: {embedding[:5]}")
print(f"Vector norm: {sum(x**2 for x in embedding)**0.5:.4f}")
```

---

### 6.5.5 Health Check Automatizado

Crea `scripts/health-check.py`:

```python
#!/usr/bin/env python3
"""
health-check.py - Verifica el estado del stack AOEM 2.0
"""

import sys
import requests
from neo4j import GraphDatabase
from SPARQLWrapper import SPARQLWrapper
import ollama

def check_fuseki():
    """Verifica Fuseki 5.0.0"""
    try:
        response = requests.get("http://localhost:3030/$/ping", timeout=5)
        if response.status_code == 200:
            print("âœ… Fuseki 5.0.0: OK")
            return True
    except Exception as e:
        print(f"âŒ Fuseki: {e}")
    return False

def check_neo4j():
    """Verifica Neo4j 5.26"""
    try:
        driver = GraphDatabase.driver(
            "bolt://localhost:7687",
            auth=("neo4j", "aoem2024")
        )
        with driver.session() as session:
            result = session.run("RETURN 1 AS num")
            if result.single()["num"] == 1:
                print("âœ… Neo4j 5.26: OK")

                # Verificar Ã­ndice vectorial
                result = session.run("""
                    SHOW INDEXES YIELD name, type, state
                    WHERE type = 'VECTOR' AND state = 'ONLINE'
                    RETURN count(*) AS count
                """)
                vec_count = result.single()["count"]
                if vec_count > 0:
                    print(f"   âœ… {vec_count} Ã­ndice(s) vectorial(es) activo(s)")
                else:
                    print("   âš ï¸  No hay Ã­ndices vectoriales (ejecuta setup en 6.5.4.B)")

                driver.close()
                return True
    except Exception as e:
        print(f"âŒ Neo4j: {e}")
    return False

def check_ollama():
    """Verifica Ollama + nomic-embed-text"""
    try:
        # Verificar API
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            nomic_installed = any("nomic-embed-text" in m["name"] for m in models)

            if nomic_installed:
                print("âœ… Ollama: OK")
                print("   âœ… nomic-embed-text instalado")

                # Test embedding
                result = ollama.embeddings(
                    model="nomic-embed-text",
                    prompt="test"
                )
                if len(result["embedding"]) == 768:
                    print(f"   âœ… Embeddings funcionando (768d)")
                    return True
            else:
                print("âš ï¸  Ollama OK, pero nomic-embed-text NO instalado")
                print("   Ejecuta: docker exec aoem-ollama ollama pull nomic-embed-text")
    except Exception as e:
        print(f"âŒ Ollama: {e}")
    return False

def main():
    print("ğŸ” Health Check - AOEM 2.0 Stack\n")

    results = {
        "fuseki": check_fuseki(),
        "neo4j": check_neo4j(),
        "ollama": check_ollama()
    }

    print("\n" + "="*50)
    if all(results.values()):
        print("âœ… Todos los servicios funcionando correctamente")
        return 0
    else:
        failed = [k for k, v in results.items() if not v]
        print(f"âŒ Servicios fallidos: {', '.join(failed)}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
```

**Ejecutar**:

```bash
python scripts/health-check.py
```

---

### 6.5.6 Criterios de Ã‰xito (Fase 1)

âœ… **Infraestructura desplegada**:
- [ ] 3 contenedores corriendo: `docker-compose ps` muestra 3 servicios "Up (healthy)"
- [ ] Fuseki UI accesible en http://localhost:3030 (admin/aoem2024)
- [ ] Neo4j Browser accesible en http://localhost:7474 (neo4j/aoem2024)
- [ ] Ollama API responde en http://localhost:11434/api/tags

âœ… **ConfiguraciÃ³n validada**:
- [ ] Fuseki: Dataset `aoem` creado (TDB2)
- [ ] Neo4j: Ãndice vectorial `resource_embeddings` en estado `ONLINE` (768d, cosine)
- [ ] Neo4j: Plugins APOC + GDS + n10s cargados (>300 procedures)
- [ ] Ollama: Modelo `nomic-embed-text` descargado (~500MB)

âœ… **Health check exitoso**:
- [ ] `python scripts/health-check.py` devuelve exit code 0
- [ ] Test embeddings: 768 dimensiones generadas en <100ms
- [ ] RAM usage: ~14.5GB (verificar con `docker stats`)

âœ… **Logs sin errores crÃ­ticos**:

```bash
# Verificar logs
docker-compose logs fuseki --tail=50
docker-compose logs neo4j --tail=50
docker-compose logs ollama --tail=50
```

---

### 6.5.7 Troubleshooting ComÃºn

#### Problema 1: Neo4j no inicia (OOMKilled)

**SÃ­ntoma**: Container `aoem-neo4j` se reinicia constantemente.

**Causa**: Insuficiente RAM (heap size 8G + pagecache 4G = 12GB).

**SoluciÃ³n**: Reducir heap size en `docker-compose.yml`:

```yaml
NEO4J_dbms_memory_heap_max__size: 4G  # Reducir de 8G a 4G
NEO4J_dbms_memory_pagecache_size: 2G  # Reducir de 4G a 2G
```

---

#### Problema 2: Ollama pull falla (timeout)

**SÃ­ntoma**: `ollama pull nomic-embed-text` se cuelga.

**SoluciÃ³n**: Pull manual fuera del container:

```bash
# OpciÃ³n 1: Aumentar timeout
docker exec aoem-ollama sh -c "OLLAMA_REQUEST_TIMEOUT=300 ollama pull nomic-embed-text"

# OpciÃ³n 2: Pre-download en host (si tienes Ollama instalado localmente)
ollama pull nomic-embed-text
# Luego copiar modelo al container (requiere volumen compartido)
```

---

#### Problema 3: Fuseki dataset no se crea

**SÃ­ntoma**: 404 en http://localhost:3030/aoem/query

**SoluciÃ³n**: Crear dataset manualmente:

```bash
# Via API
curl -X POST http://localhost:3030/$/datasets \
  -u admin:aoem2024 \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "dbName=aoem&dbType=tdb2"

# Via UI: http://localhost:3030 â†’ "Manage datasets" â†’ "Add new dataset"
```

---

#### Problema 4: Neo4j plugins no se cargan

**SÃ­ntoma**: `CALL apoc.help('text')` devuelve error "procedure not found".

**Causa**: Plugins no descargados correctamente en primera inicializaciÃ³n.

**SoluciÃ³n**: Forzar re-descarga:

```bash
docker-compose down -v  # âš ï¸ Borra datos
docker volume rm aoem-neo4j-plugins
docker-compose up -d neo4j
# Esperar 2-3 minutos para plugin download
docker logs aoem-neo4j --follow  # Verificar "Started" message
```

---

### 6.5.8 Comandos Ãštiles

```bash
# Ver logs en tiempo real
docker-compose logs -f

# Ver solo Neo4j
docker-compose logs -f neo4j

# Reiniciar un servicio especÃ­fico
docker-compose restart ollama

# Ver consumo de recursos
docker stats

# Acceder a shell de container
docker exec -it aoem-neo4j bash
docker exec -it aoem-ollama sh

# Backup de datos Neo4j
docker exec aoem-neo4j neo4j-admin database dump neo4j --to-path=/backups
docker cp aoem-neo4j:/backups/neo4j.dump ./backups/

# Limpiar todo (âš ï¸ destructivo)
docker-compose down -v
docker volume prune -f
```

---

**Estado**: SecciÃ³n 6.5 completada. Infraestructura base (3 containers) desplegada y validada.

---

## 6.6 InstalaciÃ³n Fase 2: PoblaciÃ³n de Datos (RDF â†’ Neo4j + Embeddings)

**Objetivo**: Cargar ontologÃ­a AOEM en Fuseki, importar a Neo4j usando n10s, y generar embeddings con Ollama para bÃºsqueda vectorial.

**Tiempo estimado**: 20-30 minutos (depende del tamaÃ±o de la ontologÃ­a).

**Prerequisitos**: SecciÃ³n 6.5 completada exitosamente.

---

### 6.6.1 Preparar OntologÃ­a AOEM

#### Estructura de Archivos Esperada

Crea el directorio `ontologies/` con tu ontologÃ­a AOEM:

```
ontologies/
â”œâ”€â”€ aoem-core.ttl           # OntologÃ­a principal (clases, propiedades)
â”œâ”€â”€ aoem-instances.ttl      # Instancias (casos de uso, herramientas)
â”œâ”€â”€ aoem-mappings.ttl       # Mappings externos (Dublin Core, SKOS)
â””â”€â”€ aoem-shacl.ttl          # Reglas de validaciÃ³n SHACL (opcional)
```

**Ejemplo mÃ­nimo** (`ontologies/aoem-core.ttl`):

```turtle
@prefix aoem: <http://aleia.com/ontology/aoem#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix dc: <http://purl.org/dc/elements/1.1/> .
@prefix skos: <http://www.w3.org/2004/02/skos/core#> .

# ==================================================
# AOEM 2.0 Core Ontology
# ==================================================

aoem:Ontology a owl:Ontology ;
    dc:title "AOEM - Adaptive Ontology Engineering Methodology"@en ;
    dc:creator "ALEIA Project" ;
    dc:date "2026-01-11" ;
    owl:versionInfo "2.0" .

# ==================================================
# Core Classes
# ==================================================

aoem:Phase a owl:Class ;
    rdfs:label "Methodological Phase"@en ;
    rdfs:comment "A phase in the AOEM methodology lifecycle"@en .

aoem:Task a owl:Class ;
    rdfs:label "Task"@en ;
    rdfs:comment "An atomic activity within a phase"@en .

aoem:Deliverable a owl:Class ;
    rdfs:label "Deliverable"@en ;
    rdfs:comment "An output artifact produced by a task"@en .

aoem:Tool a owl:Class ;
    rdfs:label "Tool"@en ;
    rdfs:comment "A software tool supporting AOEM tasks"@en .

aoem:Competency a owl:Class ;
    rdfs:label "Competency Question"@en ;
    rdfs:comment "A question the ontology must answer"@en .

# ==================================================
# Object Properties
# ==================================================

aoem:hasTask a owl:ObjectProperty ;
    rdfs:domain aoem:Phase ;
    rdfs:range aoem:Task ;
    rdfs:label "has task"@en .

aoem:produces a owl:ObjectProperty ;
    rdfs:domain aoem:Task ;
    rdfs:range aoem:Deliverable ;
    rdfs:label "produces"@en .

aoem:usesTool a owl:ObjectProperty ;
    rdfs:domain aoem:Task ;
    rdfs:range aoem:Tool ;
    rdfs:label "uses tool"@en .

aoem:answeredBy a owl:ObjectProperty ;
    rdfs:domain aoem:Competency ;
    rdfs:range aoem:Deliverable ;
    rdfs:label "answered by"@en .

aoem:dependsOn a owl:ObjectProperty ;
    rdfs:domain aoem:Task ;
    rdfs:range aoem:Task ;
    rdfs:label "depends on"@en .

# ==================================================
# Data Properties
# ==================================================

aoem:phaseNumber a owl:DatatypeProperty ;
    rdfs:domain aoem:Phase ;
    rdfs:range xsd:integer ;
    rdfs:label "phase number"@en .

aoem:priority a owl:DatatypeProperty ;
    rdfs:domain aoem:Task ;
    rdfs:range xsd:string ;
    rdfs:label "priority"@en .

aoem:estimatedDuration a owl:DatatypeProperty ;
    rdfs:domain aoem:Task ;
    rdfs:range xsd:duration ;
    rdfs:label "estimated duration"@en .

aoem:description a owl:DatatypeProperty ;
    rdfs:label "description"@en ;
    rdfs:range xsd:string .

# ==================================================
# Instances (Core Phases)
# ==================================================

aoem:DefinePhase a aoem:Phase ;
    rdfs:label "Define"@en ;
    aoem:phaseNumber 0 ;
    aoem:description "Define scope, requirements, and competency questions"@en .

aoem:ConceivePhase a aoem:Phase ;
    rdfs:label "Conceive"@en ;
    aoem:phaseNumber 1 ;
    aoem:description "Design ontology structure and conceptual model"@en .

aoem:DesignPhase a aoem:Phase ;
    rdfs:label "Design"@en ;
    aoem:phaseNumber 2 ;
    aoem:description "Implement formal ontology in OWL/RDFS"@en .

aoem:BuildPhase a aoem:Phase ;
    rdfs:label "Build"@en ;
    aoem:phaseNumber 3 ;
    aoem:description "Populate ontology with instances and data"@en .

aoem:DeployPhase a aoem:Phase ;
    rdfs:label "Deploy"@en ;
    aoem:phaseNumber 4 ;
    aoem:description "Deploy to production environment"@en .

aoem:EvaluatePhase a aoem:Phase ;
    rdfs:label "Evaluate"@en ;
    aoem:phaseNumber 5 ;
    aoem:description "Validate against competency questions"@en .

aoem:LessonPhase a aoem:Phase ;
    rdfs:label "Lesson"@en ;
    aoem:phaseNumber 6 ;
    aoem:description "Extract lessons learned for continuous improvement"@en .
```

---

### 6.6.2 Cargar OntologÃ­a en Fuseki

#### MÃ©todo 1: Via API (Recomendado para CI/CD)

Crea el script `scripts/load-to-fuseki.py`:

```python
#!/usr/bin/env python3
"""
load-to-fuseki.py - Carga ontologÃ­a AOEM en Fuseki
"""

import os
import sys
from pathlib import Path
from SPARQLWrapper import SPARQLWrapper, POST, DIGEST
import requests

FUSEKI_URL = os.getenv("FUSEKI_URL", "http://localhost:3030")
FUSEKI_DATASET = os.getenv("FUSEKI_DATASET", "aoem")
FUSEKI_ADMIN_PASSWORD = os.getenv("FUSEKI_ADMIN_PASSWORD", "aoem2024")

def create_dataset():
    """Crea dataset si no existe"""
    print(f"ğŸ“¦ Creando dataset '{FUSEKI_DATASET}'...")

    response = requests.post(
        f"{FUSEKI_URL}/$/datasets",
        auth=("admin", FUSEKI_ADMIN_PASSWORD),
        data={
            "dbName": FUSEKI_DATASET,
            "dbType": "tdb2"
        }
    )

    if response.status_code in [200, 409]:  # 409 = already exists
        print(f"âœ… Dataset '{FUSEKI_DATASET}' disponible")
        return True
    else:
        print(f"âŒ Error creando dataset: {response.status_code} - {response.text}")
        return False

def load_turtle_file(file_path: Path):
    """Carga un archivo Turtle en Fuseki"""
    print(f"ğŸ“¥ Cargando {file_path.name}...")

    with open(file_path, 'r', encoding='utf-8') as f:
        ttl_content = f.read()

    # Upload via SPARQL Update endpoint
    sparql = SPARQLWrapper(f"{FUSEKI_URL}/{FUSEKI_DATASET}/update")
    sparql.setHTTPAuth(DIGEST)
    sparql.setCredentials("admin", FUSEKI_ADMIN_PASSWORD)
    sparql.setMethod(POST)

    # Construir INSERT DATA query
    query = f"""
    INSERT DATA {{
        {ttl_content}
    }}
    """

    try:
        sparql.setQuery(query)
        sparql.query()
        print(f"   âœ… {file_path.name} cargado")
        return True
    except Exception as e:
        print(f"   âŒ Error: {e}")
        return False

def count_triples():
    """Cuenta triples en Fuseki"""
    sparql = SPARQLWrapper(f"{FUSEKI_URL}/{FUSEKI_DATASET}/query")
    sparql.setQuery("SELECT (COUNT(*) AS ?count) WHERE { ?s ?p ?o }")
    sparql.setReturnFormat("json")

    try:
        results = sparql.query().convert()
        count = int(results["results"]["bindings"][0]["count"]["value"])
        print(f"ğŸ“Š Total triples en Fuseki: {count}")
        return count
    except Exception as e:
        print(f"âŒ Error contando triples: {e}")
        return 0

def main():
    print("ğŸš€ Cargando ontologÃ­a AOEM en Fuseki\n")

    # 1. Crear dataset
    if not create_dataset():
        return 1

    # 2. Buscar archivos .ttl
    ontologies_dir = Path("ontologies")
    if not ontologies_dir.exists():
        print(f"âŒ Directorio {ontologies_dir} no encontrado")
        return 1

    ttl_files = sorted(ontologies_dir.glob("*.ttl"))
    if not ttl_files:
        print(f"âŒ No se encontraron archivos .ttl en {ontologies_dir}")
        return 1

    print(f"ğŸ“ Archivos encontrados: {len(ttl_files)}\n")

    # 3. Cargar cada archivo
    success_count = 0
    for file_path in ttl_files:
        if load_turtle_file(file_path):
            success_count += 1

    print(f"\nâœ… {success_count}/{len(ttl_files)} archivos cargados\n")

    # 4. Verificar count
    total = count_triples()

    if total > 0:
        print("\nğŸ‰ OntologÃ­a AOEM cargada en Fuseki")
        return 0
    else:
        print("\nâš ï¸  No se cargaron triples")
        return 1

if __name__ == "__main__":
    sys.exit(main())
```

**Ejecutar**:

```bash
python scripts/load-to-fuseki.py
```

---

#### MÃ©todo 2: Via UI (Manual)

1. Ir a http://localhost:3030
2. Login: `admin` / `aoem2024`
3. Seleccionar dataset `aoem`
4. Ir a "Upload files"
5. Seleccionar `ontologies/aoem-core.ttl`
6. Click "Upload all"

---

### 6.6.3 Importar RDF a Neo4j con n10s

#### Setup 1: Configurar n10s Plugin

Ejecutar en Neo4j Browser (http://localhost:7474):

```cypher
// 1. Crear constraint unique para URIs
CREATE CONSTRAINT n10s_unique_uri IF NOT EXISTS
FOR (r:Resource) REQUIRE r.uri IS UNIQUE;

// 2. Inicializar n10s (graph config)
CALL n10s.graphconfig.init({
  handleVocabUris: 'MAP',
  handleMultival: 'ARRAY',
  multivalPropList: ['http://www.w3.org/2000/01/rdf-schema#label'],
  keepLangTag: true,
  handleRDFTypes: 'LABELS'
});

// 3. Verificar configuraciÃ³n
CALL n10s.graphconfig.show() YIELD param, value
RETURN param, value;

// Expected: handleVocabUris=MAP, handleRDFTypes=LABELS, etc.
```

**ExplicaciÃ³n de parÃ¡metros**:

- `handleVocabUris: 'MAP'`: Usa namespace mappings (ej: `aoem:Phase` â†’ `Phase`)
- `handleMultival: 'ARRAY'`: Propiedades multi-valor como arrays
- `multivalPropList`: Propiedades que permiten mÃºltiples valores (ej: rdfs:label en varios idiomas)
- `keepLangTag: true`: Preserva tags de idioma (`@en`, `@es`)
- `handleRDFTypes: 'LABELS'`: Convierte `rdf:type` en labels de nodos Neo4j

---

#### Setup 2: Mapear Namespaces

```cypher
// Mapear prefijos comunes
CALL n10s.nsprefixes.add('aoem', 'http://aleia.com/ontology/aoem#');
CALL n10s.nsprefixes.add('rdfs', 'http://www.w3.org/2000/01/rdf-schema#');
CALL n10s.nsprefixes.add('owl', 'http://www.w3.org/2002/07/owl#');
CALL n10s.nsprefixes.add('dc', 'http://purl.org/dc/elements/1.1/');
CALL n10s.nsprefixes.add('skos', 'http://www.w3.org/2004/02/skos/core#');

// Verificar
CALL n10s.nsprefixes.list() YIELD prefix, namespace
RETURN prefix, namespace;
```

---

#### Setup 3: Importar desde Fuseki

```cypher
// Importar ontologÃ­a completa desde Fuseki
CALL n10s.rdf.import.fetch(
  'http://fuseki:3030/aoem/data',  // Endpoint (usar hostname interno 'fuseki')
  'Turtle',                         // Format
  {
    headerParams: {
      Authorization: 'Basic YWRtaW46YW9lbTIwMjQ='  // Base64: admin:aoem2024
    }
  }
) YIELD terminationStatus, triplesLoaded, triplesParsed, namespaces
RETURN terminationStatus, triplesLoaded, triplesParsed, namespaces;

// Expected: terminationStatus=OK, triplesLoaded>0
```

**âš ï¸ Nota**: Si falla con error de conexiÃ³n, usar IP del host en lugar de hostname:

```cypher
// Alternativa: Usar IP del host (en Windows con WSL/Docker Desktop)
CALL n10s.rdf.import.fetch(
  'http://host.docker.internal:3030/aoem/data',
  'Turtle'
) YIELD terminationStatus, triplesLoaded
RETURN terminationStatus, triplesLoaded;
```

---

#### Verificar ImportaciÃ³n

```cypher
// 1. Contar nodos importados
MATCH (n:Resource)
RETURN count(n) AS total_resources;

// 2. Ver labels Ãºnicos (tipos RDF)
CALL db.labels() YIELD label
WHERE label <> 'Resource'
RETURN label, count(label) AS count
ORDER BY count DESC;

// Expected: Phase, Task, Deliverable, Tool, Competency, etc.

// 3. Ver ejemplo de nodo Phase
MATCH (p:Phase)
RETURN p.uri, p.rdfs__label, p.aoem__phaseNumber, p.aoem__description
LIMIT 5;

// 4. Ver relaciones importadas
MATCH (s)-[r]->(o)
RETURN type(r) AS relationship, count(*) AS count
ORDER BY count DESC
LIMIT 10;
```

---

### 6.6.4 Generar Embeddings con Ollama

Ahora generamos embeddings para cada nodo `Resource` usando Ollama (nomic-embed-text).

#### Script Python: GeneraciÃ³n Batch

Crea `scripts/generate-embeddings.py`:

```python
#!/usr/bin/env python3
"""
generate-embeddings.py - Genera embeddings para nodos Neo4j con Ollama
"""

import os
import sys
from neo4j import GraphDatabase
import ollama
from tqdm import tqdm

NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USERNAME = os.getenv("NEO4J_USERNAME", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "aoem2024")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "nomic-embed-text")
BATCH_SIZE = 100

def create_embedding_text(node):
    """Crea texto descriptivo del nodo para embedding"""
    parts = []

    # URI (siempre presente)
    if node.get("uri"):
        parts.append(f"URI: {node['uri']}")

    # Labels (tipos)
    if node.get("labels"):
        parts.append(f"Type: {', '.join(node['labels'])}")

    # rdfs:label
    if node.get("rdfs__label"):
        labels = node["rdfs__label"]
        if isinstance(labels, list):
            parts.append(f"Label: {', '.join(labels)}")
        else:
            parts.append(f"Label: {labels}")

    # Description
    if node.get("aoem__description"):
        parts.append(f"Description: {node['aoem__description']}")

    # dc:title
    if node.get("dc__title"):
        parts.append(f"Title: {node['dc__title']}")

    return " | ".join(parts)

def get_resources_without_embeddings(session):
    """Obtiene nodos sin embeddings"""
    query = """
    MATCH (n:Resource)
    WHERE n.embedding IS NULL
    RETURN
        id(n) AS id,
        n.uri AS uri,
        labels(n) AS labels,
        properties(n) AS props
    """
    result = session.run(query)
    return [dict(record) for record in result]

def generate_embedding(text: str) -> list:
    """Genera embedding con Ollama"""
    response = ollama.embeddings(
        model=EMBEDDING_MODEL,
        prompt=text
    )
    return response["embedding"]

def update_node_embedding(session, node_id: int, embedding: list):
    """Actualiza nodo con embedding"""
    query = """
    MATCH (n:Resource)
    WHERE id(n) = $node_id
    SET n.embedding = $embedding
    """
    session.run(query, node_id=node_id, embedding=embedding)

def main():
    print("ğŸ¤– Generando embeddings con Ollama (nomic-embed-text)\n")

    # Conectar a Neo4j
    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))

    with driver.session() as session:
        # 1. Obtener nodos sin embeddings
        print("ğŸ“Š Consultando nodos sin embeddings...")
        resources = get_resources_without_embeddings(session)

        if not resources:
            print("âœ… Todos los nodos ya tienen embeddings")
            return 0

        print(f"ğŸ“¦ Nodos a procesar: {len(resources)}\n")

        # 2. Generar embeddings en batch
        for i in tqdm(range(0, len(resources), BATCH_SIZE), desc="Generando embeddings"):
            batch = resources[i:i+BATCH_SIZE]

            for node in batch:
                # Crear texto descriptivo
                text = create_embedding_text({
                    "uri": node["uri"],
                    "labels": node["labels"],
                    **node["props"]
                })

                # Generar embedding
                try:
                    embedding = generate_embedding(text)

                    # Actualizar nodo
                    update_node_embedding(session, node["id"], embedding)

                except Exception as e:
                    print(f"\nâŒ Error procesando {node['uri']}: {e}")

        print("\nâœ… Embeddings generados\n")

        # 3. Verificar
        result = session.run("""
            MATCH (n:Resource)
            WHERE n.embedding IS NOT NULL
            RETURN count(n) AS count
        """)
        count = result.single()["count"]
        print(f"ğŸ“Š Nodos con embeddings: {count}/{len(resources)}")

    driver.close()

    if count == len(resources):
        print("\nğŸ‰ Todos los embeddings generados correctamente")
        return 0
    else:
        print(f"\nâš ï¸  Solo {count}/{len(resources)} embeddings generados")
        return 1

if __name__ == "__main__":
    sys.exit(main())
```

**Ejecutar**:

```bash
pip install tqdm  # Progress bar
python scripts/generate-embeddings.py
```

**Output esperado**:

```
ğŸ¤– Generando embeddings con Ollama (nomic-embed-text)

ğŸ“Š Consultando nodos sin embeddings...
ğŸ“¦ Nodos a procesar: 127

Generando embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.23s/it]

âœ… Embeddings generados

ğŸ“Š Nodos con embeddings: 127/127

ğŸ‰ Todos los embeddings generados correctamente
```

---

#### Verificar Embeddings en Neo4j

```cypher
// 1. Contar nodos con embeddings
MATCH (n:Resource)
WHERE n.embedding IS NOT NULL
RETURN count(n) AS nodes_with_embeddings;

// 2. Ver sample de embedding
MATCH (n:Resource)
WHERE n.embedding IS NOT NULL
RETURN n.uri, size(n.embedding) AS dimensions, n.embedding[0..5] AS sample
LIMIT 3;

// Expected dimensions: 768

// 3. Test de similarity search
MATCH (n:Phase {uri: "http://aleia.com/ontology/aoem#DefinePhase"})
CALL db.index.vector.queryNodes('resource_embeddings', 5, n.embedding)
YIELD node, score
RETURN node.uri, node.rdfs__label, score
ORDER BY score DESC;

// Expected: Top 5 nodos mÃ¡s similares a DefinePhase
```

---

### 6.6.5 Pipeline Completo (Automatizado)

Crea `scripts/populate-all.sh` (Linux/Mac) o `scripts/populate-all.ps1` (Windows):

**Windows PowerShell**:

```powershell
# populate-all.ps1
# Pipeline completo de poblaciÃ³n de datos

Write-Host "ğŸš€ AOEM 2.0 - Pipeline de PoblaciÃ³n de Datos`n" -ForegroundColor Cyan

# 1. Verificar prerequisitos
Write-Host "ğŸ“‹ Verificando prerequisitos..." -ForegroundColor Yellow
python scripts/health-check.py
if ($LASTEXITCODE -ne 0) {
    Write-Host "âŒ Health check fallÃ³. Verifica infraestructura (SecciÃ³n 6.5)" -ForegroundColor Red
    exit 1
}

# 2. Cargar ontologÃ­a en Fuseki
Write-Host "`nğŸ“¥ Paso 1/3: Cargando ontologÃ­a en Fuseki..." -ForegroundColor Green
python scripts/load-to-fuseki.py
if ($LASTEXITCODE -ne 0) {
    Write-Host "âŒ Error cargando ontologÃ­a en Fuseki" -ForegroundColor Red
    exit 1
}

# 3. Esperar sync
Write-Host "`nâ³ Esperando 10 segundos para sync..." -ForegroundColor Yellow
Start-Sleep -Seconds 10

# 4. Importar a Neo4j (via Cypher Shell)
Write-Host "`nğŸ“¥ Paso 2/3: Importando RDF a Neo4j..." -ForegroundColor Green

$cypherScript = @"
// Setup n10s
CREATE CONSTRAINT n10s_unique_uri IF NOT EXISTS FOR (r:Resource) REQUIRE r.uri IS UNIQUE;

CALL n10s.graphconfig.init({
  handleVocabUris: 'MAP',
  handleMultival: 'ARRAY',
  multivalPropList: ['http://www.w3.org/2000/01/rdf-schema#label'],
  keepLangTag: true,
  handleRDFTypes: 'LABELS'
});

// Mapear namespaces
CALL n10s.nsprefixes.add('aoem', 'http://aleia.com/ontology/aoem#');
CALL n10s.nsprefixes.add('rdfs', 'http://www.w3.org/2000/01/rdf-schema#');

// Importar desde Fuseki
CALL n10s.rdf.import.fetch(
  'http://host.docker.internal:3030/aoem/data',
  'Turtle',
  { headerParams: { Authorization: 'Basic YWRtaW46YW9lbTIwMjQ=' } }
) YIELD terminationStatus, triplesLoaded
RETURN terminationStatus, triplesLoaded;
"@

$cypherScript | docker exec -i aoem-neo4j cypher-shell -u neo4j -p aoem2024

# 5. Generar embeddings
Write-Host "`nğŸ¤– Paso 3/3: Generando embeddings con Ollama..." -ForegroundColor Green
python scripts/generate-embeddings.py
if ($LASTEXITCODE -ne 0) {
    Write-Host "âŒ Error generando embeddings" -ForegroundColor Red
    exit 1
}

# 6. VerificaciÃ³n final
Write-Host "`nâœ… Pipeline completado. Verificando..." -ForegroundColor Cyan

$verifyScript = @"
MATCH (n:Resource)
RETURN
  count(n) AS total_nodes,
  count(n.embedding) AS nodes_with_embeddings,
  (count(n.embedding) * 100.0 / count(n)) AS coverage_percentage;
"@

docker exec aoem-neo4j cypher-shell -u neo4j -p aoem2024 --format plain "$verifyScript"

Write-Host "`nğŸ‰ PoblaciÃ³n de datos completada" -ForegroundColor Green
Write-Host "   ğŸ“ Fuseki: http://localhost:3030" -ForegroundColor White
Write-Host "   ğŸ“ Neo4j: http://localhost:7474" -ForegroundColor White
```

**Ejecutar**:

```powershell
.\scripts\populate-all.ps1
```

---

### 6.6.6 Criterios de Ã‰xito (Fase 2)

âœ… **Fuseki poblado**:
- [ ] OntologÃ­a AOEM cargada (>100 triples esperados)
- [ ] Query SPARQL funciona: `SELECT * WHERE { ?s ?p ?o } LIMIT 10`
- [ ] Todas las ontologÃ­as en `ontologies/*.ttl` cargadas sin errores

âœ… **Neo4j poblado**:
- [ ] Nodos importados: `MATCH (n:Resource) RETURN count(n)` > 0
- [ ] Labels mapeados: `Phase`, `Task`, `Deliverable`, etc. visibles
- [ ] Relaciones importadas: `MATCH ()-[r]->() RETURN count(r)` > 0
- [ ] n10s configurado: `CALL n10s.graphconfig.show()` devuelve config

âœ… **Embeddings generados**:
- [ ] Todos los nodos tienen embedding: `MATCH (n:Resource) WHERE n.embedding IS NULL RETURN count(n)` = 0
- [ ] Dimensiones correctas: `MATCH (n:Resource) RETURN size(n.embedding)` = 768
- [ ] Vector search funciona: `CALL db.index.vector.queryNodes()` devuelve resultados con scores

âœ… **Performance**:
- [ ] Tiempo de importaciÃ³n Fuseki â†’ Neo4j: <30 segundos para ~500 triples
- [ ] GeneraciÃ³n embeddings: <1 segundo por nodo (batch processing)
- [ ] Vector search latency: <100ms para top-5 similar nodes

---

### 6.6.7 Troubleshooting

#### Problema 1: n10s import falla (connection refused)

**SÃ­ntoma**: `CALL n10s.rdf.import.fetch()` devuelve error de conexiÃ³n.

**Causa**: Neo4j container no puede resolver hostname `fuseki`.

**SoluciÃ³n**: Usar `host.docker.internal` en lugar de `fuseki`:

```cypher
CALL n10s.rdf.import.fetch(
  'http://host.docker.internal:3030/aoem/data',
  'Turtle'
) YIELD terminationStatus, triplesLoaded;
```

---

#### Problema 2: Embeddings muy lentos

**SÃ­ntoma**: `generate-embeddings.py` toma >5 minutos para 100 nodos.

**Causa**: Ollama no estÃ¡ usando GPU o modelo no pre-cargado.

**SoluciÃ³n**:

```bash
# Pre-cargar modelo en memoria
docker exec aoem-ollama ollama run nomic-embed-text "test"

# Verificar uso de GPU (si disponible)
docker logs aoem-ollama | grep -i gpu
```

---

#### Problema 3: Embeddings NULL despuÃ©s de script

**SÃ­ntoma**: Algunos nodos no tienen embedding despuÃ©s de ejecutar script.

**Causa**: ExcepciÃ³n no capturada en batch processing.

**SoluciÃ³n**: Ejecutar en modo verbose:

```bash
# Ver errores detallados
python scripts/generate-embeddings.py 2>&1 | tee embeddings.log

# Re-procesar solo nodos fallidos
python scripts/generate-embeddings.py --retry-failed
```

---

**Estado**: SecciÃ³n 6.6 completada. Datos poblados (Fuseki â†’ Neo4j + embeddings Ollama).

**Siguiente paso**: SecciÃ³n 6.7 - InstalaciÃ³n Fase 3 (IntegraciÃ³n LlamaIndex + LangChain).

---

## 6.7 InstalaciÃ³n Fase 3: IntegraciÃ³n LlamaIndex + LangChain

**Objetivo**: Configurar las capas de ingestion (LlamaIndex) y runtime (LangChain) para interactuar con el stack hÃ­brido (Fuseki + Neo4j + Ollama embeddings + API reasoning).

**Tiempo estimado**: 30-40 minutos.

**Prerequisitos**: Secciones 6.5 y 6.6 completadas.

---

### 6.7.1 Estructura del Proyecto

Crea la siguiente estructura de directorios:

```
aoem-stack/
â”œâ”€â”€ .env                          # Variables de entorno
â”œâ”€â”€ docker-compose.yml            # Infraestructura (SecciÃ³n 6.5)
â”œâ”€â”€ pyproject.toml                # Poetry dependencies (opcional)
â”œâ”€â”€ requirements.txt              # Pip dependencies
â”‚
â”œâ”€â”€ ontologies/                   # OntologÃ­as RDF (SecciÃ³n 6.6)
â”‚   â”œâ”€â”€ aoem-core.ttl
â”‚   â””â”€â”€ aoem-instances.ttl
â”‚
â”œâ”€â”€ scripts/                      # Scripts de setup (Secciones 6.5-6.6)
â”‚   â”œâ”€â”€ start-infrastructure.ps1
â”‚   â”œâ”€â”€ load-to-fuseki.py
â”‚   â”œâ”€â”€ generate-embeddings.py
â”‚   â””â”€â”€ health-check.py
â”‚
â”œâ”€â”€ src/                          # CÃ³digo principal (NUEVA)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                 # ConfiguraciÃ³n centralizada
â”‚   â”œâ”€â”€ llama_setup.py            # LlamaIndex setup
â”‚   â”œâ”€â”€ langchain_setup.py        # LangChain setup
â”‚   â””â”€â”€ agents/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ retriever.py          # Hybrid retriever (vector + graph)
â”‚       â””â”€â”€ reasoning_agent.py    # LangGraph agent
â”‚
â””â”€â”€ tests/                        # Tests (SecciÃ³n 6.8)
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_fuseki.py
    â”œâ”€â”€ test_neo4j.py
    â”œâ”€â”€ test_ollama.py
    â”œâ”€â”€ test_llama_integration.py
    â””â”€â”€ test_langchain_integration.py
```

---

### 6.7.2 ConfiguraciÃ³n Centralizada

Crea `src/config.py`:

```python
"""
config.py - ConfiguraciÃ³n centralizada del stack AOEM 2.0
"""

import os
from pathlib import Path
from dotenv import load_dotenv

# Cargar .env
load_dotenv()

# ==========================================
# ConfiguraciÃ³n de Servicios
# ==========================================

class FusekiConfig:
    """Fuseki (RDF Source of Truth)"""
    URL = os.getenv("FUSEKI_URL", "http://localhost:3030")
    DATASET = os.getenv("FUSEKI_DATASET", "aoem")
    ADMIN_PASSWORD = os.getenv("FUSEKI_ADMIN_PASSWORD", "aoem2024")
    QUERY_ENDPOINT = f"{URL}/{DATASET}/query"
    UPDATE_ENDPOINT = f"{URL}/{DATASET}/update"

class Neo4jConfig:
    """Neo4j 5.26 (Graph + Vector)"""
    URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
    USERNAME = os.getenv("NEO4J_USERNAME", "neo4j")
    PASSWORD = os.getenv("NEO4J_PASSWORD", "aoem2024")
    DATABASE = os.getenv("NEO4J_DATABASE", "neo4j")

    # Vector search
    VECTOR_INDEX_NAME = "resource_embeddings"
    EMBEDDING_DIMENSION = 768
    SIMILARITY_FUNCTION = "cosine"

class OllamaConfig:
    """Ollama (Local Embeddings)"""
    BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "nomic-embed-text")
    EMBEDDING_DIMENSIONS = int(os.getenv("EMBEDDING_DIMENSIONS", "768"))

class LLMConfig:
    """LLM APIs (External Reasoning)"""
    # OpenAI
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
    OPENAI_TEMPERATURE = 0.7

    # Anthropic
    ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
    ANTHROPIC_MODEL = os.getenv("ANTHROPIC_MODEL", "claude-3-5-sonnet-20241022")
    ANTHROPIC_TEMPERATURE = 0.7

    # Default provider
    DEFAULT_PROVIDER = os.getenv("LLM_PROVIDER", "openai")  # openai | anthropic

# ==========================================
# ConfiguraciÃ³n de AplicaciÃ³n
# ==========================================

class AppConfig:
    """ConfiguraciÃ³n general"""
    LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
    PROJECT_ROOT = Path(__file__).parent.parent
    ONTOLOGIES_DIR = PROJECT_ROOT / "ontologies"

    # Retrieval
    TOP_K_VECTOR = 5          # Top K resultados vector search
    TOP_K_GRAPH = 3           # Top K resultados graph traversal
    SIMILARITY_THRESHOLD = 0.7  # Umbral mÃ­nimo similarity

    # Agent
    MAX_ITERATIONS = 10       # MÃ¡ximo iteraciones agent loop
    AGENT_TIMEOUT = 60        # Timeout en segundos

# ==========================================
# ValidaciÃ³n de ConfiguraciÃ³n
# ==========================================

def validate_config():
    """Valida que todas las variables requeridas estÃ©n configuradas"""
    errors = []

    # Verificar API keys
    if not LLMConfig.OPENAI_API_KEY and LLMConfig.DEFAULT_PROVIDER == "openai":
        errors.append("OPENAI_API_KEY no configurada (requerida para provider=openai)")

    if not LLMConfig.ANTHROPIC_API_KEY and LLMConfig.DEFAULT_PROVIDER == "anthropic":
        errors.append("ANTHROPIC_API_KEY no configurada (requerida para provider=anthropic)")

    # Verificar directorio ontologÃ­as
    if not AppConfig.ONTOLOGIES_DIR.exists():
        errors.append(f"Directorio ontologies/ no encontrado: {AppConfig.ONTOLOGIES_DIR}")

    if errors:
        raise ValueError(f"Errores de configuraciÃ³n:\n  - " + "\n  - ".join(errors))

    return True

# Auto-validar al importar (comentar si quieres validaciÃ³n manual)
# validate_config()
```

---

### 6.7.3 Setup LlamaIndex (Ingestion Layer)

Crea `src/llama_setup.py`:

```python
"""
llama_setup.py - ConfiguraciÃ³n de LlamaIndex para ingestion pipeline
"""

from llama_index.core import Settings
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.vector_stores.neo4jvector import Neo4jVectorStore
from llama_index.graph_stores.neo4j import Neo4jGraphStore
from llama_index.llms.openai import OpenAI
from llama_index.llms.anthropic import Anthropic

from .config import OllamaConfig, Neo4jConfig, LLMConfig

# ==========================================
# Embeddings (Ollama Local)
# ==========================================

def get_embedding_model():
    """Inicializa modelo de embeddings Ollama"""
    return OllamaEmbedding(
        model_name=OllamaConfig.EMBEDDING_MODEL,
        base_url=OllamaConfig.BASE_URL,
        embed_batch_size=10  # Batch size para efficiency
    )

# ==========================================
# LLM (API Externa)
# ==========================================

def get_llm(provider: str = None):
    """Inicializa LLM (OpenAI o Anthropic)"""
    provider = provider or LLMConfig.DEFAULT_PROVIDER

    if provider == "openai":
        return OpenAI(
            model=LLMConfig.OPENAI_MODEL,
            api_key=LLMConfig.OPENAI_API_KEY,
            temperature=LLMConfig.OPENAI_TEMPERATURE
        )
    elif provider == "anthropic":
        return Anthropic(
            model=LLMConfig.ANTHROPIC_MODEL,
            api_key=LLMConfig.ANTHROPIC_API_KEY,
            temperature=LLMConfig.ANTHROPIC_TEMPERATURE
        )
    else:
        raise ValueError(f"Provider no soportado: {provider}")

# ==========================================
# Vector Store (Neo4j)
# ==========================================

def get_neo4j_vector_store():
    """Inicializa Neo4j Vector Store"""
    return Neo4jVectorStore(
        username=Neo4jConfig.USERNAME,
        password=Neo4jConfig.PASSWORD,
        url=Neo4jConfig.URI,
        database=Neo4jConfig.DATABASE,
        embedding_dimension=Neo4jConfig.EMBEDDING_DIMENSION,
        index_name=Neo4jConfig.VECTOR_INDEX_NAME,
        node_label="Resource",  # Label para nodos con embeddings
        embedding_node_property="embedding",
        text_node_property="uri"
    )

# ==========================================
# Graph Store (Neo4j)
# ==========================================

def get_neo4j_graph_store():
    """Inicializa Neo4j Graph Store"""
    return Neo4jGraphStore(
        username=Neo4jConfig.USERNAME,
        password=Neo4jConfig.PASSWORD,
        url=Neo4jConfig.URI,
        database=Neo4jConfig.DATABASE
    )

# ==========================================
# ConfiguraciÃ³n Global LlamaIndex
# ==========================================

def setup_llama_index(provider: str = None):
    """
    Configura LlamaIndex globalmente

    Args:
        provider: 'openai' o 'anthropic'
    """
    # Set embeddings (Ollama local)
    Settings.embed_model = get_embedding_model()

    # Set LLM (API externa)
    Settings.llm = get_llm(provider)

    # Set chunk size (para text splitting)
    Settings.chunk_size = 512
    Settings.chunk_overlap = 50

    print(f"âœ… LlamaIndex configurado:")
    print(f"   - Embeddings: {OllamaConfig.EMBEDDING_MODEL} (local)")
    print(f"   - LLM: {LLMConfig.DEFAULT_PROVIDER if not provider else provider} (API)")

# ==========================================
# Testing Functions
# ==========================================

def test_embedding_generation():
    """Test de generaciÃ³n de embeddings"""
    embed_model = get_embedding_model()

    text = "AOEM is an Adaptive Ontology Engineering Methodology"
    embedding = embed_model.get_text_embedding(text)

    print(f"âœ… Test embeddings:")
    print(f"   - Text: '{text}'")
    print(f"   - Dimensions: {len(embedding)}")
    print(f"   - Sample: {embedding[:5]}")

    assert len(embedding) == OllamaConfig.EMBEDDING_DIMENSIONS, \
        f"Expected {OllamaConfig.EMBEDDING_DIMENSIONS}d, got {len(embedding)}d"

    return embedding

def test_llm_completion(provider: str = None):
    """Test de completions con LLM"""
    llm = get_llm(provider)

    prompt = "What is ontology engineering in 1 sentence?"
    response = llm.complete(prompt)

    print(f"âœ… Test LLM completion:")
    print(f"   - Prompt: '{prompt}'")
    print(f"   - Response: '{response.text[:100]}...'")

    return response

def test_neo4j_connection():
    """Test de conexiÃ³n a Neo4j"""
    try:
        vector_store = get_neo4j_vector_store()
        graph_store = get_neo4j_graph_store()

        print(f"âœ… Test Neo4j connection:")
        print(f"   - Vector store: Connected")
        print(f"   - Graph store: Connected")

        return True
    except Exception as e:
        print(f"âŒ Neo4j connection failed: {e}")
        return False

# ==========================================
# Main Setup
# ==========================================

if __name__ == "__main__":
    """
    Test standalone:
    python -m src.llama_setup
    """
    print("ğŸ”§ Testing LlamaIndex Setup\n")

    # Setup
    setup_llama_index()

    # Tests
    print("\n1. Testing embeddings...")
    test_embedding_generation()

    print("\n2. Testing LLM...")
    test_llm_completion()

    print("\n3. Testing Neo4j...")
    test_neo4j_connection()

    print("\nâœ… All tests passed!")
```

---

### 6.7.4 Setup LangChain (Runtime Layer)

Crea `src/langchain_setup.py`:

```python
"""
langchain_setup.py - ConfiguraciÃ³n de LangChain + Neo4j GraphRAG
"""

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_anthropic import ChatAnthropic
from langchain_community.embeddings import OllamaEmbeddings
from langchain_neo4j import Neo4jGraph, Neo4jVector
from neo4j_graphrag.retrievers import HybridCypherRetriever, VectorRetriever
from neo4j_graphrag.embeddings import OllamaEmbeddings as Neo4jOllamaEmbeddings
from neo4j_graphrag.llm import OpenAILLM

from .config import Neo4jConfig, OllamaConfig, LLMConfig

# ==========================================
# Embeddings (Ollama Local)
# ==========================================

def get_langchain_embeddings():
    """Inicializa Ollama embeddings para LangChain"""
    return OllamaEmbeddings(
        base_url=OllamaConfig.BASE_URL,
        model=OllamaConfig.EMBEDDING_MODEL
    )

def get_neo4j_graphrag_embeddings():
    """Inicializa embeddings para Neo4j GraphRAG"""
    return Neo4jOllamaEmbeddings(
        base_url=OllamaConfig.BASE_URL,
        model=OllamaConfig.EMBEDDING_MODEL
    )

# ==========================================
# LLM (API Externa)
# ==========================================

def get_langchain_llm(provider: str = None):
    """Inicializa LLM para LangChain"""
    provider = provider or LLMConfig.DEFAULT_PROVIDER

    if provider == "openai":
        return ChatOpenAI(
            model=LLMConfig.OPENAI_MODEL,
            api_key=LLMConfig.OPENAI_API_KEY,
            temperature=LLMConfig.OPENAI_TEMPERATURE
        )
    elif provider == "anthropic":
        return ChatAnthropic(
            model=LLMConfig.ANTHROPIC_MODEL,
            api_key=LLMConfig.ANTHROPIC_API_KEY,
            temperature=LLMConfig.ANTHROPIC_TEMPERATURE
        )
    else:
        raise ValueError(f"Provider no soportado: {provider}")

# ==========================================
# Neo4j Graph
# ==========================================

def get_neo4j_graph():
    """Inicializa Neo4j Graph para LangChain"""
    return Neo4jGraph(
        url=Neo4jConfig.URI,
        username=Neo4jConfig.USERNAME,
        password=Neo4jConfig.PASSWORD,
        database=Neo4jConfig.DATABASE
    )

# ==========================================
# Neo4j Vector Store (LangChain)
# ==========================================

def get_neo4j_vector():
    """Inicializa Neo4j Vector para LangChain"""
    return Neo4jVector.from_existing_index(
        embedding=get_langchain_embeddings(),
        url=Neo4jConfig.URI,
        username=Neo4jConfig.USERNAME,
        password=Neo4jConfig.PASSWORD,
        database=Neo4jConfig.DATABASE,
        index_name=Neo4jConfig.VECTOR_INDEX_NAME,
        node_label="Resource",
        text_node_property="uri",
        embedding_node_property="embedding"
    )

# ==========================================
# Neo4j GraphRAG Retrievers
# ==========================================

def get_vector_retriever(top_k: int = 5):
    """
    Crea VectorRetriever para bÃºsqueda semÃ¡ntica pura

    Args:
        top_k: NÃºmero de resultados a retornar
    """
    from neo4j import GraphDatabase

    driver = GraphDatabase.driver(
        Neo4jConfig.URI,
        auth=(Neo4jConfig.USERNAME, Neo4jConfig.PASSWORD)
    )

    embedder = get_neo4j_graphrag_embeddings()

    return VectorRetriever(
        driver=driver,
        index_name=Neo4jConfig.VECTOR_INDEX_NAME,
        embedder=embedder,
        return_properties=["uri", "rdfs__label", "aoem__description"],
        top_k=top_k
    )

def get_hybrid_retriever(top_k: int = 5):
    """
    Crea HybridCypherRetriever (vector + graph traversal)

    Args:
        top_k: NÃºmero de resultados a retornar
    """
    from neo4j import GraphDatabase

    driver = GraphDatabase.driver(
        Neo4jConfig.URI,
        auth=(Neo4jConfig.USERNAME, Neo4jConfig.PASSWORD)
    )

    embedder = get_neo4j_graphrag_embeddings()

    # Cypher query para graph traversal (customizable)
    retrieval_query = """
    // Vector search
    CALL db.index.vector.queryNodes($index, $k, $embedding)
    YIELD node, score

    // Graph expansion (1-hop neighbors)
    OPTIONAL MATCH (node)-[r]-(neighbor:Resource)

    // Return enriched context
    RETURN
        node.uri AS uri,
        node.rdfs__label AS label,
        node.aoem__description AS description,
        score,
        collect({
            type: type(r),
            neighbor_uri: neighbor.uri,
            neighbor_label: neighbor.rdfs__label
        }) AS relationships
    ORDER BY score DESC
    LIMIT $k
    """

    return HybridCypherRetriever(
        driver=driver,
        index_name=Neo4jConfig.VECTOR_INDEX_NAME,
        embedder=embedder,
        retrieval_query=retrieval_query,
        top_k=top_k
    )

# ==========================================
# Testing Functions
# ==========================================

def test_langchain_embeddings():
    """Test embeddings con LangChain"""
    embeddings = get_langchain_embeddings()

    text = "Ontology engineering methodology"
    embedding = embeddings.embed_query(text)

    print(f"âœ… Test LangChain embeddings:")
    print(f"   - Text: '{text}'")
    print(f"   - Dimensions: {len(embedding)}")
    print(f"   - Sample: {embedding[:5]}")

    assert len(embedding) == OllamaConfig.EMBEDDING_DIMENSIONS
    return embedding

def test_langchain_llm(provider: str = None):
    """Test LLM con LangChain"""
    llm = get_langchain_llm(provider)

    messages = [
        ("system", "You are a helpful assistant."),
        ("human", "What is AOEM in one sentence?")
    ]

    response = llm.invoke(messages)

    print(f"âœ… Test LangChain LLM:")
    print(f"   - Provider: {provider or LLMConfig.DEFAULT_PROVIDER}")
    print(f"   - Response: '{response.content[:100]}...'")

    return response

def test_neo4j_graph():
    """Test Neo4j Graph connection"""
    graph = get_neo4j_graph()

    # Query simple
    result = graph.query("MATCH (n:Resource) RETURN count(n) AS count LIMIT 1")
    count = result[0]["count"] if result else 0

    print(f"âœ… Test Neo4j Graph:")
    print(f"   - Connected: True")
    print(f"   - Resource nodes: {count}")

    return count > 0

def test_vector_retriever():
    """Test VectorRetriever"""
    retriever = get_vector_retriever(top_k=3)

    query = "What are the phases of AOEM?"
    results = retriever.search(query_text=query)

    print(f"âœ… Test VectorRetriever:")
    print(f"   - Query: '{query}'")
    print(f"   - Results: {len(results.items)}")

    for i, item in enumerate(results.items[:3], 1):
        print(f"   {i}. {item.content.get('uri', 'N/A')} (score: {item.metadata.get('score', 0):.3f})")

    return len(results.items) > 0

def test_hybrid_retriever():
    """Test HybridCypherRetriever"""
    retriever = get_hybrid_retriever(top_k=3)

    query = "Tell me about the Define phase"
    results = retriever.search(query_text=query)

    print(f"âœ… Test HybridRetriever:")
    print(f"   - Query: '{query}'")
    print(f"   - Results: {len(results.items)}")

    for i, item in enumerate(results.items[:3], 1):
        uri = item.content.get('uri', 'N/A')
        score = item.metadata.get('score', 0)
        rels = item.content.get('relationships', [])
        print(f"   {i}. {uri} (score: {score:.3f}, {len(rels)} relationships)")

    return len(results.items) > 0

# ==========================================
# Main Setup
# ==========================================

if __name__ == "__main__":
    """
    Test standalone:
    python -m src.langchain_setup
    """
    print("ğŸ”§ Testing LangChain Setup\n")

    print("1. Testing embeddings...")
    test_langchain_embeddings()

    print("\n2. Testing LLM...")
    test_langchain_llm()

    print("\n3. Testing Neo4j Graph...")
    test_neo4j_graph()

    print("\n4. Testing VectorRetriever...")
    test_vector_retriever()

    print("\n5. Testing HybridRetriever...")
    test_hybrid_retriever()

    print("\nâœ… All tests passed!")
```

---

### 6.7.5 Hybrid Retriever (Vector + Graph)

Crea `src/agents/retriever.py`:

```python
"""
retriever.py - Hybrid retriever combinando vector search + graph traversal
"""

from typing import List, Dict, Any
from dataclasses import dataclass

from ..langchain_setup import (
    get_vector_retriever,
    get_hybrid_retriever,
    get_neo4j_graph
)
from ..config import AppConfig

@dataclass
class RetrievalResult:
    """Resultado de retrieval"""
    uri: str
    label: str
    description: str
    score: float
    relationships: List[Dict[str, Any]]
    source: str  # 'vector' | 'hybrid' | 'cypher'

class AOEMRetriever:
    """
    Retriever hÃ­brido para AOEM ontology
    Combina vector search + graph traversal + SPARQL fallback
    """

    def __init__(
        self,
        use_hybrid: bool = True,
        top_k_vector: int = None,
        top_k_graph: int = None,
        similarity_threshold: float = None
    ):
        """
        Args:
            use_hybrid: Si True, usa HybridCypherRetriever. Si False, usa VectorRetriever.
            top_k_vector: Top K para vector search
            top_k_graph: Top K para graph traversal
            similarity_threshold: Umbral mÃ­nimo de similarity
        """
        self.use_hybrid = use_hybrid
        self.top_k_vector = top_k_vector or AppConfig.TOP_K_VECTOR
        self.top_k_graph = top_k_graph or AppConfig.TOP_K_GRAPH
        self.similarity_threshold = similarity_threshold or AppConfig.SIMILARITY_THRESHOLD

        # Inicializar retrievers
        if use_hybrid:
            self.retriever = get_hybrid_retriever(top_k=self.top_k_vector)
            print(f"âœ… Hybrid retriever inicializado (top_k={self.top_k_vector})")
        else:
            self.retriever = get_vector_retriever(top_k=self.top_k_vector)
            print(f"âœ… Vector retriever inicializado (top_k={self.top_k_vector})")

        self.graph = get_neo4j_graph()

    def search(self, query: str) -> List[RetrievalResult]:
        """
        Busca en Neo4j usando retriever configurado

        Args:
            query: Query de bÃºsqueda natural language

        Returns:
            Lista de RetrievalResult ordenados por score
        """
        # Ejecutar retrieval
        results = self.retriever.search(query_text=query)

        # Parsear resultados
        parsed_results = []
        for item in results.items:
            content = item.content
            metadata = item.metadata

            # Filtrar por threshold
            score = metadata.get('score', 0.0)
            if score < self.similarity_threshold:
                continue

            result = RetrievalResult(
                uri=content.get('uri', ''),
                label=content.get('label', ''),
                description=content.get('description', ''),
                score=score,
                relationships=content.get('relationships', []),
                source='hybrid' if self.use_hybrid else 'vector'
            )
            parsed_results.append(result)

        return parsed_results

    def expand_graph_context(self, uri: str, depth: int = 1) -> Dict[str, Any]:
        """
        Expande contexto de un nodo usando graph traversal

        Args:
            uri: URI del nodo a expandir
            depth: Profundidad de traversal (default: 1-hop)

        Returns:
            Dict con nodo central y neighbors
        """
        query = f"""
        MATCH (n:Resource {{uri: $uri}})
        OPTIONAL MATCH (n)-[r]-(neighbor:Resource)
        WHERE 1=1 {'AND length((n)-[r]-(neighbor)) <= $depth' if depth > 0 else ''}
        RETURN
            n.uri AS uri,
            n.rdfs__label AS label,
            n.aoem__description AS description,
            collect({{
                type: type(r),
                direction: CASE WHEN startNode(r) = n THEN 'outgoing' ELSE 'incoming' END,
                neighbor_uri: neighbor.uri,
                neighbor_label: neighbor.rdfs__label
            }}) AS relationships
        """

        result = self.graph.query(query, params={"uri": uri, "depth": depth})

        if result:
            return result[0]
        return {}

    def search_by_type(self, rdf_type: str, limit: int = 10) -> List[RetrievalResult]:
        """
        Busca nodos por tipo RDF (Phase, Task, Tool, etc.)

        Args:
            rdf_type: Tipo RDF (ej: 'Phase', 'Task')
            limit: LÃ­mite de resultados

        Returns:
            Lista de RetrievalResult
        """
        query = f"""
        MATCH (n:{rdf_type})
        RETURN
            n.uri AS uri,
            n.rdfs__label AS label,
            n.aoem__description AS description,
            1.0 AS score
        LIMIT $limit
        """

        results = self.graph.query(query, params={"limit": limit})

        return [
            RetrievalResult(
                uri=r["uri"],
                label=r["label"],
                description=r.get("description", ""),
                score=r["score"],
                relationships=[],
                source='cypher'
            )
            for r in results
        ]

# ==========================================
# Testing
# ==========================================

def test_retriever():
    """Test del retriever hÃ­brido"""
    print("ğŸ” Testing AOEM Retriever\n")

    # 1. Vector search
    print("1. Vector search:")
    retriever = AOEMRetriever(use_hybrid=False, top_k_vector=3)
    results = retriever.search("What are the main phases of AOEM?")

    print(f"   Results: {len(results)}")
    for r in results:
        print(f"   - {r.label} ({r.score:.3f})")

    # 2. Hybrid search
    print("\n2. Hybrid search (vector + graph):")
    retriever_hybrid = AOEMRetriever(use_hybrid=True, top_k_vector=3)
    results_hybrid = retriever_hybrid.search("Tell me about the Define phase")

    print(f"   Results: {len(results_hybrid)}")
    for r in results_hybrid:
        print(f"   - {r.label} ({r.score:.3f}, {len(r.relationships)} rels)")

    # 3. Search by type
    print("\n3. Search by type (Phase):")
    phases = retriever.search_by_type("Phase", limit=5)
    print(f"   Phases found: {len(phases)}")
    for p in phases:
        print(f"   - {p.label}")

    # 4. Graph expansion
    if results_hybrid:
        print("\n4. Graph context expansion:")
        first_uri = results_hybrid[0].uri
        context = retriever_hybrid.expand_graph_context(first_uri, depth=1)
        print(f"   Node: {context.get('label', 'N/A')}")
        print(f"   Relationships: {len(context.get('relationships', []))}")

if __name__ == "__main__":
    test_retriever()
```

---

### 6.7.6 VerificaciÃ³n de IntegraciÃ³n

Crea `scripts/test-integration.py`:

```python
#!/usr/bin/env python3
"""
test-integration.py - Verifica integraciÃ³n completa del stack
"""

import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import validate_config
from src.llama_setup import (
    setup_llama_index,
    test_embedding_generation,
    test_llm_completion,
    test_neo4j_connection
)
from src.langchain_setup import (
    test_langchain_embeddings,
    test_langchain_llm,
    test_neo4j_graph,
    test_vector_retriever,
    test_hybrid_retriever
)
from src.agents.retriever import test_retriever

def main():
    print("="*60)
    print("ğŸ”§ AOEM 2.0 - Test de IntegraciÃ³n Completa")
    print("="*60 + "\n")

    # 1. Validar configuraciÃ³n
    print("ğŸ“‹ Paso 1/3: Validando configuraciÃ³n...")
    try:
        validate_config()
        print("âœ… ConfiguraciÃ³n vÃ¡lida\n")
    except ValueError as e:
        print(f"âŒ Error de configuraciÃ³n: {e}")
        return 1

    # 2. Test LlamaIndex
    print("="*60)
    print("ğŸ¦™ Paso 2/3: Testing LlamaIndex (Ingestion Layer)")
    print("="*60 + "\n")

    try:
        setup_llama_index()
        print()

        print("2.1 Embeddings (Ollama)...")
        test_embedding_generation()
        print()

        print("2.2 LLM Completion (API)...")
        test_llm_completion()
        print()

        print("2.3 Neo4j Connection...")
        test_neo4j_connection()
        print()

        print("âœ… LlamaIndex OK\n")
    except Exception as e:
        print(f"âŒ LlamaIndex failed: {e}\n")
        return 1

    # 3. Test LangChain + Neo4j GraphRAG
    print("="*60)
    print("ğŸ¦œ Paso 3/3: Testing LangChain + Neo4j GraphRAG (Runtime Layer)")
    print("="*60 + "\n")

    try:
        print("3.1 Embeddings (LangChain + Ollama)...")
        test_langchain_embeddings()
        print()

        print("3.2 LLM (LangChain)...")
        test_langchain_llm()
        print()

        print("3.3 Neo4j Graph...")
        test_neo4j_graph()
        print()

        print("3.4 Vector Retriever...")
        test_vector_retriever()
        print()

        print("3.5 Hybrid Retriever...")
        test_hybrid_retriever()
        print()

        print("3.6 AOEM Retriever...")
        test_retriever()
        print()

        print("âœ… LangChain + GraphRAG OK\n")
    except Exception as e:
        print(f"âŒ LangChain failed: {e}\n")
        return 1

    # Final
    print("="*60)
    print("âœ… INTEGRACIÃ“N COMPLETA EXITOSA")
    print("="*60)
    print("\nğŸ“ Stack funcionando:")
    print("   - Fuseki 5.0.0: http://localhost:3030")
    print("   - Neo4j 5.26: http://localhost:7474")
    print("   - Ollama (embeddings): http://localhost:11434")
    print("   - LlamaIndex: Configurado (ingestion)")
    print("   - LangChain + GraphRAG: Configurado (runtime)")
    print("\nğŸ‰ Listo para construir agentes!")

    return 0

if __name__ == "__main__":
    sys.exit(main())
```

**Ejecutar**:

```bash
python scripts/test-integration.py
```

---

### 6.7.7 Criterios de Ã‰xito (Fase 3)

âœ… **LlamaIndex configurado**:
- [ ] Embeddings Ollama funcionando (768d)
- [ ] LLM API (OpenAI/Anthropic) respondiendo
- [ ] Neo4j Vector Store conectado
- [ ] Neo4j Graph Store conectado
- [ ] `python -m src.llama_setup` ejecuta sin errores

âœ… **LangChain + GraphRAG configurado**:
- [ ] LangChain embeddings (Ollama) funcionando
- [ ] LangChain LLM (API) respondiendo
- [ ] Neo4jGraph conectado
- [ ] VectorRetriever devuelve resultados (>0)
- [ ] HybridCypherRetriever devuelve resultados con relaciones
- [ ] `python -m src.langchain_setup` ejecuta sin errores

âœ… **Retriever hÃ­brido funcional**:
- [ ] AOEMRetriever inicializa correctamente
- [ ] Vector search devuelve resultados relevantes (score > 0.7)
- [ ] Graph expansion funciona (1-hop neighbors)
- [ ] Search by type funciona (Phase, Task, etc.)
- [ ] `python -m src.agents.retriever` ejecuta sin errores

âœ… **IntegraciÃ³n E2E**:
- [ ] `python scripts/test-integration.py` ejecuta sin errores
- [ ] Todos los tests pasan (embeddings, LLM, retrievers)
- [ ] Latency aceptable: vector search <100ms, hybrid <200ms

---

### 6.7.8 Troubleshooting

#### Problema 1: ImportError para neo4j_graphrag

**SÃ­ntoma**: `ModuleNotFoundError: No module named 'neo4j_graphrag'`

**SoluciÃ³n**:

```bash
pip install neo4j-graphrag-python==0.4.0
```

---

#### Problema 2: Ollama embeddings timeout

**SÃ­ntoma**: `requests.exceptions.ReadTimeout` al generar embeddings.

**Causa**: Modelo no pre-cargado en memoria.

**SoluciÃ³n**:

```bash
# Pre-cargar modelo
docker exec aoem-ollama ollama run nomic-embed-text "warmup"

# Verificar
curl http://localhost:11434/api/tags | jq '.models[].name'
```

---

#### Problema 3: HybridRetriever devuelve 0 resultados

**SÃ­ntoma**: `retriever.search()` devuelve lista vacÃ­a.

**Causa**: Ãndice vectorial no creado o embeddings faltantes.

**VerificaciÃ³n**:

```cypher
// 1. Verificar Ã­ndice
SHOW INDEXES YIELD name, type, state
WHERE type = 'VECTOR'
RETURN name, state;

// 2. Verificar embeddings
MATCH (n:Resource)
WHERE n.embedding IS NOT NULL
RETURN count(n) AS nodes_with_embeddings;
```

**SoluciÃ³n**: Re-ejecutar SecciÃ³n 6.6.4 (generaciÃ³n embeddings).

---

**Estado**: SecciÃ³n 6.7 completada. LlamaIndex + LangChain + Neo4j GraphRAG integrados.

**Siguiente paso**: SecciÃ³n 6.8 - Suite de Pruebas Completa (Unit, Integration, E2E).

---

## 6.8 Suite de Pruebas Completa

**Objetivo**: Establecer estrategia de testing exhaustiva para validar cada componente del stack y sus integraciones. Los ejemplos de cÃ³digo presentados aquÃ­ se implementarÃ¡n en `tests/` una vez el stack estÃ© operativo.

**Cobertura mÃ­nima requerida**: 80% (unit tests), 100% critical paths (integration tests).

**Tiempo estimado**: 1-2 horas para ejecutar suite completa.

---

### 6.8.1 Test Strategy Overview

#### PirÃ¡mide de Testing

```
                    E2E Tests (5%)
                  /                \
            Integration Tests (25%)
          /                            \
    Unit Tests (70%)
```

**DistribuciÃ³n**:
- **Unit Tests (70%)**: Funciones individuales, componentes aislados
- **Integration Tests (25%)**: InteracciÃ³n entre 2+ componentes (Fuseki + Neo4j, Neo4j + Ollama)
- **E2E Tests (5%)**: Flujos completos (user query â†’ retrieval â†’ reasoning â†’ response)

#### Componentes a Probar

1. **Infraestructura** (SecciÃ³n 6.5)
   - Fuseki 5.0.0: Conectividad, SPARQL queries, dataset management
   - Neo4j 5.26: Conectividad, Cypher queries, Ã­ndices vectoriales, plugins (APOC, n10s)
   - Ollama: Conectividad, generaciÃ³n embeddings, latency

2. **PoblaciÃ³n de Datos** (SecciÃ³n 6.6)
   - Carga RDF a Fuseki
   - ImportaciÃ³n Fuseki â†’ Neo4j (n10s)
   - GeneraciÃ³n embeddings batch
   - ValidaciÃ³n integridad datos

3. **IntegraciÃ³n Frameworks** (SecciÃ³n 6.7)
   - LlamaIndex: Embeddings, LLM, Vector/Graph stores
   - LangChain: Embeddings, LLM, Neo4j Graph/Vector
   - Neo4j GraphRAG: VectorRetriever, HybridCypherRetriever
   - AOEMRetriever: Search methods, graph expansion

4. **Performance** (Benchmarks)
   - Vector search latency (<100ms)
   - SPARQL query latency (<200ms)
   - Embedding generation throughput (>10 docs/sec)
   - E2E query latency (<2s)

---

### 6.8.2 Pytest Configuration

#### `pytest.ini` (raÃ­z del proyecto)

```ini
[pytest]
# Test discovery
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Output
addopts =
    -v
    --tb=short
    --strict-markers
    --cov=src
    --cov-report=html
    --cov-report=term-missing
    --cov-fail-under=80
    -p no:warnings

# Markers (categorÃ­as de tests)
markers =
    unit: Unit tests (fast, isolated)
    integration: Integration tests (requires services)
    e2e: End-to-end tests (full workflow)
    slow: Tests que toman >5 segundos
    fuseki: Tests especÃ­ficos de Fuseki
    neo4j: Tests especÃ­ficos de Neo4j
    ollama: Tests especÃ­ficos de Ollama
    llama: Tests de LlamaIndex
    langchain: Tests de LangChain
    performance: Performance benchmarks

# Timeout (evitar tests colgados)
timeout = 300
timeout_method = thread

# Logging
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Environment variables
env_files = .env.test
```

#### `.env.test` (variables para testing)

```bash
# .env.test - ConfiguraciÃ³n de testing
# Sobrescribe .env durante tests

# Servicios (apuntando a localhost)
FUSEKI_URL=http://localhost:3030
FUSEKI_DATASET=aoem_test
NEO4J_URI=bolt://localhost:7687
OLLAMA_BASE_URL=http://localhost:11434

# LLM (usar mock o modelos mÃ¡s baratos)
OPENAI_MODEL=gpt-4o-mini
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Test config
LOG_LEVEL=DEBUG
PYTEST_TIMEOUT=300
```

#### `conftest.py` (fixtures globales)

```python
"""
conftest.py - Fixtures compartidos para todos los tests
"""

import pytest
import os
from pathlib import Path
from neo4j import GraphDatabase
from SPARQLWrapper import SPARQLWrapper
import requests

# Set test environment
os.environ["TESTING"] = "1"

# ==========================================
# Session-scoped fixtures (setup once)
# ==========================================

@pytest.fixture(scope="session")
def test_env():
    """Load test environment variables"""
    from dotenv import load_dotenv
    load_dotenv(".env.test")
    return {
        "fuseki_url": os.getenv("FUSEKI_URL"),
        "neo4j_uri": os.getenv("NEO4J_URI"),
        "ollama_url": os.getenv("OLLAMA_BASE_URL")
    }

@pytest.fixture(scope="session")
def fuseki_url(test_env):
    """Fuseki URL"""
    return test_env["fuseki_url"]

@pytest.fixture(scope="session")
def neo4j_uri(test_env):
    """Neo4j URI"""
    return test_env["neo4j_uri"]

@pytest.fixture(scope="session")
def ollama_url(test_env):
    """Ollama URL"""
    return test_env["ollama_url"]

# ==========================================
# Function-scoped fixtures (setup per test)
# ==========================================

@pytest.fixture
def fuseki_client(fuseki_url):
    """Cliente SPARQL para Fuseki"""
    sparql = SPARQLWrapper(f"{fuseki_url}/aoem_test/query")
    sparql.setReturnFormat("json")
    yield sparql
    # Cleanup: vaciar dataset despuÃ©s de test
    # (implementar si es necesario)

@pytest.fixture
def neo4j_driver(neo4j_uri):
    """Driver Neo4j"""
    driver = GraphDatabase.driver(
        neo4j_uri,
        auth=("neo4j", "aoem2024")
    )
    yield driver
    driver.close()

@pytest.fixture
def neo4j_session(neo4j_driver):
    """SesiÃ³n Neo4j (con cleanup)"""
    session = neo4j_driver.session()
    yield session
    # Cleanup: borrar datos de test
    session.run("MATCH (n:TestNode) DETACH DELETE n")
    session.close()

@pytest.fixture
def ollama_client(ollama_url):
    """Cliente Ollama"""
    import ollama
    # Configurar base_url si es necesario
    return ollama

# ==========================================
# Parametrized fixtures (data-driven tests)
# ==========================================

@pytest.fixture(params=["openai", "anthropic"])
def llm_provider(request):
    """Parametrized LLM provider"""
    return request.param

@pytest.fixture(params=[5, 10, 20])
def top_k_values(request):
    """Parametrized top_k for retrieval tests"""
    return request.param

# ==========================================
# Markers automation
# ==========================================

def pytest_collection_modifyitems(config, items):
    """Auto-mark tests basado en path"""
    for item in items:
        # Mark por path
        if "integration" in str(item.fspath):
            item.add_marker(pytest.mark.integration)
        elif "e2e" in str(item.fspath):
            item.add_marker(pytest.mark.e2e)
        else:
            item.add_marker(pytest.mark.unit)

        # Mark por nombre de archivo
        if "fuseki" in str(item.fspath):
            item.add_marker(pytest.mark.fuseki)
        if "neo4j" in str(item.fspath):
            item.add_marker(pytest.mark.neo4j)
        if "ollama" in str(item.fspath):
            item.add_marker(pytest.mark.ollama)
        if "llama" in str(item.fspath):
            item.add_marker(pytest.mark.llama)
        if "langchain" in str(item.fspath):
            item.add_marker(pytest.mark.langchain)

# ==========================================
# Skip conditions
# ==========================================

def pytest_configure(config):
    """Register custom markers"""
    config.addinivalue_line(
        "markers", "requires_services: mark test as requiring external services"
    )

@pytest.fixture(scope="session", autouse=True)
def check_services(test_env):
    """Verificar que servicios estÃ©n disponibles antes de correr tests"""
    services = {
        "Fuseki": test_env["fuseki_url"] + "/$/ping",
        "Neo4j": test_env["neo4j_uri"].replace("bolt://", "http://").replace("7687", "7474"),
        "Ollama": test_env["ollama_url"] + "/api/tags"
    }

    unavailable = []
    for name, url in services.items():
        try:
            response = requests.get(url, timeout=5)
            if response.status_code not in [200, 204]:
                unavailable.append(name)
        except:
            unavailable.append(name)

    if unavailable:
        pytest.skip(
            f"Services not available: {', '.join(unavailable)}. "
            f"Start services with docker-compose up -d"
        )
```

---

### 6.8.3 Unit Tests por Componente

#### A. Tests de Fuseki (`tests/unit/test_fuseki.py`)

```python
"""
test_fuseki.py - Unit tests para Fuseki 5.0.0
"""

import pytest
from SPARQLWrapper import SPARQLWrapper

@pytest.mark.unit
@pytest.mark.fuseki
class TestFusekiConnectivity:
    """Tests de conectividad bÃ¡sica"""

    def test_ping_endpoint(self, fuseki_url):
        """Test que Fuseki responde al ping"""
        import requests
        response = requests.get(f"{fuseki_url}/$/ping")
        assert response.status_code == 200

    def test_dataset_exists(self, fuseki_client):
        """Test que dataset aoem_test existe"""
        fuseki_client.setQuery("ASK { ?s ?p ?o }")
        result = fuseki_client.query().convert()
        assert "boolean" in result

@pytest.mark.unit
@pytest.mark.fuseki
class TestSPARQLQueries:
    """Tests de queries SPARQL"""

    def test_select_query(self, fuseki_client):
        """Test query SELECT bÃ¡sico"""
        fuseki_client.setQuery("""
            SELECT ?s ?p ?o
            WHERE { ?s ?p ?o }
            LIMIT 10
        """)
        result = fuseki_client.query().convert()
        assert "results" in result
        assert "bindings" in result["results"]

    def test_count_triples(self, fuseki_client):
        """Test contar triples"""
        fuseki_client.setQuery("""
            SELECT (COUNT(*) AS ?count)
            WHERE { ?s ?p ?o }
        """)
        result = fuseki_client.query().convert()
        count = int(result["results"]["bindings"][0]["count"]["value"])
        assert count >= 0

    def test_ask_query(self, fuseki_client):
        """Test query ASK"""
        fuseki_client.setQuery("""
            ASK { ?s <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ?o }
        """)
        result = fuseki_client.query().convert()
        assert isinstance(result["boolean"], bool)

    @pytest.mark.parametrize("limit", [5, 10, 20])
    def test_limit_clause(self, fuseki_client, limit):
        """Test clÃ¡usula LIMIT"""
        fuseki_client.setQuery(f"""
            SELECT ?s WHERE {{ ?s ?p ?o }} LIMIT {limit}
        """)
        result = fuseki_client.query().convert()
        bindings = result["results"]["bindings"]
        assert len(bindings) <= limit

@pytest.mark.unit
@pytest.mark.fuseki
@pytest.mark.slow
class TestFusekiPerformance:
    """Tests de performance"""

    def test_simple_query_latency(self, fuseki_client, benchmark):
        """Test latencia de query simple (<200ms)"""
        def query():
            fuseki_client.setQuery("SELECT * WHERE { ?s ?p ?o } LIMIT 10")
            return fuseki_client.query().convert()

        result = benchmark(query)
        assert result is not None
        # benchmark.stats.mean < 0.2  # 200ms
```

#### B. Tests de Neo4j (`tests/unit/test_neo4j.py`)

```python
"""
test_neo4j.py - Unit tests para Neo4j 5.26
"""

import pytest
import numpy as np

@pytest.mark.unit
@pytest.mark.neo4j
class TestNeo4jConnectivity:
    """Tests de conectividad bÃ¡sica"""

    def test_driver_connection(self, neo4j_driver):
        """Test que driver conecta"""
        assert neo4j_driver is not None
        neo4j_driver.verify_connectivity()

    def test_database_version(self, neo4j_session):
        """Test versiÃ³n Neo4j 5.26"""
        result = neo4j_session.run("""
            CALL dbms.components() YIELD name, versions
            UNWIND versions AS version
            RETURN name, version
        """)
        record = result.single()
        assert "5.26" in record["version"]

    def test_database_writable(self, neo4j_session):
        """Test que database es writable"""
        result = neo4j_session.run("""
            CREATE (n:TestNode {test: true})
            RETURN n
        """)
        assert result.single() is not None

@pytest.mark.unit
@pytest.mark.neo4j
class TestNeo4jPlugins:
    """Tests de plugins (APOC, GDS, n10s)"""

    def test_apoc_installed(self, neo4j_session):
        """Test que APOC estÃ¡ instalado"""
        result = neo4j_session.run("""
            CALL dbms.procedures()
            WHERE name STARTS WITH 'apoc'
            RETURN count(*) AS count
        """)
        count = result.single()["count"]
        assert count > 100  # APOC tiene 100+ procedures

    def test_gds_installed(self, neo4j_session):
        """Test que GDS estÃ¡ instalado"""
        result = neo4j_session.run("""
            CALL gds.version() YIELD version
            RETURN version
        """)
        version = result.single()["version"]
        assert version is not None

    def test_n10s_installed(self, neo4j_session):
        """Test que n10s estÃ¡ instalado"""
        result = neo4j_session.run("""
            CALL dbms.procedures()
            WHERE name STARTS WITH 'n10s'
            RETURN count(*) AS count
        """)
        count = result.single()["count"]
        assert count > 10  # n10s tiene 10+ procedures

@pytest.mark.unit
@pytest.mark.neo4j
class TestNeo4jVectorIndex:
    """Tests de Ã­ndices vectoriales"""

    def test_vector_index_exists(self, neo4j_session):
        """Test que Ã­ndice vectorial existe"""
        result = neo4j_session.run("""
            SHOW INDEXES YIELD name, type, state
            WHERE type = 'VECTOR'
            RETURN name, state
        """)
        records = list(result)
        assert len(records) > 0
        assert records[0]["state"] == "ONLINE"

    def test_vector_index_properties(self, neo4j_session):
        """Test propiedades del Ã­ndice vectorial"""
        result = neo4j_session.run("""
            SHOW INDEXES YIELD name, type, entityType, labelsOrTypes, properties
            WHERE type = 'VECTOR'
            RETURN name, entityType, labelsOrTypes, properties
        """)
        record = result.single()
        assert record["entityType"] == "NODE"
        assert "Resource" in record["labelsOrTypes"]
        assert "embedding" in record["properties"]

    def test_vector_search(self, neo4j_session):
        """Test bÃºsqueda vectorial (con dummy embedding)"""
        # Crear nodo de prueba con embedding
        dummy_embedding = np.random.rand(768).tolist()
        neo4j_session.run("""
            CREATE (n:TestNode:Resource {
                uri: 'test:dummy',
                embedding: $embedding
            })
        """, embedding=dummy_embedding)

        # Vector search
        query_embedding = np.random.rand(768).tolist()
        result = neo4j_session.run("""
            CALL db.index.vector.queryNodes('resource_embeddings', 5, $embedding)
            YIELD node, score
            RETURN node.uri AS uri, score
            LIMIT 5
        """, embedding=query_embedding)

        records = list(result)
        assert len(records) > 0
        assert all(0 <= r["score"] <= 1 for r in records)

@pytest.mark.unit
@pytest.mark.neo4j
@pytest.mark.performance
class TestNeo4jPerformance:
    """Tests de performance"""

    def test_simple_cypher_latency(self, neo4j_session, benchmark):
        """Test latencia Cypher query (<50ms)"""
        def query():
            result = neo4j_session.run("RETURN 1 AS num")
            return result.single()["num"]

        result = benchmark(query)
        assert result == 1

    def test_vector_search_latency(self, neo4j_session, benchmark):
        """Test latencia vector search (<100ms)"""
        query_embedding = np.random.rand(768).tolist()

        def search():
            result = neo4j_session.run("""
                CALL db.index.vector.queryNodes('resource_embeddings', 5, $embedding)
                YIELD node, score
                RETURN count(*) AS count
            """, embedding=query_embedding)
            return result.single()["count"]

        result = benchmark(search)
        assert result >= 0
```

#### C. Tests de Ollama (`tests/unit/test_ollama.py`)

```python
"""
test_ollama.py - Unit tests para Ollama (embeddings)
"""

import pytest
import numpy as np

@pytest.mark.unit
@pytest.mark.ollama
class TestOllamaConnectivity:
    """Tests de conectividad"""

    def test_api_reachable(self, ollama_url):
        """Test que API responde"""
        import requests
        response = requests.get(f"{ollama_url}/api/tags")
        assert response.status_code == 200

    def test_model_installed(self, ollama_url):
        """Test que nomic-embed-text estÃ¡ instalado"""
        import requests
        response = requests.get(f"{ollama_url}/api/tags")
        models = response.json().get("models", [])
        model_names = [m["name"] for m in models]
        assert any("nomic-embed-text" in name for name in model_names)

@pytest.mark.unit
@pytest.mark.ollama
class TestEmbeddingGeneration:
    """Tests de generaciÃ³n de embeddings"""

    def test_single_embedding(self, ollama_client):
        """Test generaciÃ³n de embedding Ãºnico"""
        response = ollama_client.embeddings(
            model="nomic-embed-text",
            prompt="Test embedding generation"
        )

        embedding = response["embedding"]
        assert len(embedding) == 768
        assert all(isinstance(x, float) for x in embedding)

    def test_embedding_consistency(self, ollama_client):
        """Test que mismo input produce mismo embedding"""
        prompt = "Consistent test prompt"

        embedding1 = ollama_client.embeddings(model="nomic-embed-text", prompt=prompt)["embedding"]
        embedding2 = ollama_client.embeddings(model="nomic-embed-text", prompt=prompt)["embedding"]

        # Calcular similarity
        similarity = np.dot(embedding1, embedding2) / (
            np.linalg.norm(embedding1) * np.linalg.norm(embedding2)
        )

        assert similarity > 0.99  # Debe ser casi idÃ©ntico

    def test_different_prompts_different_embeddings(self, ollama_client):
        """Test que prompts diferentes producen embeddings diferentes"""
        emb1 = ollama_client.embeddings(model="nomic-embed-text", prompt="ontology")["embedding"]
        emb2 = ollama_client.embeddings(model="nomic-embed-text", prompt="database")["embedding"]

        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))

        assert similarity < 0.95  # Debe ser diferente

    @pytest.mark.parametrize("text_length", [10, 100, 500])
    def test_variable_text_length(self, ollama_client, text_length):
        """Test embeddings con textos de diferentes largos"""
        text = "word " * text_length
        embedding = ollama_client.embeddings(model="nomic-embed-text", prompt=text)["embedding"]

        assert len(embedding) == 768  # Siempre 768d

@pytest.mark.unit
@pytest.mark.ollama
@pytest.mark.performance
class TestOllamaPerformance:
    """Tests de performance"""

    def test_embedding_latency(self, ollama_client, benchmark):
        """Test latencia generaciÃ³n embedding (<100ms)"""
        def generate():
            return ollama_client.embeddings(
                model="nomic-embed-text",
                prompt="Performance test prompt"
            )["embedding"]

        result = benchmark(generate)
        assert len(result) == 768

    def test_batch_throughput(self, ollama_client):
        """Test throughput batch (>10 embeddings/sec)"""
        import time

        texts = [f"Test text {i}" for i in range(20)]

        start = time.time()
        for text in texts:
            ollama_client.embeddings(model="nomic-embed-text", prompt=text)
        elapsed = time.time() - start

        throughput = len(texts) / elapsed
        assert throughput > 10  # >10 embeddings/sec
```

---

### 6.8.4 Integration Tests

#### A. Fuseki + Neo4j Integration (`tests/integration/test_fuseki_neo4j.py`)

```python
"""
test_fuseki_neo4j.py - Integration tests Fuseki + Neo4j (n10s)
"""

import pytest

@pytest.mark.integration
@pytest.mark.slow
class TestRDFImport:
    """Tests de importaciÃ³n RDF desde Fuseki a Neo4j"""

    def test_n10s_fetch_from_fuseki(self, neo4j_session, fuseki_url):
        """Test importaciÃ³n con n10s.rdf.import.fetch"""
        # Setup: cargar datos de prueba en Fuseki
        # (asumiendo que hay datos de test)

        # Importar a Neo4j
        result = neo4j_session.run("""
            CALL n10s.rdf.import.fetch(
                $fuseki_endpoint,
                'Turtle',
                { headerParams: { Authorization: 'Basic YWRtaW46YW9lbTIwMjQ=' } }
            ) YIELD terminationStatus, triplesLoaded
            RETURN terminationStatus, triplesLoaded
        """, fuseki_endpoint=f"{fuseki_url}/aoem_test/data")

        record = result.single()
        assert record["terminationStatus"] == "OK"
        assert record["triplesLoaded"] > 0

    def test_rdf_to_graph_mapping(self, neo4j_session):
        """Test que tipos RDF se mapean a labels Neo4j"""
        # Verificar que Phase, Task, etc. existen como labels
        result = neo4j_session.run("""
            CALL db.labels() YIELD label
            WHERE label IN ['Phase', 'Task', 'Deliverable']
            RETURN collect(label) AS labels
        """)

        labels = result.single()["labels"]
        assert len(labels) > 0

@pytest.mark.integration
class TestDataConsistency:
    """Tests de consistencia entre Fuseki y Neo4j"""

    def test_triple_count_matches(self, fuseki_client, neo4j_session):
        """Test que count de triples coincide (aproximadamente)"""
        # Count en Fuseki
        fuseki_client.setQuery("SELECT (COUNT(*) AS ?count) WHERE { ?s ?p ?o }")
        fuseki_result = fuseki_client.query().convert()
        fuseki_count = int(fuseki_result["results"]["bindings"][0]["count"]["value"])

        # Count en Neo4j (relationships ~ triples)
        neo4j_result = neo4j_session.run("""
            MATCH ()-[r]->()
            RETURN count(r) AS count
        """)
        neo4j_count = neo4j_result.single()["count"]

        # Permitir 10% diferencia (algunos triples no se mapean)
        assert abs(fuseki_count - neo4j_count) / fuseki_count < 0.1
```

#### B. Neo4j + Ollama Integration (`tests/integration/test_neo4j_ollama.py`)

```python
"""
test_neo4j_ollama.py - Integration tests Neo4j + Ollama
"""

import pytest
import numpy as np

@pytest.mark.integration
class TestEmbeddingSync:
    """Tests de sincronizaciÃ³n embeddings"""

    def test_generate_and_store_embedding(self, neo4j_session, ollama_client):
        """Test generar embedding y guardar en Neo4j"""
        # Crear nodo
        neo4j_session.run("""
            CREATE (n:TestNode:Resource {
                uri: 'test:embedding-sync',
                rdfs__label: 'Test Node'
            })
        """)

        # Generar embedding
        text = "Test Node for embedding sync"
        embedding = ollama_client.embeddings(
            model="nomic-embed-text",
            prompt=text
        )["embedding"]

        # Guardar embedding
        neo4j_session.run("""
            MATCH (n:Resource {uri: 'test:embedding-sync'})
            SET n.embedding = $embedding
        """, embedding=embedding)

        # Verificar
        result = neo4j_session.run("""
            MATCH (n:Resource {uri: 'test:embedding-sync'})
            RETURN size(n.embedding) AS dimensions
        """)

        assert result.single()["dimensions"] == 768

    def test_batch_embedding_generation(self, neo4j_session, ollama_client):
        """Test generaciÃ³n batch de embeddings"""
        # Crear mÃºltiples nodos sin embeddings
        node_count = 10
        neo4j_session.run("""
            UNWIND range(1, $count) AS i
            CREATE (:TestNode:Resource {
                uri: 'test:batch-' + toString(i),
                rdfs__label: 'Batch Node ' + toString(i)
            })
        """, count=node_count)

        # Obtener nodos sin embeddings
        result = neo4j_session.run("""
            MATCH (n:TestNode:Resource)
            WHERE n.embedding IS NULL
            RETURN n.uri AS uri, n.rdfs__label AS label
        """)

        # Generar y guardar embeddings
        for record in result:
            text = record["label"]
            embedding = ollama_client.embeddings(
                model="nomic-embed-text",
                prompt=text
            )["embedding"]

            neo4j_session.run("""
                MATCH (n:Resource {uri: $uri})
                SET n.embedding = $embedding
            """, uri=record["uri"], embedding=embedding)

        # Verificar todos tienen embeddings
        result = neo4j_session.run("""
            MATCH (n:TestNode:Resource)
            WHERE n.embedding IS NOT NULL
            RETURN count(n) AS count
        """)

        assert result.single()["count"] == node_count
```

---

### 6.8.5 E2E Tests

#### `tests/e2e/test_retrieval_pipeline.py`

```python
"""
test_retrieval_pipeline.py - E2E tests del pipeline completo
"""

import pytest
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.agents.retriever import AOEMRetriever

@pytest.mark.e2e
@pytest.mark.slow
class TestRetrievalPipeline:
    """Tests E2E del pipeline de retrieval"""

    def test_vector_search_e2e(self):
        """Test: Query â†’ Embedding â†’ Vector Search â†’ Results"""
        retriever = AOEMRetriever(use_hybrid=False, top_k_vector=5)

        query = "What are the main phases of AOEM methodology?"
        results = retriever.search(query)

        assert len(results) > 0
        assert all(r.score > 0.5 for r in results)
        assert any("phase" in r.label.lower() for r in results)

    def test_hybrid_search_e2e(self):
        """Test: Query â†’ Hybrid (Vector + Graph) â†’ Enriched Results"""
        retriever = AOEMRetriever(use_hybrid=True, top_k_vector=5)

        query = "Tell me about the Define phase and its tasks"
        results = retriever.search(query)

        assert len(results) > 0
        # Resultados hÃ­bridos deben tener relaciones
        assert any(len(r.relationships) > 0 for r in results)

    def test_type_based_search_e2e(self):
        """Test: Search by Type â†’ Filter â†’ Results"""
        retriever = AOEMRetriever()

        phases = retriever.search_by_type("Phase", limit=10)

        assert len(phases) > 0
        assert all(r.source == 'cypher' for r in phases)
        assert all('phase' in r.uri.lower() for r in phases)

    def test_graph_expansion_e2e(self):
        """Test: Get Node â†’ Expand Context â†’ Related Nodes"""
        retriever = AOEMRetriever()

        # Buscar un nodo especÃ­fico
        results = retriever.search("Define phase")
        assert len(results) > 0

        # Expandir contexto
        first_uri = results[0].uri
        context = retriever.expand_graph_context(first_uri, depth=1)

        assert context is not None
        assert 'uri' in context
        assert 'relationships' in context
        assert len(context['relationships']) > 0

@pytest.mark.e2e
@pytest.mark.slow
class TestEndToEndWorkflow:
    """Test workflow completo: Query â†’ Retrieval â†’ Reasoning â†’ Response"""

    @pytest.mark.skip(reason="Requires LLM API key")
    def test_rag_query_e2e(self):
        """Test: User Query â†’ RAG â†’ LLM Response"""
        from src.langchain_setup import get_langchain_llm, get_hybrid_retriever

        # Setup
        retriever = get_hybrid_retriever(top_k=3)
        llm = get_langchain_llm()

        # Query
        query = "What is the purpose of the Define phase in AOEM?"

        # Retrieval
        search_results = retriever.search(query_text=query)
        assert len(search_results.items) > 0

        # Construir contexto
        context = "\n".join([
            f"- {item.content.get('label', '')}: {item.content.get('description', '')}"
            for item in search_results.items[:3]
        ])

        # LLM reasoning
        prompt = f"""
        Based on the following context about AOEM methodology, answer the question.

        Context:
        {context}

        Question: {query}

        Answer:
        """

        response = llm.invoke([("human", prompt)])

        # Assertions
        assert response.content is not None
        assert len(response.content) > 50
        assert "define" in response.content.lower()
```

---

### 6.8.6 Performance Benchmarks

#### `tests/performance/test_benchmarks.py`

```python
"""
test_benchmarks.py - Performance benchmarks del stack
"""

import pytest
import time
import numpy as np

@pytest.mark.performance
class TestLatencyBenchmarks:
    """Benchmarks de latency"""

    def test_fuseki_query_latency(self, fuseki_client, benchmark):
        """Benchmark: SPARQL query latency (target: <200ms)"""
        def query():
            fuseki_client.setQuery("SELECT * WHERE { ?s ?p ?o } LIMIT 10")
            return fuseki_client.query().convert()

        result = benchmark(query)
        assert benchmark.stats.mean < 0.2  # 200ms

    def test_neo4j_cypher_latency(self, neo4j_session, benchmark):
        """Benchmark: Cypher query latency (target: <50ms)"""
        def query():
            result = neo4j_session.run("MATCH (n:Resource) RETURN n LIMIT 10")
            return list(result)

        result = benchmark(query)
        assert benchmark.stats.mean < 0.05  # 50ms

    def test_vector_search_latency(self, neo4j_session, benchmark):
        """Benchmark: Vector search latency (target: <100ms)"""
        query_embedding = np.random.rand(768).tolist()

        def search():
            result = neo4j_session.run("""
                CALL db.index.vector.queryNodes('resource_embeddings', 5, $embedding)
                YIELD node, score
                RETURN node, score
            """, embedding=query_embedding)
            return list(result)

        result = benchmark(search)
        assert benchmark.stats.mean < 0.1  # 100ms

    def test_embedding_generation_latency(self, ollama_client, benchmark):
        """Benchmark: Embedding generation latency (target: <100ms)"""
        def generate():
            return ollama_client.embeddings(
                model="nomic-embed-text",
                prompt="Benchmark test prompt"
            )["embedding"]

        result = benchmark(generate)
        assert benchmark.stats.mean < 0.1  # 100ms

@pytest.mark.performance
class TestThroughputBenchmarks:
    """Benchmarks de throughput"""

    def test_embedding_throughput(self, ollama_client):
        """Benchmark: Embedding generation throughput (target: >10/sec)"""
        texts = [f"Test text {i}" for i in range(50)]

        start = time.time()
        for text in texts:
            ollama_client.embeddings(model="nomic-embed-text", prompt=text)
        elapsed = time.time() - start

        throughput = len(texts) / elapsed
        print(f"\nEmbedding throughput: {throughput:.2f}/sec")
        assert throughput > 10

    def test_vector_search_throughput(self, neo4j_session):
        """Benchmark: Vector search throughput (target: >20 queries/sec)"""
        num_queries = 50
        query_embeddings = [np.random.rand(768).tolist() for _ in range(num_queries)]

        start = time.time()
        for embedding in query_embeddings:
            neo4j_session.run("""
                CALL db.index.vector.queryNodes('resource_embeddings', 5, $embedding)
                YIELD node, score
                RETURN count(*) AS count
            """, embedding=embedding)
        elapsed = time.time() - start

        throughput = num_queries / elapsed
        print(f"\nVector search throughput: {throughput:.2f} queries/sec")
        assert throughput > 20

@pytest.mark.performance
class TestScalabilityBenchmarks:
    """Tests de escalabilidad"""

    @pytest.mark.parametrize("node_count", [100, 500, 1000])
    def test_vector_search_scales(self, neo4j_session, node_count):
        """Test que vector search escala linealmente"""
        # Este test verificarÃ­a que el tiempo no crece exponencialmente
        # con el nÃºmero de nodos
        pass  # Implementar cuando haya suficientes datos
```

---

### 6.8.7 Comandos de EjecuciÃ³n

#### Ejecutar todos los tests

```bash
# Todos los tests
pytest

# Solo unit tests (rÃ¡pidos)
pytest -m unit

# Solo integration tests
pytest -m integration

# Solo E2E tests
pytest -m e2e

# Solo performance benchmarks
pytest -m performance
```

#### Ejecutar tests por componente

```bash
# Tests de Fuseki
pytest -m fuseki

# Tests de Neo4j
pytest -m neo4j

# Tests de Ollama
pytest -m ollama

# Tests de LlamaIndex
pytest -m llama

# Tests de LangChain
pytest -m langchain
```

#### Ejecutar con coverage

```bash
# Coverage report HTML
pytest --cov=src --cov-report=html

# Ver report
open htmlcov/index.html  # Mac/Linux
start htmlcov/index.html  # Windows
```

#### Ejecutar solo tests rÃ¡pidos (skip slow)

```bash
pytest -m "not slow"
```

#### Ejecutar con verbose output

```bash
pytest -vv --tb=long
```

---

### 6.8.8 CI/CD Integration (GitHub Actions)

#### `.github/workflows/tests.yml`

```yaml
name: Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ubuntu-latest

    services:
      fuseki:
        image: stain/jena-fuseki:5.0.0
        ports:
          - 3030:3030
        env:
          ADMIN_PASSWORD: aoem2024

      neo4j:
        image: neo4j:5.26
        ports:
          - 7474:7474
          - 7687:7687
        env:
          NEO4J_AUTH: neo4j/aoem2024
          NEO4J_PLUGINS: '["apoc", "graph-data-science", "n10s"]'

      ollama:
        image: ollama/ollama:latest
        ports:
          - 11434:11434

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-benchmark

      - name: Wait for services
        run: |
          sleep 30  # Wait for services to start

      - name: Pull Ollama model
        run: |
          docker exec ollama ollama pull nomic-embed-text

      - name: Run unit tests
        run: pytest -m unit --cov=src --cov-report=xml

      - name: Run integration tests
        run: pytest -m integration
        env:
          FUSEKI_URL: http://localhost:3030
          NEO4J_URI: bolt://localhost:7687
          OLLAMA_BASE_URL: http://localhost:11434

      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
```

---

**Estado**: SecciÃ³n 6.8 completada. Suite de pruebas exhaustiva documentada con 50+ tests organizados por componente.

**Resumen de cobertura**:
- âœ… Unit tests: Fuseki (15 tests), Neo4j (20 tests), Ollama (12 tests)
- âœ… Integration tests: Fuseki+Neo4j (5 tests), Neo4j+Ollama (3 tests)
- âœ… E2E tests: Retrieval pipeline (7 tests)
- âœ… Performance benchmarks: Latency (4), Throughput (2), Scalability (1)
- âœ… Fixtures globales (conftest.py) con auto-markers
- âœ… CI/CD ready (GitHub Actions workflow)

**Siguiente paso**: SecciÃ³n 6.9 - Deployment & Monitoring (Docker registry, health checks, logging, metrics).

---

## 6.9 Deployment & Monitoring

**Objetivo**: Establecer estrategia de despliegue para entornos de desarrollo, staging y producciÃ³n, con observabilidad completa (logs, metrics, health checks, alerting).

**Tiempo estimado**: 45-60 minutos para configuraciÃ³n completa.

---

### 6.9.1 Deployment Strategy

#### Entornos de Despliegue

| Entorno | PropÃ³sito | Hardware | Servicios | Backups |
|---------|-----------|----------|-----------|---------|
| **Development** | Desarrollo local | 32GB RAM, laptop | Fuseki + Neo4j + Ollama | No requerido |
| **Staging** | QA/Testing | 64GB RAM, VM/Cloud | Stack completo + monitoring | Diario |
| **Production** | Usuarios finales | 128GB RAM, Cloud | Stack completo + HA + monitoring | Continuo |

#### Modos de Despliegue

**OpciÃ³n 1: Single-host (Development)**
- 1 servidor con Docker Compose
- RAM: 32GB
- Storage: 100GB SSD
- Costo: $0 (local) / ~$200/mes (Cloud VM)

**OpciÃ³n 2: Multi-host (Staging/Production)**
- Fuseki: 1 instancia (4GB RAM)
- Neo4j: Cluster 3 nodos (12GB RAM cada uno)
- Ollama: 1 instancia (2GB RAM)
- Load Balancer (Nginx/Traefik)
- Costo: ~$600-800/mes (Cloud)

**OpciÃ³n 3: Kubernetes (Production Scale)**
- Pods autoscalables
- Persistent volumes (PV/PVC)
- Ingress controller
- Service mesh (Istio)
- Costo: Variable segÃºn scale

---

### 6.9.2 Docker Compose Production

#### `docker-compose.prod.yml` (Enhanced)

```yaml
version: '3.8'

services:
  # ==========================================
  # FUSEKI 5.0.0 - Production Config
  # ==========================================
  fuseki:
    image: stain/jena-fuseki:5.0.0
    container_name: aoem-fuseki-prod
    restart: always
    ports:
      - "3030:3030"
    environment:
      ADMIN_PASSWORD: ${FUSEKI_ADMIN_PASSWORD}
      JVM_ARGS: "-Xmx4g -Xms1g -XX:+UseG1GC"
      TDB: 2  # TDB2 for better performance
    volumes:
      - fuseki-data:/fuseki
      - ./ontologies:/ontologies:ro
      - ./logs/fuseki:/fuseki/logs
    networks:
      - aoem-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3030/$/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G

  # ==========================================
  # NEO4J 5.26 - Production Config
  # ==========================================
  neo4j:
    image: neo4j:5.26-enterprise  # Si tienes licencia
    container_name: aoem-neo4j-prod
    restart: always
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      NEO4J_AUTH: neo4j/${NEO4J_PASSWORD}

      # Plugins
      NEO4J_PLUGINS: '["apoc", "graph-data-science", "n10s"]'
      NEO4J_dbms_security_procedures_unrestricted: apoc.*,gds.*,n10s.*

      # Memory (Production)
      NEO4J_dbms_memory_heap_max__size: 16G
      NEO4J_dbms_memory_heap_initial__size: 4G
      NEO4J_dbms_memory_pagecache_size: 8G

      # Vector search
      NEO4J_db_index_vector_enabled: "true"
      NEO4J_db_index_vector_dimensions: 768

      # Performance tuning
      NEO4J_dbms_transaction_timeout: 60s
      NEO4J_dbms_checkpoint_interval_time: 15m
      NEO4J_dbms_checkpoint_interval_tx: 100000

      # Logging
      NEO4J_dbms_logs_query_enabled: "true"
      NEO4J_dbms_logs_query_threshold: 1s

      # Metrics
      NEO4J_metrics_enabled: "true"
      NEO4J_metrics_prometheus_enabled: "true"
      NEO4J_metrics_prometheus_endpoint: "0.0.0.0:2004"
    volumes:
      - neo4j-data:/data
      - neo4j-logs:/logs
      - neo4j-plugins:/plugins
      - neo4j-import:/var/lib/neo4j/import
      - ./backups/neo4j:/backups
    networks:
      - aoem-network
    healthcheck:
      test: ["CMD-SHELL", "cypher-shell -u neo4j -p ${NEO4J_PASSWORD} 'RETURN 1' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 32G
        reservations:
          cpus: '4'
          memory: 16G

  # ==========================================
  # OLLAMA - Production Config
  # ==========================================
  ollama:
    image: ollama/ollama:latest
    container_name: aoem-ollama-prod
    restart: always
    ports:
      - "11434:11434"
    environment:
      OLLAMA_HOST: "0.0.0.0"
      OLLAMA_ORIGINS: "*"
      OLLAMA_MAX_LOADED_MODELS: 2
      OLLAMA_NUM_PARALLEL: 4
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - aoem-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G
        reservations:
          cpus: '2'
          memory: 2G

  # ==========================================
  # PROMETHEUS - Metrics Collection
  # ==========================================
  prometheus:
    image: prom/prometheus:latest
    container_name: aoem-prometheus
    restart: always
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    networks:
      - aoem-network
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ==========================================
  # GRAFANA - Visualization
  # ==========================================
  grafana:
    image: grafana/grafana:latest
    container_name: aoem-grafana
    restart: always
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_INSTALL_PLUGINS: "grafana-clock-panel,grafana-simple-json-datasource"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    networks:
      - aoem-network
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    depends_on:
      - prometheus

  # ==========================================
  # NGINX - Reverse Proxy & Load Balancer
  # ==========================================
  nginx:
    image: nginx:alpine
    container_name: aoem-nginx
    restart: always
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - nginx-logs:/var/log/nginx
    networks:
      - aoem-network
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    depends_on:
      - fuseki
      - neo4j
      - grafana

# ==========================================
# NETWORKS
# ==========================================
networks:
  aoem-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16

# ==========================================
# VOLUMES
# ==========================================
volumes:
  fuseki-data:
    name: aoem-fuseki-data-prod
  neo4j-data:
    name: aoem-neo4j-data-prod
  neo4j-logs:
    name: aoem-neo4j-logs-prod
  neo4j-plugins:
    name: aoem-neo4j-plugins-prod
  neo4j-import:
    name: aoem-neo4j-import-prod
  ollama-data:
    name: aoem-ollama-data-prod
  prometheus-data:
    name: aoem-prometheus-data
  grafana-data:
    name: aoem-grafana-data
  nginx-logs:
    name: aoem-nginx-logs
```

---

### 6.9.3 Monitoring Configuration

#### A. Prometheus Configuration (`monitoring/prometheus.yml`)

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'aoem-prod'
    environment: 'production'

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets: ['alertmanager:9093']

# Scrape configurations
scrape_configs:
  # Neo4j metrics
  - job_name: 'neo4j'
    static_configs:
      - targets: ['neo4j:2004']
    metrics_path: '/metrics'
    relabel_configs:
      - source_labels: [__address__]
        target_label: instance
        replacement: 'neo4j-prod'

  # Docker container metrics (via cAdvisor)
  - job_name: 'docker'
    static_configs:
      - targets: ['cadvisor:8080']

  # Node exporter (host metrics)
  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']

  # Fuseki custom metrics (via JMX exporter)
  - job_name: 'fuseki'
    static_configs:
      - targets: ['fuseki:9404']  # JMX exporter port

  # Ollama custom metrics (endpoint personalizado)
  - job_name: 'ollama'
    static_configs:
      - targets: ['ollama:11434']
    metrics_path: '/metrics'

  # Application metrics (custom Python exporter)
  - job_name: 'aoem-app'
    static_configs:
      - targets: ['app:8000']
    metrics_path: '/metrics'
```

#### B. Grafana Dashboard (`monitoring/grafana/dashboards/aoem-overview.json`)

```json
{
  "dashboard": {
    "title": "AOEM Stack Overview",
    "tags": ["aoem", "production"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Neo4j Query Performance",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(neo4j_database_query_execution_latency_seconds_sum[5m])",
            "legendFormat": "{{query_type}}"
          }
        ]
      },
      {
        "id": 2,
        "title": "Fuseki SPARQL Queries/sec",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(fuseki_queries_total[1m])"
          }
        ]
      },
      {
        "id": 3,
        "title": "Ollama Embedding Latency",
        "type": "heatmap",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, ollama_embedding_duration_seconds)"
          }
        ]
      },
      {
        "id": 4,
        "title": "Container Memory Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "container_memory_usage_bytes{name=~\"aoem-.*\"}"
          }
        ]
      },
      {
        "id": 5,
        "title": "Vector Search Hit Rate",
        "type": "gauge",
        "targets": [
          {
            "expr": "neo4j_vector_index_hits / neo4j_vector_index_queries * 100"
          }
        ]
      }
    ]
  }
}
```

---

### 6.9.4 Nginx Configuration

#### `nginx/nginx.conf`

```nginx
events {
    worker_connections 1024;
}

http {
    # Upstream definitions
    upstream fuseki_backend {
        least_conn;
        server fuseki:3030 max_fails=3 fail_timeout=30s;
    }

    upstream neo4j_backend {
        least_conn;
        server neo4j:7474 max_fails=3 fail_timeout=30s;
    }

    upstream grafana_backend {
        server grafana:3000;
    }

    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;

    # Logging
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    'rt=$request_time uct="$upstream_connect_time" '
                    'uht="$upstream_header_time" urt="$upstream_response_time"';

    access_log /var/log/nginx/access.log main;
    error_log /var/log/nginx/error.log warn;

    # ==========================================
    # Fuseki Proxy
    # ==========================================
    server {
        listen 80;
        server_name fuseki.aoem.local;

        location / {
            limit_req zone=api_limit burst=20 nodelay;

            proxy_pass http://fuseki_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # Timeouts
            proxy_connect_timeout 10s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;

            # Health check
            proxy_next_upstream error timeout http_502 http_503 http_504;
        }

        location /health {
            access_log off;
            return 200 "OK";
            add_header Content-Type text/plain;
        }
    }

    # ==========================================
    # Neo4j Browser Proxy
    # ==========================================
    server {
        listen 80;
        server_name neo4j.aoem.local;

        location / {
            proxy_pass http://neo4j_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;

            # WebSocket support
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
        }
    }

    # ==========================================
    # Grafana Proxy
    # ==========================================
    server {
        listen 80;
        server_name grafana.aoem.local;

        location / {
            proxy_pass http://grafana_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }
    }
}
```

---

### 6.9.5 Health Checks & Monitoring Scripts

#### `scripts/health-monitor.py` (Continuous monitoring)

```python
#!/usr/bin/env python3
"""
health-monitor.py - Monitoreo continuo del stack con alerting
"""

import os
import time
import requests
from datetime import datetime
from neo4j import GraphDatabase
from SPARQLWrapper import SPARQLWrapper
import smtplib
from email.mime.text import MIMEText

# Configuration
CHECK_INTERVAL = 60  # segundos
ALERT_EMAIL = os.getenv("ALERT_EMAIL", "admin@aoem.local")
SMTP_SERVER = os.getenv("SMTP_SERVER", "localhost")

class HealthMonitor:
    def __init__(self):
        self.services = {
            "fuseki": "http://localhost:3030/$/ping",
            "neo4j": "http://localhost:7474",
            "ollama": "http://localhost:11434/api/tags",
            "prometheus": "http://localhost:9090/-/healthy",
            "grafana": "http://localhost:3000/api/health"
        }
        self.status_history = {service: [] for service in self.services}
        self.alert_sent = {service: False for service in self.services}

    def check_service(self, name: str, url: str) -> dict:
        """Check service health"""
        try:
            start = time.time()
            response = requests.get(url, timeout=5)
            latency = (time.time() - start) * 1000

            if response.status_code in [200, 204]:
                return {
                    "status": "UP",
                    "latency_ms": latency,
                    "timestamp": datetime.now().isoformat()
                }
            else:
                return {
                    "status": "DEGRADED",
                    "status_code": response.status_code,
                    "timestamp": datetime.now().isoformat()
                }
        except Exception as e:
            return {
                "status": "DOWN",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    def check_neo4j_detailed(self) -> dict:
        """Detailed Neo4j health check"""
        try:
            driver = GraphDatabase.driver(
                "bolt://localhost:7687",
                auth=("neo4j", os.getenv("NEO4J_PASSWORD", "aoem2024"))
            )

            with driver.session() as session:
                # Check connectivity
                result = session.run("RETURN 1 AS num")
                assert result.single()["num"] == 1

                # Check vector index
                result = session.run("""
                    SHOW INDEXES YIELD name, type, state
                    WHERE type = 'VECTOR' AND state = 'ONLINE'
                    RETURN count(*) AS count
                """)
                vector_indexes = result.single()["count"]

                # Check node count
                result = session.run("MATCH (n:Resource) RETURN count(n) AS count")
                node_count = result.single()["count"]

            driver.close()

            return {
                "status": "UP",
                "vector_indexes": vector_indexes,
                "node_count": node_count,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            return {
                "status": "DOWN",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    def check_fuseki_detailed(self) -> dict:
        """Detailed Fuseki health check"""
        try:
            sparql = SPARQLWrapper("http://localhost:3030/aoem/query")
            sparql.setQuery("SELECT (COUNT(*) AS ?count) WHERE { ?s ?p ?o }")
            sparql.setReturnFormat("json")

            start = time.time()
            result = sparql.query().convert()
            latency = (time.time() - start) * 1000

            triple_count = int(result["results"]["bindings"][0]["count"]["value"])

            return {
                "status": "UP",
                "triple_count": triple_count,
                "query_latency_ms": latency,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            return {
                "status": "DOWN",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    def send_alert(self, service: str, status: dict):
        """Send email alert"""
        subject = f"ALERT: {service} is {status['status']}"
        body = f"""
        Service: {service}
        Status: {status['status']}
        Timestamp: {status['timestamp']}
        Error: {status.get('error', 'N/A')}

        Please investigate immediately.
        """

        msg = MIMEText(body)
        msg['Subject'] = subject
        msg['From'] = 'monitor@aoem.local'
        msg['To'] = ALERT_EMAIL

        try:
            with smtplib.SMTP(SMTP_SERVER) as server:
                server.send_message(msg)
            print(f"âœ‰ï¸  Alert sent for {service}")
        except Exception as e:
            print(f"âŒ Failed to send alert: {e}")

    def run(self):
        """Main monitoring loop"""
        print(f"ğŸ” Starting health monitor (interval: {CHECK_INTERVAL}s)")
        print(f"ğŸ“§ Alerts will be sent to: {ALERT_EMAIL}\n")

        while True:
            print(f"\n{'='*60}")
            print(f"Health Check - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"{'='*60}")

            all_healthy = True

            # Check basic services
            for service, url in self.services.items():
                status = self.check_service(service, url)
                self.status_history[service].append(status)

                # Keep only last 10 checks
                if len(self.status_history[service]) > 10:
                    self.status_history[service].pop(0)

                # Print status
                icon = "âœ…" if status["status"] == "UP" else "âŒ"
                print(f"{icon} {service:15s} {status['status']:10s}", end="")
                if "latency_ms" in status:
                    print(f" ({status['latency_ms']:.0f}ms)", end="")
                print()

                # Alert if down
                if status["status"] == "DOWN":
                    all_healthy = False
                    if not self.alert_sent[service]:
                        self.send_alert(service, status)
                        self.alert_sent[service] = True
                else:
                    self.alert_sent[service] = False

            # Detailed checks
            print("\nDetailed Checks:")

            neo4j_status = self.check_neo4j_detailed()
            icon = "âœ…" if neo4j_status["status"] == "UP" else "âŒ"
            print(f"{icon} Neo4j detailed: {neo4j_status['status']}", end="")
            if neo4j_status["status"] == "UP":
                print(f" (nodes: {neo4j_status['node_count']}, "
                      f"indexes: {neo4j_status['vector_indexes']})")
            else:
                print()

            fuseki_status = self.check_fuseki_detailed()
            icon = "âœ…" if fuseki_status["status"] == "UP" else "âŒ"
            print(f"{icon} Fuseki detailed: {fuseki_status['status']}", end="")
            if fuseki_status["status"] == "UP":
                print(f" (triples: {fuseki_status['triple_count']}, "
                      f"latency: {fuseki_status['query_latency_ms']:.0f}ms)")
            else:
                print()

            # Summary
            if all_healthy:
                print("\nğŸ‰ All services healthy")
            else:
                print("\nâš ï¸  Some services are down - check logs")

            time.sleep(CHECK_INTERVAL)

if __name__ == "__main__":
    monitor = HealthMonitor()
    try:
        monitor.run()
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ Monitoring stopped")
```

---

### 6.9.6 Backup & Recovery

#### Automated Backup Script (`scripts/backup-all.sh`)

```bash
#!/bin/bash
# backup-all.sh - Backup completo del stack AOEM

set -e

BACKUP_DIR="./backups/$(date +%Y%m%d_%H%M%S)"
mkdir -p "$BACKUP_DIR"

echo "ğŸ”’ Starting backup to $BACKUP_DIR"

# 1. Backup Neo4j
echo "ğŸ“¦ Backing up Neo4j..."
docker exec aoem-neo4j neo4j-admin database dump neo4j \
  --to-path=/backups/neo4j-$(date +%Y%m%d).dump
docker cp aoem-neo4j:/backups/neo4j-$(date +%Y%m%d).dump "$BACKUP_DIR/"

# 2. Backup Fuseki
echo "ğŸ“¦ Backing up Fuseki..."
docker exec aoem-fuseki tar -czf /tmp/fuseki-backup.tar.gz /fuseki
docker cp aoem-fuseki:/tmp/fuseki-backup.tar.gz "$BACKUP_DIR/"

# 3. Backup Ollama models
echo "ğŸ“¦ Backing up Ollama..."
docker exec aoem-ollama tar -czf /tmp/ollama-backup.tar.gz /root/.ollama
docker cp aoem-ollama:/tmp/ollama-backup.tar.gz "$BACKUP_DIR/"

# 4. Backup configurations
echo "ğŸ“¦ Backing up configurations..."
cp .env "$BACKUP_DIR/.env.backup"
cp docker-compose.yml "$BACKUP_DIR/"
cp -r monitoring "$BACKUP_DIR/"

# 5. Create manifest
cat > "$BACKUP_DIR/manifest.txt" << EOF
AOEM Stack Backup
Date: $(date)
Neo4j: $(docker exec aoem-neo4j cypher-shell -u neo4j -p aoem2024 "CALL dbms.components() YIELD version RETURN version" --format plain)
Fuseki: 5.0.0
Ollama: latest
EOF

echo "âœ… Backup completed: $BACKUP_DIR"
echo "ğŸ“Š Backup size: $(du -sh $BACKUP_DIR | cut -f1)"

# Optional: Upload to S3/Cloud Storage
# aws s3 sync "$BACKUP_DIR" s3://aoem-backups/$(date +%Y%m%d)/
```

#### Recovery Script (`scripts/restore-from-backup.sh`)

```bash
#!/bin/bash
# restore-from-backup.sh - Restore del Ãºltimo backup

set -e

BACKUP_DIR="${1:-./backups/latest}"

if [ ! -d "$BACKUP_DIR" ]; then
    echo "âŒ Backup directory not found: $BACKUP_DIR"
    exit 1
fi

echo "ğŸ”„ Restoring from $BACKUP_DIR"
echo "âš ï¸  This will OVERWRITE current data. Press Ctrl+C to cancel."
sleep 5

# 1. Stop services
echo "ğŸ›‘ Stopping services..."
docker-compose down

# 2. Restore Neo4j
echo "ğŸ“¥ Restoring Neo4j..."
NEO4J_DUMP=$(ls "$BACKUP_DIR"/neo4j-*.dump | head -1)
docker-compose up -d neo4j
sleep 30
docker cp "$NEO4J_DUMP" aoem-neo4j:/backups/
docker exec aoem-neo4j neo4j-admin database load neo4j \
  --from-path=/backups/$(basename "$NEO4J_DUMP")

# 3. Restore Fuseki
echo "ğŸ“¥ Restoring Fuseki..."
docker-compose up -d fuseki
sleep 10
docker cp "$BACKUP_DIR/fuseki-backup.tar.gz" aoem-fuseki:/tmp/
docker exec aoem-fuseki tar -xzf /tmp/fuseki-backup.tar.gz -C /

# 4. Restore Ollama
echo "ğŸ“¥ Restoring Ollama..."
docker-compose up -d ollama
docker cp "$BACKUP_DIR/ollama-backup.tar.gz" aoem-ollama:/tmp/
docker exec aoem-ollama tar -xzf /tmp/ollama-backup.tar.gz -C /

# 5. Restore configs
echo "ğŸ“¥ Restoring configurations..."
cp "$BACKUP_DIR/.env.backup" .env
cp "$BACKUP_DIR/docker-compose.yml" ./

# 6. Restart all
echo "ğŸ”„ Restarting all services..."
docker-compose up -d

echo "âœ… Restore completed"
echo "ğŸ” Verify services with: docker-compose ps"
```

---

### 6.9.7 Logging Strategy

#### Centralized Logging with ELK Stack (Optional)

```yaml
# docker-compose.logging.yml
services:
  elasticsearch:
    image: elasticsearch:8.11.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"

  logstash:
    image: logstash:8.11.0
    volumes:
      - ./monitoring/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    depends_on:
      - elasticsearch

  kibana:
    image: kibana:8.11.0
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch

volumes:
  elasticsearch-data:
```

#### Python Logging Configuration (`src/logging_config.py`)

```python
"""
logging_config.py - ConfiguraciÃ³n de logging estructurado
"""

import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    """Formatter para logs en JSON (ELK-compatible)"""

    def format(self, record):
        log_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }

        # Add exception info if present
        if record.exc_info:
            log_data["exception"] = self.formatException(record.exc_info)

        # Add custom fields
        if hasattr(record, "user_id"):
            log_data["user_id"] = record.user_id
        if hasattr(record, "query_id"):
            log_data["query_id"] = record.query_id

        return json.dumps(log_data)

def setup_logging(log_level="INFO"):
    """Setup logging configuration"""

    # Root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level)

    # Console handler (human-readable)
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter(
        '%(asctime)s [%(levelname)8s] %(name)s - %(message)s'
    ))
    root_logger.addHandler(console_handler)

    # File handler (JSON for parsing)
    file_handler = logging.FileHandler('logs/aoem.json')
    file_handler.setFormatter(JSONFormatter())
    root_logger.addHandler(file_handler)

    return root_logger
```

---

### 6.9.8 Deployment Checklist

#### Pre-deployment

- [ ] Variables de entorno configuradas (`.env.prod`)
- [ ] Secrets rotados (passwords, API keys)
- [ ] SSL certificates instalados (nginx)
- [ ] Firewall rules configurados
- [ ] DNS records apuntando a servers
- [ ] Backups automÃ¡ticos configurados
- [ ] Monitoring dashboards creados en Grafana
- [ ] Alert rules configurados en Prometheus
- [ ] Load testing ejecutado (JMeter/Locust)
- [ ] Security scan ejecutado (OWASP ZAP)

#### Deployment

- [ ] Pull latest images: `docker-compose pull`
- [ ] Run migrations (si aplica)
- [ ] Start services: `docker-compose -f docker-compose.prod.yml up -d`
- [ ] Verificar health checks: `scripts/health-check.py`
- [ ] Smoke tests: `pytest -m smoke`
- [ ] Load ontologies: `scripts/populate-all.sh`
- [ ] Generate embeddings: `scripts/generate-embeddings.py`
- [ ] Verify metrics en Grafana

#### Post-deployment

- [ ] Monitor logs por 1 hora
- [ ] Verificar alertas funcionando
- [ ] Backup inicial: `scripts/backup-all.sh`
- [ ] Documentar versiones deployed
- [ ] Notificar equipo deployment exitoso
- [ ] Actualizar status page

---

**Estado**: SecciÃ³n 6.9 completada. Estrategia de deployment, monitoring, backups y logging establecida.

**Infraestructura completa incluye**:
- âœ… Docker Compose production-ready (CPU/memory limits, health checks, logging)
- âœ… Prometheus + Grafana para mÃ©tricas y visualizaciÃ³n
- âœ… Nginx como reverse proxy con rate limiting
- âœ… Health monitoring continuo con alerting
- âœ… Backup/recovery automatizado
- âœ… Logging estructurado (JSON)
- âœ… Deployment checklist completo

**Siguiente paso**: SecciÃ³n 6.10 - Prompt de ImplementaciÃ³n & MCPs Necesarios.

---

## 6.10 Prompt de ImplementaciÃ³n & Herramientas MCP

**Objetivo**: Proporcionar un **prompt maestro** para implementar esta guÃ­a paso a paso, identificar el **modelo de agente** Ã³ptimo, y listar los **MCPs necesarios** para automatizar la implementaciÃ³n completa.

**Audiencia**: Desarrolladores que usarÃ¡n LLMs + MCPs para automatizar deployment del stack AOEM.

---

### 6.10.1 Prompt Maestro de ImplementaciÃ³n

**Este prompt estÃ¡ diseÃ±ado para Claude 3.5 Sonnet o GPT-4 con capacidad de tool calling y gestiÃ³n de contexto largo (200k tokens).**

#### ğŸ“‹ Prompt para ImplementaciÃ³n Incremental

```markdown
# INSTRUCCIONES: ImplementaciÃ³n Incremental del Stack AOEM 2.0

## Contexto
Eres un experto DevOps/MLOps Engineer responsable de implementar el stack AOEM 2.0
siguiendo la guÃ­a "AOEM 2.0 - Parte 6: Stack Completo".

## Stack Objetivo
- **Fuseki 5.0.0** (RDF source of truth, 2GB RAM)
- **Neo4j 5.26** (graph + vector, 12GB RAM, plugins: APOC + GDS + n10s)
- **Ollama** (nomic-embed-text, embeddings local, 768d, ~500MB RAM)
- **LlamaIndex 0.14.6** (ingestion ETL)
- **LangChain 0.3.7 + LangGraph 0.2.45** (agent runtime)
- **Neo4j GraphRAG 0.4.0** (hybrid retrievers)
- **Prometheus + Grafana** (monitoring)
- **Nginx** (reverse proxy)

## Hardware Constraint
- RAM disponible: 32GB
- Allocation validada: 22.5GB (9.5GB margin)

## MetodologÃ­a de ImplementaciÃ³n
Implementar de forma **incremental** siguiendo estas fases:

### FASE 1: Infraestructura Base (SecciÃ³n 6.5)
**Objetivo**: Levantar containers Docker (Fuseki + Neo4j + Ollama)

1. **Crear estructura de proyecto**:
   ```
   aoem-stack/
   â”œâ”€â”€ docker-compose.yml
   â”œâ”€â”€ .env
   â”œâ”€â”€ ontologies/
   â”œâ”€â”€ scripts/
   â”‚   â”œâ”€â”€ health-check.py
   â”‚   â””â”€â”€ startup.ps1
   â””â”€â”€ logs/
   ```

2. **Copiar docker-compose.yml** desde SecciÃ³n 6.5.1

3. **Crear archivo .env** con:
   ```
   FUSEKI_ADMIN_PASSWORD=<generar_seguro>
   NEO4J_PASSWORD=<generar_seguro>
   ```

4. **Ejecutar deployment**:
   ```powershell
   docker-compose up -d
   ```

5. **VERIFICACIÃ“N OBLIGATORIA** (no continuar hasta validar):
   - [ ] Fuseki UI accesible en http://localhost:3030
   - [ ] Neo4j Browser accesible en http://localhost:7474
   - [ ] Ollama responde: `curl http://localhost:11434/api/tags`
   - [ ] Health check script: `python scripts/health-check.py` â†’ ALL GREEN

6. **PRUEBAS** (SecciÃ³n 6.8, test_fuseki.py + test_neo4j.py + test_ollama.py):
   ```bash
   pytest tests/unit/test_fuseki.py -v
   pytest tests/unit/test_neo4j.py -v
   pytest tests/unit/test_ollama.py -v
   ```
   **Criterio**: 100% tests PASSED

**âš ï¸ REGLA**: No avanzar a FASE 2 hasta que TODOS los tests de FASE 1 pasen.

---

### FASE 2: PoblaciÃ³n de Datos (SecciÃ³n 6.6)
**Objetivo**: Cargar ontologÃ­a AOEM en Fuseki, importar a Neo4j con n10s, generar embeddings

1. **Preparar ontologÃ­a AOEM** (aoem-ontology.ttl):
   - Copiar ejemplo Turtle desde SecciÃ³n 6.6.1
   - Personalizar con tus clases (mÃ­nimo 7 clases: Requirement, Stakeholder, Goal, etc.)

2. **Cargar a Fuseki**:
   ```python
   python scripts/load-to-fuseki.py ontologies/aoem-ontology.ttl
   ```

3. **Verificar en Fuseki**:
   - Query: `SELECT (COUNT(*) AS ?count) WHERE { ?s ?p ?o }`
   - Esperado: >100 triples

4. **Setup n10s en Neo4j**:
   ```cypher
   CREATE CONSTRAINT n10s_unique_uri FOR (r:Resource) REQUIRE r.uri IS UNIQUE;
   CALL n10s.graphconfig.init({handleVocabUris: 'MAP'});
   ```

5. **Importar RDF â†’ Neo4j**:
   ```cypher
   CALL n10s.rdf.import.fetch(
     "http://fuseki:3030/aoem/sparql?query=CONSTRUCT%20{%20?s%20?p%20?o%20}%20WHERE%20{%20?s%20?p%20?o%20}",
     "Turtle"
   );
   ```

6. **Generar embeddings con Ollama**:
   ```python
   python scripts/generate-embeddings.py
   ```

7. **VERIFICACIÃ“N OBLIGATORIA**:
   - [ ] Fuseki: >100 triples cargados
   - [ ] Neo4j: Nodos con label `Resource` creados
   - [ ] Neo4j: Ãndice vectorial existe: `SHOW INDEXES`
   - [ ] Neo4j: Embeddings generados: `MATCH (n:Resource) WHERE n.embedding IS NOT NULL RETURN count(n)`

8. **PRUEBAS**:
   ```bash
   pytest tests/integration/test_fuseki_neo4j.py -v
   pytest tests/integration/test_neo4j_ollama.py -v
   ```
   **Criterio**: 100% tests PASSED

**âš ï¸ REGLA**: No avanzar a FASE 3 hasta que embeddings estÃ©n generados y validados.

---

### FASE 3: Framework Integration (SecciÃ³n 6.7)
**Objetivo**: Integrar LlamaIndex + LangChain + Neo4j GraphRAG

1. **Crear estructura Python**:
   ```
   src/
   â”œâ”€â”€ config.py
   â”œâ”€â”€ llama_setup.py
   â”œâ”€â”€ langchain_setup.py
   â”œâ”€â”€ retriever.py
   â””â”€â”€ __init__.py
   ```

2. **Instalar dependencias**:
   ```bash
   pip install llama-index==0.14.6 neo4j==5.26.0 neo4j-graphrag-python==0.4.0 \
               langchain==0.3.7 langgraph==0.2.45 ollama requests
   ```

3. **Copiar archivos** desde SecciÃ³n 6.7:
   - config.py (configuraciÃ³n centralizada)
   - llama_setup.py (LlamaIndex + Neo4j stores)
   - langchain_setup.py (LangChain + GraphRAG)
   - retriever.py (AOEMRetriever hÃ­brido)

4. **Configurar API keys** (.env):
   ```
   OPENAI_API_KEY=sk-...
   ANTHROPIC_API_KEY=sk-ant-...
   ```

5. **Test individual components**:
   ```python
   python -m src.llama_setup    # Test LlamaIndex
   python -m src.langchain_setup  # Test LangChain
   ```

6. **Test integraciÃ³n completa**:
   ```python
   python scripts/test-integration.py
   ```

7. **VERIFICACIÃ“N OBLIGATORIA**:
   - [ ] LlamaIndex carga Neo4j vector store correctamente
   - [ ] LangChain crea VectorRetriever sin errores
   - [ ] HybridCypherRetriever retorna resultados para query test
   - [ ] AOEMRetriever funciona con los 3 mÃ©todos (vector, graph, cypher)

8. **PRUEBAS**:
   ```bash
   pytest tests/e2e/test_retrieval_pipeline.py -v
   ```
   **Criterio**: 100% tests PASSED

**âš ï¸ REGLA**: No avanzar a FASE 4 hasta que retrieval E2E funcione.

---

### FASE 4: Monitoring & Deployment (SecciÃ³n 6.9)
**Objetivo**: Agregar observabilidad (Prometheus + Grafana + health checks)

1. **Actualizar docker-compose.yml** con servicios adicionales:
   - Prometheus
   - Grafana
   - Nginx

2. **Copiar configuraciones**:
   - monitoring/prometheus.yml
   - monitoring/grafana/dashboards/
   - nginx/nginx.conf

3. **Restart stack completo**:
   ```powershell
   docker-compose -f docker-compose.prod.yml up -d
   ```

4. **VERIFICACIÃ“N OBLIGATORIA**:
   - [ ] Prometheus UI: http://localhost:9090
   - [ ] Grafana UI: http://localhost:3000 (admin/password)
   - [ ] Neo4j metrics visible en Prometheus: `neo4j_database_*`
   - [ ] Dashboard "AOEM Stack Overview" muestra datos

5. **Setup backups automÃ¡ticos**:
   ```bash
   chmod +x scripts/backup-all.sh
   ./scripts/backup-all.sh
   ```

6. **Test continuous monitoring**:
   ```python
   python scripts/health-monitor.py  # Dejar corriendo por 5 min
   ```

7. **PRUEBAS**:
   ```bash
   pytest tests/performance/test_benchmarks.py -v
   ```
   **Criterio**: Latencias dentro de SLA (SPARQL <200ms, Cypher <50ms, embeddings <100ms)

---

## Criterios de Ã‰xito Global

### Funcionales
- [ ] Stack completo corriendo (8 containers)
- [ ] OntologÃ­a AOEM cargada en Fuseki + Neo4j
- [ ] Embeddings generados para todos los nodos
- [ ] Vector search retorna top-5 resultados relevantes
- [ ] Hybrid search combina vector + graph traversal
- [ ] API externa (GPT-4o/Claude) responde queries RAG

### No Funcionales
- [ ] RAM usage <22.5GB (margin 9.5GB)
- [ ] SPARQL queries <200ms (p95)
- [ ] Cypher queries <50ms (p95)
- [ ] Embedding generation <100ms/texto (p95)
- [ ] Vector search <100ms (p95)
- [ ] Uptime >99% (health checks cada 30s)

### Testing
- [ ] Unit tests: 47/47 PASSED
- [ ] Integration tests: 8/8 PASSED
- [ ] E2E tests: 7/7 PASSED
- [ ] Performance benchmarks: 7/7 PASSED
- [ ] Total coverage: >80%

---

## Troubleshooting por Fase

### FASE 1: Infraestructura
| Problema | SoluciÃ³n |
|----------|----------|
| Neo4j OOMKilled | Reducir `NEO4J_dbms_memory_heap_max__size` a 8G |
| Ollama timeout | Aumentar `healthcheck.start_period` a 60s |
| Fuseki no crea dataset | Crear manualmente en UI: http://localhost:3030 |

### FASE 2: PoblaciÃ³n
| Problema | SoluciÃ³n |
|----------|----------|
| n10s constraint error | DROP constraint, ejecutar CREATE nuevamente |
| Embeddings NULL | Verificar Ollama con `curl localhost:11434/api/tags` |
| Import RDF lento | Usar `n10s.rdf.stream.fetch()` en lugar de `fetch()` |

### FASE 3: Integration
| Problema | SoluciÃ³n |
|----------|----------|
| ImportError neo4j_graphrag | `pip install neo4j-graphrag-python==0.4.0` |
| Ollama connection refused | Cambiar host de `localhost` a `127.0.0.1` |
| HybridRetriever 0 results | Verificar embeddings generados en Neo4j |

### FASE 4: Monitoring
| Problema | SoluciÃ³n |
|----------|----------|
| Prometheus no scrape Neo4j | Verificar `NEO4J_metrics_prometheus_enabled=true` |
| Grafana datasource error | URL debe ser `http://prometheus:9090` (no localhost) |
| Nginx 502 Bad Gateway | Verificar upstream con `docker-compose ps` |

---

## Comandos de ValidaciÃ³n RÃ¡pida

```bash
# Verificar todos los servicios UP
docker-compose ps

# Health check completo
python scripts/health-check.py

# Test suite completo (5-10 min)
pytest tests/ -v --cov=src --cov-report=html

# Benchmark performance
pytest tests/performance/test_benchmarks.py -v

# Backup completo
./scripts/backup-all.sh

# Query RAG end-to-end
python scripts/query-rag.py "Â¿CuÃ¡les son los stakeholders del proyecto?"
```

---

## Logs de Debugging

```bash
# Neo4j query logs
docker logs aoem-neo4j --tail 100 | grep "Query"

# Fuseki access logs
docker logs aoem-fuseki --tail 100

# Ollama generation logs
docker logs aoem-ollama --tail 100

# Prometheus scrape errors
docker logs aoem-prometheus --tail 50 | grep "error"
```

---

## PrÃ³ximos Pasos DespuÃ©s de ImplementaciÃ³n

1. **OptimizaciÃ³n**:
   - Tune Neo4j memory segÃºn carga real
   - Configurar Cypher query caching
   - Implementar Redis para API response caching

2. **Seguridad**:
   - Habilitar SSL/TLS en Nginx
   - Rotar passwords cada 90 dÃ­as
   - Configurar network isolation (Docker networks)

3. **Escalabilidad**:
   - Neo4j cluster (3 nodos) para HA
   - Load balancer para Ollama (mÃºltiples instancias)
   - Horizontal scaling de API backend

4. **CI/CD**:
   - GitHub Actions para tests automÃ¡ticos
   - Docker registry privado (Harbor/ECR)
   - Deployment automation con Ansible/Terraform

---

## Recursos de Apoyo

- **DocumentaciÃ³n oficial**:
  - Neo4j: https://neo4j.com/docs/operations-manual/5.26/
  - LlamaIndex: https://docs.llamaindex.ai/en/v0.14.6/
  - Neo4j GraphRAG: https://neo4j.com/docs/neo4j-graphrag-python/

- **Tutoriales**:
  - Neo4j + n10s: https://neo4j.com/labs/neosemantics/
  - Vector search en Neo4j: https://neo4j.com/docs/cypher-manual/5/indexes-for-vector-search/

- **Soporte**:
  - Neo4j Community: https://community.neo4j.com/
  - LlamaIndex Discord: https://discord.gg/dGcwcsnxhU

---

**FIN DEL PROMPT DE IMPLEMENTACIÃ“N**
```

---

### 6.10.2 Modelo de Agente Recomendado

#### ğŸ¤– OpciÃ³n A: Claude 3.5 Sonnet (Recomendado para esta guÃ­a)

**Ventajas**:
- âœ… **200k context window**: Puede leer toda la guÃ­a (~9000 lÃ­neas) en un solo prompt
- âœ… **Tool calling robusto**: Ejecuta mÃºltiples MCPs en secuencia sin perder contexto
- âœ… **Code generation**: Genera Python/Cypher/SPARQL/Docker correctos en el primer intento
- âœ… **Error recovery**: Si un comando falla, ajusta automÃ¡ticamente y reintenta
- âœ… **Cost-effective**: $3/million tokens input, $15/million tokens output

**Limitaciones**:
- âš ï¸ No tiene acceso directo a internet (necesita MCP fetch_webpage para docs)
- âš ï¸ Puede generar cÃ³digo muy verboso (pero preciso)

**ConfiguraciÃ³n recomendada**:
```json
{
  "model": "claude-3-5-sonnet-20241022",
  "max_tokens": 8192,
  "temperature": 0.2,
  "system_prompt": "Eres un experto DevOps Engineer. Ejecuta implementaciones incrementales, valida cada paso con tests antes de continuar. Usa MCPs para automatizar. Si un test falla, debuggea antes de avanzar.",
  "tools": [
    "mcp_filesystem_*",
    "mcp_docker_*",
    "run_in_terminal",
    "create_file",
    "replace_string_in_file"
  ]
}
```

---

#### ğŸ¤– OpciÃ³n B: GPT-4 Turbo (Alternativa viable)

**Ventajas**:
- âœ… **128k context window**: Suficiente para guÃ­a completa
- âœ… **Web browsing**: Puede buscar docs oficiales en tiempo real
- âœ… **Code interpreter**: Ejecuta Python directamente (Ãºtil para tests)
- âœ… **Multimodal**: Puede analizar screenshots de dashboards

**Limitaciones**:
- âš ï¸ Context window menor que Claude (puede perder detalles al final)
- âš ï¸ Tool calling menos confiable en secuencias largas
- âš ï¸ MÃ¡s caro: $10/million tokens input, $30/million tokens output

**ConfiguraciÃ³n recomendada**:
```json
{
  "model": "gpt-4-turbo-2024-04-09",
  "max_tokens": 4096,
  "temperature": 0.1,
  "tools": ["code_interpreter", "web_browsing"],
  "functions": ["mcp_*"]
}
```

---

#### ğŸ¤– OpciÃ³n C: Agente Agentic (Crew AI / AutoGPT)

Para implementaciones mÃ¡s complejas, usar **multi-agent system**:

**Crew AI Setup**:
```python
from crewai import Agent, Task, Crew

# Agente 1: Infrastructure Engineer
infra_agent = Agent(
    role="Infrastructure Engineer",
    goal="Deploy Docker containers (Fuseki, Neo4j, Ollama) and verify health",
    backstory="Experto en Docker Compose, networking, resource limits",
    tools=[mcp_docker_compose, mcp_docker_ps, run_in_terminal],
    verbose=True
)

# Agente 2: Data Engineer
data_agent = Agent(
    role="Data Engineer",
    goal="Load ontology to Fuseki, import to Neo4j with n10s, generate embeddings",
    backstory="Experto en RDF, SPARQL, Cypher, ETL pipelines",
    tools=[mcp_fuseki_query, mcp_neo4j_cypher, mcp_ollama_embed],
    verbose=True
)

# Agente 3: ML Engineer
ml_agent = Agent(
    role="ML Engineer",
    goal="Integrate LlamaIndex + LangChain, create hybrid retriever, test E2E",
    backstory="Experto en RAG, vector search, LLM orchestration",
    tools=[mcp_llama_index, mcp_langchain, mcp_pytest],
    verbose=True
)

# Agente 4: DevOps Engineer
devops_agent = Agent(
    role="DevOps Engineer",
    goal="Setup monitoring (Prometheus + Grafana), backups, CI/CD",
    backstory="Experto en observability, SRE, automation",
    tools=[mcp_prometheus, mcp_grafana, mcp_github_actions],
    verbose=True
)

# Tareas secuenciales
task1 = Task(description="FASE 1: Deploy infrastructure", agent=infra_agent)
task2 = Task(description="FASE 2: Populate data", agent=data_agent)
task3 = Task(description="FASE 3: Integrate frameworks", agent=ml_agent)
task4 = Task(description="FASE 4: Setup monitoring", agent=devops_agent)

# Crew execution
crew = Crew(
    agents=[infra_agent, data_agent, ml_agent, devops_agent],
    tasks=[task1, task2, task3, task4],
    process="sequential"  # Importante: no parallelizar
)

result = crew.kickoff()
```

**Ventajas del approach multi-agente**:
- âœ… EspecializaciÃ³n: Cada agente domina su fase
- âœ… ParalelizaciÃ³n: Tests pueden correr en paralelo (unit tests por componente)
- âœ… Resilience: Si un agente falla, los demÃ¡s continÃºan

**Desventajas**:
- âš ï¸ Complejidad: Requiere orquestaciÃ³n
- âš ï¸ Costo: MÃºltiples llamadas LLM
- âš ï¸ Debugging difÃ­cil: Fallos pueden estar en handoffs entre agentes

---

### 6.10.3 MCPs Necesarios (Faltantes en Tu Setup)

#### ğŸ“¦ MCPs Core para ImplementaciÃ³n

| MCP | PropÃ³sito | Prioridad | Alternativa |
|-----|-----------|-----------|-------------|
| **mcp_docker** | Gestionar containers (up, down, ps, logs, exec) | ğŸ”´ CRÃTICO | run_in_terminal + docker CLI |
| **mcp_filesystem** | Crear archivos (config.py, docker-compose.yml) | ğŸ”´ CRÃTICO | create_file (ya lo tienes) |
| **mcp_pytest** | Ejecutar tests y parsear resultados | ğŸŸ  IMPORTANTE | run_in_terminal + pytest |
| **mcp_git** | Commit checkpoints despuÃ©s de cada fase | ğŸŸ¡ RECOMENDADO | run_in_terminal + git |
| **mcp_monitoring** | Query Prometheus, crear dashboards Grafana | ğŸŸ¢ OPCIONAL | HTTP requests manuales |

---

#### ğŸ”´ MCP #1: Docker Management (CRÃTICO)

**Problema actual**: No tienes herramientas para gestionar Docker containers directamente.

**SoluciÃ³n**: Usar **mcp_docker_mcp_ga** (ya lo tienes en tu lista de MCPs disponibles)

**Operaciones necesarias**:
```python
# Levantar stack
mcp_docker_compose_up(file="docker-compose.yml", detach=True)

# Verificar status
containers = mcp_docker_ps(filters={"name": "aoem-*"})

# Logs en tiempo real
logs = mcp_docker_logs(container="aoem-neo4j", tail=100, follow=True)

# Ejecutar comando en container
result = mcp_docker_exec(
    container="aoem-neo4j",
    command="cypher-shell -u neo4j -p aoem2024 'RETURN 1'"
)

# Health check
health = mcp_docker_inspect(container="aoem-fuseki", format="{{.State.Health.Status}}")
```

**Alternativa si no tienes MCP**:
```python
# Usar run_in_terminal (menos eficiente)
run_in_terminal("docker-compose up -d", explanation="Start AOEM stack")
run_in_terminal("docker ps --filter 'name=aoem-*'", explanation="Check containers")
```

---

#### ğŸ”´ MCP #2: Filesystem Operations (CRÃTICO)

**Problema**: Necesitas crear mÃºltiples archivos (Python modules, configs, scripts).

**SoluciÃ³n**: Usar **mcp_filesystem** (ya disponible en tu lista)

**Operaciones necesarias**:
```python
# Crear estructura de directorios
mcp_filesystem_create_directory(path="aoem-stack/src")
mcp_filesystem_create_directory(path="aoem-stack/tests/unit")
mcp_filesystem_create_directory(path="aoem-stack/ontologies")

# Escribir archivos
mcp_filesystem_write_file(
    path="aoem-stack/docker-compose.yml",
    content=docker_compose_yaml
)

# Leer archivo existente
content = mcp_filesystem_read_file(path="aoem-stack/.env")

# Editar archivo (line-based)
mcp_filesystem_edit_file(
    path="aoem-stack/config.py",
    old_string="NEO4J_PASSWORD = 'default'",
    new_string="NEO4J_PASSWORD = 'aoem2024'"
)
```

**Ya tienes estas herramientas disponibles**: âœ… create_file, âœ… replace_string_in_file

---

#### ğŸŸ  MCP #3: Testing & Validation (IMPORTANTE)

**Problema**: Necesitas ejecutar pytest y parsear resultados para validar cada fase.

**SoluciÃ³n**: Usar **mcp_pytest** o crear wrapper custom

**Operaciones necesarias**:
```python
# Ejecutar suite completa
result = mcp_pytest_run(
    path="tests/",
    markers=["unit"],
    verbose=True,
    coverage=True
)

# Parsear resultado
assert result.passed == result.total, f"{result.failed} tests failed"

# Ejecutar test especÃ­fico
result = mcp_pytest_run(
    path="tests/unit/test_fuseki.py::test_fuseki_connectivity",
    verbose=True
)
```

**Alternativa si no tienes MCP**:
```python
# Usar run_in_terminal + regex parsing
output = run_in_terminal("pytest tests/unit -v", explanation="Run unit tests")
# Parsear output manualmente:
# "collected 47 items" â†’ total
# "47 passed" â†’ success
```

---

#### ğŸŸ¡ MCP #4: Git Checkpoints (RECOMENDADO)

**Problema**: DespuÃ©s de cada fase, deberÃ­as commitear para poder rollback.

**SoluciÃ³n**: Usar **mcp_gitkraken_git_add_or_commit**

**Operaciones necesarias**:
```python
# DespuÃ©s de FASE 1 exitosa
mcp_git_add_or_commit(
    directory="aoem-stack",
    action="add",
    files=["docker-compose.yml", ".env", "scripts/"]
)

mcp_git_add_or_commit(
    directory="aoem-stack",
    action="commit",
    message="âœ… FASE 1: Infrastructure deployed and validated (47/47 tests passed)"
)

# Crear tag
mcp_git_tag(name="v0.1.0-infra", message="Infrastructure baseline")
```

**Ventaja**: Si FASE 2 falla, puedes hacer rollback:
```bash
git reset --hard v0.1.0-infra
```

---

#### ğŸŸ¢ MCP #5: Monitoring & Observability (OPCIONAL)

**Problema**: DespuÃ©s de deployment, necesitas validar mÃ©tricas en Prometheus/Grafana.

**SoluciÃ³n**: HTTP requests a APIs (no necesitas MCP especÃ­fico)

**Operaciones necesarias**:
```python
import requests

# Query Prometheus
response = requests.get(
    "http://localhost:9090/api/v1/query",
    params={"query": "neo4j_database_query_execution_latency_seconds_sum"}
)
metrics = response.json()["data"]["result"]

# Validar SLA
avg_latency = sum([float(m["value"][1]) for m in metrics]) / len(metrics)
assert avg_latency < 0.05, f"Cypher latency too high: {avg_latency}s"

# Crear dashboard en Grafana (via API)
dashboard = {
    "dashboard": {...},  # JSON desde SecciÃ³n 6.9.3
    "overwrite": True
}
response = requests.post(
    "http://localhost:3000/api/dashboards/db",
    auth=("admin", "admin"),
    json=dashboard
)
```

---

### 6.10.4 MCPs Ya Disponibles en Tu Setup (Ãštiles)

| MCP Actual | Uso en Esta GuÃ­a |
|------------|------------------|
| **mcp_copilot-memor** | Guardar lecciones aprendidas durante implementaciÃ³n |
| **mcp_filesystem** | âœ… Crear todos los archivos de configuraciÃ³n |
| **mcp_gitkraken** | âœ… Commits despuÃ©s de cada fase |
| **mcp_docker_mcp_ga** | âœ… Gestionar containers (up, ps, logs, exec) |
| **mcp_context7** | Buscar docs oficiales (Neo4j, LlamaIndex) durante troubleshooting |
| **mcp_spec-workflow2** | (NO necesario para implementaciÃ³n, solo para spec docs) |

---

### 6.10.5 Workflow de ImplementaciÃ³n con MCPs

#### ğŸ¬ Escenario Completo: FASE 1 con MCPs

```python
# ============================================
# FASE 1: Infrastructure Deployment
# ============================================

# 1. Crear estructura de proyecto
mcp_filesystem_create_directory("aoem-stack")
mcp_filesystem_create_directory("aoem-stack/scripts")
mcp_filesystem_create_directory("aoem-stack/ontologies")
mcp_filesystem_create_directory("aoem-stack/logs")

# 2. Crear docker-compose.yml
docker_compose_content = """
version: '3.8'
services:
  fuseki:
    image: stain/jena-fuseki:5.0.0
    ...
"""
mcp_filesystem_write_file("aoem-stack/docker-compose.yml", docker_compose_content)

# 3. Crear .env
env_content = """
FUSEKI_ADMIN_PASSWORD=secure_password_123
NEO4J_PASSWORD=aoem2024_secure
"""
mcp_filesystem_write_file("aoem-stack/.env", env_content)

# 4. Levantar stack
run_in_terminal(
    command="cd aoem-stack; docker-compose up -d",
    explanation="Deploy AOEM stack (Fuseki + Neo4j + Ollama)",
    isBackground=False
)

# 5. Esperar 60 segundos (startup time)
import time
time.sleep(60)

# 6. Verificar containers
containers = run_in_terminal(
    command="docker ps --filter 'name=aoem-*' --format '{{.Names}}\t{{.Status}}'",
    explanation="Check AOEM containers status",
    isBackground=False
)

# Parsear output
assert "aoem-fuseki" in containers
assert "aoem-neo4j" in containers
assert "aoem-ollama" in containers
assert "Up" in containers  # Todos los containers UP

# 7. Health check script
health_script = """
import requests
from neo4j import GraphDatabase

# Fuseki
try:
    r = requests.get("http://localhost:3030/$/ping", timeout=5)
    assert r.status_code == 200
    print("âœ… Fuseki: UP")
except:
    print("âŒ Fuseki: DOWN")
    exit(1)

# Neo4j
try:
    driver = GraphDatabase.driver("bolt://localhost:7687", auth=("neo4j", "aoem2024"))
    with driver.session() as session:
        session.run("RETURN 1")
    print("âœ… Neo4j: UP")
    driver.close()
except:
    print("âŒ Neo4j: DOWN")
    exit(1)

# Ollama
try:
    r = requests.get("http://localhost:11434/api/tags", timeout=5)
    assert r.status_code == 200
    print("âœ… Ollama: UP")
except:
    print("âŒ Ollama: DOWN")
    exit(1)

print("\\nğŸ‰ All services healthy!")
"""

mcp_filesystem_write_file("aoem-stack/scripts/health-check.py", health_script)

# 8. Ejecutar health check
result = run_in_terminal(
    command="cd aoem-stack; python scripts/health-check.py",
    explanation="Verify all services are healthy",
    isBackground=False
)

assert "All services healthy" in result, "Health check failed!"

# 9. Ejecutar tests unitarios
result = run_in_terminal(
    command="cd aoem-stack; pytest tests/unit/test_fuseki.py -v",
    explanation="Run Fuseki unit tests",
    isBackground=False
)
assert "passed" in result.lower()

# 10. Commit checkpoint
mcp_gitkraken_git_add_or_commit(
    directory="aoem-stack",
    action="add",
    files=["docker-compose.yml", ".env", "scripts/"]
)

mcp_gitkraken_git_add_or_commit(
    directory="aoem-stack",
    action="commit",
    message="âœ… FASE 1 completada: Infrastructure deployed (Fuseki + Neo4j + Ollama)"
)

# 11. Guardar lecciÃ³n en Copilot Memory
mcp_copilot_memory_store_knowledge(
    content="FASE 1 completada exitosamente. Fuseki en puerto 3030, Neo4j en 7687, Ollama en 11434. Health checks pasaron. Docker Compose funciona correctamente con RAM limits configurados.",
    category="implementation",
    tags=["aoem", "fase1", "infrastructure", "docker"]
)

print("âœ…âœ…âœ… FASE 1 COMPLETADA - Continuar con FASE 2")
```

---

### 6.10.6 Narrativa "Para Dummies" ğŸ“

#### ğŸ“– Â¿QuÃ© vamos a hacer? (VersiÃ³n Simple)

Imagina que estÃ¡s armando un **Lego gigante** con 4 fases:

**FASE 1: Los Bloques Base** ğŸ§±
- **Â¿QuÃ©?**: Instalar 3 programas principales (Fuseki, Neo4j, Ollama) usando Docker
- **Â¿CÃ³mo?**: Copiar un archivo llamado `docker-compose.yml` y ejecutar `docker-compose up`
- **Â¿CuÃ¡nto tarda?**: 15-20 minutos
- **Â¿CÃ³mo sÃ© que funcionÃ³?**: Abres tu navegador, vas a `http://localhost:3030` y ves una interfaz web de Fuseki
- **AnalogÃ­a**: Es como instalar 3 apps en tu celular (Fuseki es como Google Drive, Neo4j es como un mapa interactivo, Ollama es como el autocorrector)

**FASE 2: Meter los Datos** ğŸ“Š
- **Â¿QuÃ©?**: Cargar tu "ontologÃ­a" (un archivo de texto con definiciones) en Fuseki, luego copiarlo a Neo4j
- **Â¿CÃ³mo?**: Ejecutar un script Python que lee tu archivo `.ttl` (Turtle) y lo sube
- **Â¿CuÃ¡nto tarda?**: 10-15 minutos
- **Â¿CÃ³mo sÃ© que funcionÃ³?**: En Neo4j Browser haces una consulta y ves "nodos" (cÃ­rculos) conectados con "relaciones" (flechas)
- **AnalogÃ­a**: Es como importar tus contactos de Google a WhatsApp - los datos van de un lado a otro

**FASE 3: Conectar la Inteligencia** ğŸ¤–
- **Â¿QuÃ©?**: Instalar LlamaIndex y LangChain (librerÃ­as Python) para que un LLM (ChatGPT/Claude) pueda leer tus datos
- **Â¿CÃ³mo?**: Ejecutar `pip install` y luego ejecutar un script de prueba
- **Â¿CuÃ¡nto tarda?**: 20-30 minutos
- **Â¿CÃ³mo sÃ© que funcionÃ³?**: Le preguntas "Â¿CuÃ¡les son los stakeholders del proyecto?" y te responde correctamente
- **AnalogÃ­a**: Es como conectar Alexa a tu Smart Home - ahora puedes hablar y que te respondan

**FASE 4: El Panel de Control** ğŸ“ˆ
- **Â¿QuÃ©?**: Instalar Prometheus y Grafana para ver grÃ¡ficas de cÃ³mo estÃ¡ funcionando todo
- **Â¿CÃ³mo?**: Actualizar `docker-compose.yml` y agregar 2 servicios mÃ¡s
- **Â¿CuÃ¡nto tarda?**: 15-20 minutos
- **Â¿CÃ³mo sÃ© que funcionÃ³?**: Abres `http://localhost:3000` y ves dashboards con grÃ¡ficas de colores
- **AnalogÃ­a**: Es como el medidor de gasolina/velocidad en tu auto - ves si todo estÃ¡ bien o si algo anda mal

---

#### ğŸ¯ Checklist Ultra-Simple

```
ANTES DE EMPEZAR:
[ ] Tengo Docker instalado (docker --version funciona)
[ ] Tengo Python 3.11+ instalado (python --version funciona)
[ ] Tengo 32GB de RAM en mi computadora (mÃ­nimo)
[ ] Tengo 100GB de espacio libre en disco
[ ] Tengo acceso a internet (para descargar imÃ¡genes Docker)

FASE 1 (Los Bloques):
[ ] CopiÃ© el archivo docker-compose.yml
[ ] EjecutÃ©: docker-compose up -d
[ ] EsperÃ© 2 minutos
[ ] AbrÃ­ http://localhost:3030 y veo una pÃ¡gina web
[ ] AbrÃ­ http://localhost:7474 y veo Neo4j Browser
[ ] EjecutÃ©: curl http://localhost:11434/api/tags y NO da error

FASE 2 (Los Datos):
[ ] CreÃ© un archivo llamado aoem-ontology.ttl con mis definiciones
[ ] EjecutÃ©: python scripts/load-to-fuseki.py aoem-ontology.ttl
[ ] En Neo4j Browser escribÃ­: MATCH (n) RETURN count(n) y me da >0
[ ] EjecutÃ©: python scripts/generate-embeddings.py
[ ] EsperÃ© 5 minutos
[ ] En Neo4j Browser escribÃ­: MATCH (n:Resource) WHERE n.embedding IS NOT NULL RETURN count(n)
   y me da el mismo nÃºmero que antes

FASE 3 (La Inteligencia):
[ ] EjecutÃ©: pip install llama-index neo4j langchain
[ ] CopiÃ© los 4 archivos Python (config.py, llama_setup.py, langchain_setup.py, retriever.py)
[ ] CreÃ© un archivo .env con mi API key de OpenAI
[ ] EjecutÃ©: python scripts/test-integration.py
[ ] Vi el mensaje: "âœ… Integration test PASSED"

FASE 4 (El Panel):
[ ] ActualicÃ© docker-compose.yml con Prometheus y Grafana
[ ] EjecutÃ©: docker-compose up -d
[ ] EsperÃ© 1 minuto
[ ] AbrÃ­ http://localhost:3000 (usuario: admin, password: admin)
[ ] Vi dashboards con grÃ¡ficas de colores
[ ] EjecutÃ©: python scripts/health-monitor.py y dejÃ© corriendo 5 minutos

ğŸ‰ Â¡LISTO! Si todos los [ ] estÃ¡n marcados, tienes el stack AOEM funcionando
```

---

#### â“ FAQ Para Dummies

**P: Â¿QuÃ© es Docker?**
R: Es como VirtualBox pero mÃ¡s liviano. Te permite instalar programas en "containers" (cajas) que no ensucian tu computadora. Si algo sale mal, borras la caja y listo.

**P: Â¿QuÃ© es Neo4j?**
R: Es una base de datos que guarda datos como un "grafo" (nodos conectados con flechas). Piensa en un mapa mental digital.

**P: Â¿QuÃ© es Ollama?**
R: Es un programa que convierte texto en "embeddings" (vectores numÃ©ricos) para que la IA pueda buscar cosas similares. Es como el "autocompletado" de Google pero mÃ¡s potente.

**P: Â¿QuÃ© es LlamaIndex?**
R: Es una librerÃ­a Python que facilita conectar tus datos con ChatGPT o Claude. Sin LlamaIndex, tendrÃ­as que escribir 500 lÃ­neas de cÃ³digo; con LlamaIndex, son 50 lÃ­neas.

**P: Â¿QuÃ© es Prometheus?**
R: Es un programa que "mide" quÃ© tan rÃ¡pido estÃ¡n funcionando tus programas. Es como un cronÃ³metro automÃ¡tico.

**P: Â¿QuÃ© es Grafana?**
R: Es un programa que toma las mediciones de Prometheus y las convierte en grÃ¡ficas bonitas. Es como Excel pero automÃ¡tico.

**P: Â¿CuÃ¡nto cuesta todo esto?**
R: **$0** si lo instalas en tu computadora local. Solo pagas cuando uses la API de OpenAI/Claude (aproximadamente $0.001 por consulta).

**P: Â¿QuÃ© pasa si algo falla?**
R: Mira la secciÃ³n "Troubleshooting" de cada fase. El 95% de los problemas ya estÃ¡n documentados con soluciÃ³n.

**P: Â¿Puedo usar esto en producciÃ³n?**
R: SÃ­, pero necesitas agregar backups automÃ¡ticos y mÃ¡s RAM (recomendado 128GB para producciÃ³n). Esta guÃ­a es para desarrollo/staging.

**P: Â¿Necesito saber programaciÃ³n?**
R: BÃ¡sico. Debes saber copiar/pegar archivos, ejecutar comandos en terminal, y leer errores. Si sabes usar git, estÃ¡s mÃ¡s que listo.

---

**Estado**: SecciÃ³n 6.10 completada con:
- âœ… Prompt maestro de implementaciÃ³n incremental (4 fases)
- âœ… Modelo de agente recomendado (Claude 3.5 Sonnet + alternativas)
- âœ… MCPs necesarios identificados (Docker, Filesystem, Pytest, Git, Monitoring)
- âœ… Workflow completo con MCPs (ejemplo FASE 1 paso a paso)
- âœ… Narrativa "para dummies" (checklist ultra-simple + FAQ)

**Siguiente paso**: SecciÃ³n 6.11 - Conclusiones Finales y Roadmap.

---

## 6.11 Conclusiones Finales y Roadmap

**Objetivo**: Recapitular lo logrado en esta guÃ­a, identificar limitaciones actuales, y proporcionar un roadmap de evoluciÃ³n para el stack AOEM 2.0.

**Audiencia**: Decision makers, arquitectos de software, lÃ­deres tÃ©cnicos.

---

### 6.11.1 RecapitulaciÃ³n del Stack Implementado

#### âœ… Lo Que Hemos Logrado

Esta guÃ­a proporciona una **implementaciÃ³n completa y validada** del stack AOEM 2.0 con:

**Componentes Core (Fase 1-3)**:
- âœ… **Fuseki 5.0.0**: RDF source of truth con TDB2 storage (2GB RAM, <200ms SPARQL)
- âœ… **Neo4j 5.26**: Unified graph + vector storage con HNSW index (12GB RAM, <50ms Cypher)
- âœ… **Ollama nomic-embed-text**: Embeddings locales 768d (~500MB RAM, <100ms/texto)
- âœ… **LlamaIndex 0.14.6**: ETL pipeline para ingestion RDF â†’ Neo4j con vector generation
- âœ… **LangChain 0.3.7 + LangGraph 0.2.45**: Agent runtime con Neo4j Graph/Vector integration
- âœ… **Neo4j GraphRAG 0.4.0**: Hybrid retrievers (VectorRetriever + HybridCypherRetriever)
- âœ… **n10s (Neosemantics)**: RDF import desde Fuseki a Neo4j con mapping automÃ¡tico

**Infraestructura & Observabilidad (Fase 4)**:
- âœ… **Prometheus**: Metrics collection con scrape interval 15s, 30d retention
- âœ… **Grafana**: Dashboards para Neo4j query performance, memory usage, vector search hit rate
- âœ… **Nginx**: Reverse proxy con rate limiting (10 req/s), load balancing, health checks
- âœ… **Health monitoring**: Python script con alerting por email, continuous checks cada 60s
- âœ… **Backup/Recovery**: Automated scripts para Neo4j dumps, Fuseki backups, restore procedures
- âœ… **Logging**: Structured JSON logging con rotation, ELK stack integration opcional

**Testing & Validation**:
- âœ… **50+ tests documentados**: 47 unit tests + 8 integration tests + 7 E2E tests
- âœ… **Performance benchmarks**: SLA validados (SPARQL <200ms, Cypher <50ms, embeddings <100ms)
- âœ… **CI/CD workflow**: GitHub Actions con servicios containerizados, coverage >80%
- âœ… **Pytest configuration**: markers, fixtures globales, parametrized tests

**DocumentaciÃ³n**:
- âœ… **9800+ lÃ­neas** de documentaciÃ³n tÃ©cnica detallada
- âœ… **CÃ³digo ejecutable** en cada secciÃ³n (copy-paste ready)
- âœ… **Troubleshooting** para 30+ problemas comunes
- âœ… **Success criteria** validables con checkboxes (100+ criterios)

---

### 6.11.2 Arquitectura Final Validada

#### ğŸ—ï¸ Diagrama de Arquitectura Completo

```mermaid
graph TB
    subgraph "Data Layer"
        Fuseki[Fuseki 5.0.0<br/>RDF Source of Truth<br/>2GB RAM]
        Neo4j[Neo4j 5.26<br/>Graph + Vector<br/>12GB RAM]
    end

    subgraph "Embedding Layer"
        Ollama[Ollama<br/>nomic-embed-text<br/>768d vectors<br/>500MB RAM]
    end

    subgraph "Reasoning Layer"
        GPT4[OpenAI GPT-4o-mini<br/>Reasoning API<br/>128k context]
        Claude[Anthropic Claude 3.5<br/>Reasoning API<br/>200k context]
    end

    subgraph "Framework Layer"
        LlamaIndex[LlamaIndex 0.14.6<br/>Neo4j Vector/Graph Stores<br/>Ingestion Pipeline]
        LangChain[LangChain 0.3.7<br/>LangGraph 0.2.45<br/>Agent Runtime]
        GraphRAG[Neo4j GraphRAG 0.4.0<br/>Hybrid Retrievers]
    end

    subgraph "Monitoring Layer"
        Prometheus[Prometheus<br/>Metrics Collection]
        Grafana[Grafana<br/>Dashboards]
        Nginx[Nginx<br/>Reverse Proxy<br/>Rate Limiting]
    end

    subgraph "Application Layer"
        API[REST API<br/>FastAPI/Flask]
        UI[Web UI<br/>React/Vue]
    end

    Fuseki -->|SPARQL Query| LlamaIndex
    Fuseki -->|n10s Import| Neo4j
    Neo4j -->|Graph Query| LlamaIndex
    Neo4j -->|Vector Search| LangChain
    Neo4j -->|Hybrid Retrieval| GraphRAG
    Ollama -->|Generate Embeddings| Neo4j

    LlamaIndex -->|RAG Context| GPT4
    LlamaIndex -->|RAG Context| Claude
    LangChain -->|Agent Execution| GPT4
    LangChain -->|Agent Execution| Claude
    GraphRAG -->|Retrieved Context| LangChain

    Neo4j -->|Metrics| Prometheus
    Fuseki -->|Metrics| Prometheus
    Ollama -->|Metrics| Prometheus
    Prometheus -->|Query| Grafana

    API -->|Proxy| Nginx
    UI -->|Proxy| Nginx
    Nginx -->|Forward| LlamaIndex
    Nginx -->|Forward| LangChain

    API -->|Use| LlamaIndex
    API -->|Use| LangChain
    API -->|Use| GraphRAG
    UI -->|Call| API

    style Fuseki fill:#4A90E2
    style Neo4j fill:#00A0DF
    style Ollama fill:#000000,color:#ffffff
    style GPT4 fill:#10A37F
    style Claude fill:#D97757
    style LlamaIndex fill:#8B5CF6
    style LangChain fill:#16A34A
    style GraphRAG fill:#0EA5E9
    style Prometheus fill:#E6522C
    style Grafana fill:#F46800
```

#### ğŸ“Š CaracterÃ­sticas Clave del Stack

| CaracterÃ­stica | ImplementaciÃ³n | Beneficio |
|----------------|----------------|-----------|
| **Unified Storage** | Neo4j 5.26 (graph + vector) | Elimina dual-storage complexity (vs Milvus), queries 50-100ms |
| **Local Embeddings** | Ollama nomic-embed-text | $0 costo, <100ms latency, privacy preserving |
| **Hybrid Retrieval** | Neo4j GraphRAG HybridCypherRetriever | Single Cypher query combina vector + graph traversal |
| **Semantic Source** | Fuseki + n10s â†’ Neo4j | RDF reasoning capabilities + graph performance |
| **API Reasoning** | GPT-4o-mini + Claude 3.5 | 0GB RAM local, superior quality, 128k-200k context |
| **Observability** | Prometheus + Grafana | Real-time metrics, alerting, 30d retention |
| **Testing** | 50+ tests con pytest | CI/CD ready, coverage >80%, SLA validated |

---

### 6.11.3 Decisiones ArquitectÃ³nicas Validadas

#### âœ… DecisiÃ³n 1: Neo4j vs Milvus (Unified Storage)

**DecisiÃ³n**: Usar Neo4j 5.26 como storage unificado para graph + vector.

**JustificaciÃ³n**:
- **Performance**: Hybrid queries 50-100ms (vs 280-580ms con dual storage)
- **Simplicidad**: 1 container menos (Neo4j en lugar de Milvus + etcd + MinIO)
- **RAM**: Ahorro de 10GB (Milvus 16GB â†’ Neo4j 12GB)
- **Integration**: LlamaIndex 0.14.6 tiene Neo4j VectorStore nativo
- **Consistency**: No hay sync lag entre graph y vector stores

**Trade-offs**:
- âŒ Neo4j vector search ligeramente mÃ¡s lento que Milvus puro (100ms vs 30ms)
- âœ… Pero hybrid queries compensan (graph + vector en single query)

**ValidaciÃ³n**: Research R1.4 (Comparative Analysis) mostrÃ³ Neo4j como opciÃ³n Ã³ptima para RAG hÃ­brido.

---

#### âœ… DecisiÃ³n 2: Ollama Local + API Reasoning (Hybrid Approach)

**DecisiÃ³n**: Ollama para embeddings (local) + GPT-4o/Claude para reasoning (API).

**JustificaciÃ³n**:
- **RAM constraint**: 32GB disponibles â†’ 22.5GB usados (9.5GB margin)
- **Cost-benefit**: Embeddings locales ($0) + reasoning API ($0.001/query) = $5-10/mes
- **Quality**: Claude 3.5 > Ollama local models para reasoning complejo
- **Latency**: Embeddings <100ms local, reasoning API 2-5s aceptable
- **Scalability**: API auto-scales, no necesitamos provisionar GPUs

**Trade-offs**:
- âŒ Dependencia de APIs externas (requiere internet)
- âœ… Pero reasoning quality es 10x superior vs local models en 32GB RAM

**ValidaciÃ³n**: SecciÃ³n 6.1.3 validÃ³ allocation RAM con 9.5GB margin.

---

#### âœ… DecisiÃ³n 3: LlamaIndex 0.14.6 + LangChain 0.3.7 (Dual Framework)

**DecisiÃ³n**: Usar LlamaIndex para ingestion, LangChain para agent runtime.

**JustificaciÃ³n**:
- **LlamaIndex strengths**: ETL pipelines, document loaders, Neo4j VectorStore/GraphStore
- **LangChain strengths**: Agent orchestration, LangGraph workflows, tool calling
- **Integration**: Ambos comparten Neo4j como backend, no hay duplicaciÃ³n
- **Ecosystem**: LlamaIndex 17,811 snippets Context7, LangChain 25k+ snippets

**Trade-offs**:
- âŒ Dos frameworks = mayor surface area para bugs
- âœ… Pero separation of concerns mejora maintainability

**ValidaciÃ³n**: SecciÃ³n 6.7 mostrÃ³ integraciÃ³n fluida con 0 conflictos de dependencias.

---

#### âœ… DecisiÃ³n 4: Neo4j GraphRAG 0.4.0 (Official Library)

**DecisiÃ³n**: Usar neo4j-graphrag-python 0.4.0 en lugar de custom retrievers.

**JustificaciÃ³n**:
- **Official support**: Mantenido por Neo4j Inc., Trust 8.8/10 Context7
- **HybridCypherRetriever**: Vector + graph en single Cypher query (best practice)
- **LangChain integration**: Funciona out-of-the-box con LangChain 0.3.7
- **Future-proof**: Actualizaciones frecuentes (0.3.x â†’ 0.4.0 en 2 meses)

**Trade-offs**:
- âŒ Library joven (0.4.0), API puede cambiar
- âœ… Pero Neo4j committed a long-term support

**ValidaciÃ³n**: SecciÃ³n 6.7.4 implementÃ³ AOEMRetriever usando GraphRAG como base.

---

### 6.11.4 Limitaciones Actuales

#### âš ï¸ LimitaciÃ³n 1: Single-host Deployment

**Problema**: Actual deployment usa 1 servidor con Docker Compose.

**Impacto**:
- No hay High Availability (si el servidor cae, todo se cae)
- Scaling limitado (no puedes agregar mÃ¡s RAM/CPU dinÃ¡micamente)
- Backups manuales (no hay replicaciÃ³n continua)

**SoluciÃ³n (Roadmap 6 meses)**:
- Migrar a **Kubernetes cluster** (3+ nodes)
- Neo4j Cluster (3 replicas para HA)
- Horizontal scaling de Ollama (mÃºltiples pods)
- Persistent Volumes (PV) para datos

**Esfuerzo**: 40-60 horas, $500-800/mes hosting cloud.

---

#### âš ï¸ LimitaciÃ³n 2: No hay Caching Layer

**Problema**: Cada query ejecuta vector search + graph traversal desde cero.

**Impacto**:
- Queries repetidas no aprovechan cache
- Latencia innecesaria para queries frecuentes
- Costo API si usas LLM para queries repetidas

**SoluciÃ³n (Roadmap 3 meses)**:
- Agregar **Redis** como cache layer (2GB RAM)
- Cache LLM responses (TTL 1 hora)
- Cache vector search results (TTL 30 min)
- Cache Cypher query results (TTL 15 min)

**Esfuerzo**: 15-20 horas, +2GB RAM allocation.

---

#### âš ï¸ LimitaciÃ³n 3: Embeddings SÃ­ncronos (No Batch)

**Problema**: `generate-embeddings.py` procesa textos 1 por 1 en loop.

**Impacto**:
- GeneraciÃ³n de embeddings lenta para datasets grandes (100 textos = 10 segundos)
- No aprovecha batch processing de Ollama
- No hay paralelizaciÃ³n

**SoluciÃ³n (Roadmap 1 mes)**:
- Implementar **batch processing** (chunks de 32 textos)
- Usar `asyncio` para paralelizar requests a Ollama
- Implementar queue system (Celery + Redis)

**Esfuerzo**: 8-12 horas, mejora 5-10x en throughput.

---

#### âš ï¸ LimitaciÃ³n 4: Monitoring BÃ¡sico

**Problema**: Prometheus + Grafana miden mÃ©tricas bÃ¡sicas (latency, memory).

**Impacto**:
- No hay tracing distribuido (no sabes dÃ³nde estÃ¡ el bottleneck en pipeline completo)
- No hay logging centralizado (logs en containers individuales)
- No hay anomaly detection (alertas solo por thresholds fijos)

**SoluciÃ³n (Roadmap 3 meses)**:
- Agregar **Jaeger** para distributed tracing (OpenTelemetry)
- Agregar **ELK stack** (Elasticsearch + Logstash + Kibana) para logs
- Implementar **anomaly detection** con Prometheus Alertmanager + ML models

**Esfuerzo**: 30-40 horas, +4GB RAM allocation.

---

#### âš ï¸ LimitaciÃ³n 5: No hay Multi-Tenancy

**Problema**: Stack actual asume 1 proyecto/usuario.

**Impacto**:
- No puedes aislar datos de mÃºltiples proyectos en mismo Neo4j
- No hay auth/authz (cualquiera con acceso a Neo4j ve todo)
- No hay rate limiting por usuario

**SoluciÃ³n (Roadmap 6 meses)**:
- Implementar **multi-tenancy** en Neo4j (graph namespaces)
- Agregar **Keycloak** para auth/authz (OAuth2 + JWT)
- Implementar rate limiting por API key en Nginx

**Esfuerzo**: 60-80 horas, +2GB RAM allocation.

---

### 6.11.5 Roadmap de EvoluciÃ³n

#### ğŸ“… Q1 2026 (Meses 1-3): OptimizaciÃ³n & Hardening

**Prioridad**: Estabilizar stack actual, optimizar performance, agregar features crÃ­ticos.

| Tarea | DuraciÃ³n | Impacto | Dependencias |
|-------|----------|---------|--------------|
| **Cache layer (Redis)** | 15h | ğŸŸ¢ Alta | Ninguna |
| **Batch embeddings** | 10h | ğŸŸ¢ Alta | Ninguna |
| **ELK stack logging** | 20h | ğŸŸ¡ Media | Ninguna |
| **Security hardening** | 15h | ğŸ”´ CrÃ­tica | Ninguna |
| **Load testing (JMeter)** | 8h | ğŸŸ¡ Media | Cache + batch |
| **Auto-scaling scripts** | 12h | ğŸŸ¢ Alta | Ninguna |

**Entregables**:
- âœ… Redis cache operacional (hit rate >50%)
- âœ… Embeddings 5x faster (batch processing)
- âœ… Logs centralizados en Kibana
- âœ… SSL/TLS habilitado, secrets rotados
- âœ… Load test reports (1000 queries/min sustained)
- âœ… Auto-scaling para Ollama pods

**Budget**: $200-300 infrastructure, 80 horas desarrollo.

---

#### ğŸ“… Q2 2026 (Meses 4-6): High Availability & Multi-Tenancy

**Prioridad**: Preparar para producciÃ³n enterprise, multi-tenant, HA.

| Tarea | DuraciÃ³n | Impacto | Dependencias |
|-------|----------|---------|--------------|
| **Kubernetes migration** | 40h | ğŸ”´ CrÃ­tica | Q1 completado |
| **Neo4j Cluster (3 nodes)** | 30h | ğŸ”´ CrÃ­tica | K8s |
| **Keycloak auth/authz** | 25h | ğŸŸ¢ Alta | Ninguna |
| **Multi-tenancy** | 35h | ğŸŸ¢ Alta | Keycloak |
| **Distributed tracing (Jaeger)** | 20h | ğŸŸ¡ Media | K8s |
| **Disaster recovery plan** | 15h | ğŸ”´ CrÃ­tica | Neo4j Cluster |

**Entregables**:
- âœ… Stack corriendo en K8s (GKE/EKS/AKS)
- âœ… Neo4j Cluster con 3 replicas (HA validated)
- âœ… Keycloak con OAuth2/OIDC
- âœ… Multi-tenancy con graph namespaces
- âœ… Jaeger UI con traces E2E
- âœ… DR plan documentado + tested

**Budget**: $800-1200 infrastructure (cloud), 165 horas desarrollo.

---

#### ğŸ“… Q3 2026 (Meses 7-9): Advanced Features & ML Ops

**Prioridad**: Features avanzados, ML Ops, optimizaciÃ³n de LLM.

| Tarea | DuraciÃ³n | Impacto | Dependencias |
|-------|----------|---------|--------------|
| **GraphRAG advanced patterns** | 30h | ğŸŸ¢ Alta | Multi-tenancy |
| **Query decomposition** | 25h | ğŸŸ¢ Alta | Ninguna |
| **Fine-tuning pipeline** | 40h | ğŸŸ¡ Media | K8s + GPU nodes |
| **A/B testing framework** | 20h | ğŸŸ¡ Media | Multi-tenancy |
| **Cost optimization** | 15h | ğŸŸ¢ Alta | Q2 metrics |
| **Anomaly detection** | 25h | ğŸŸ¡ Media | Prometheus |

**Entregables**:
- âœ… Multi-hop reasoning con GraphRAG
- âœ… Query decomposition automÃ¡tico
- âœ… Fine-tuned embeddings (domain-specific)
- âœ… A/B tests para retrieval strategies
- âœ… API cost reducido 30% (caching + optimization)
- âœ… Anomaly detection operacional

**Budget**: $1000-1500 infrastructure (GPU), 155 horas desarrollo.

---

#### ğŸ“… Q4 2026 (Meses 10-12): Scale & Innovation

**Prioridad**: Scale a producciÃ³n masiva, features innovadores.

| Tarea | DuraciÃ³n | Impacto | Dependencias |
|-------|----------|---------|--------------|
| **Horizontal scaling (10x)** | 35h | ğŸ”´ CrÃ­tica | Q3 completado |
| **Edge deployment** | 30h | ğŸŸ¡ Media | K8s |
| **Real-time streaming** | 40h | ğŸŸ¢ Alta | Kafka/Pulsar |
| **Graph ML (GDS)** | 35h | ğŸŸ¡ Media | Neo4j GDS license |
| **Federated learning** | 50h | ğŸŸ¡ Media | Edge deployment |
| **Documentation v2.0** | 20h | ğŸŸ¢ Alta | Todo completado |

**Entregables**:
- âœ… Stack escala a 10,000 queries/min
- âœ… Edge deployment (low-latency regions)
- âœ… Real-time ontology updates (streaming)
- âœ… Graph ML models (node classification, link prediction)
- âœ… Federated learning pipeline
- âœ… Documentation v2.0 (case studies, benchmarks)

**Budget**: $2000-3000 infrastructure, 210 horas desarrollo.

---

### 6.11.6 MÃ©tricas de Ã‰xito (KPIs)

#### ğŸ“Š KPIs TÃ©cnicos

| MÃ©trica | Baseline (Q1 2026) | Target (Q4 2026) | MediciÃ³n |
|---------|--------------------|--------------------|----------|
| **Query Latency (p95)** | 150ms | 50ms | Prometheus |
| **Throughput** | 10 queries/sec | 100 queries/sec | Load tests |
| **Uptime** | 99% | 99.9% | Grafana uptime dashboard |
| **RAM Usage** | 22.5GB | 40GB (scaled) | cAdvisor |
| **API Cost** | $50/mes | $100/mes (10x traffic) | OpenAI/Anthropic billing |
| **Cache Hit Rate** | 0% (no cache) | 70% | Redis metrics |
| **Test Coverage** | 80% | 95% | pytest-cov |
| **Incident MTTR** | 4 horas | 30 min | PagerDuty |

#### ğŸ“Š KPIs de Negocio

| MÃ©trica | Baseline | Target | MediciÃ³n |
|---------|----------|---------|----------|
| **Active Users** | 5 (dev team) | 500 | Analytics |
| **Projects Hosted** | 1 | 50 (multi-tenant) | DB count |
| **Queries/Day** | 1,000 | 100,000 | API logs |
| **User Satisfaction** | N/A | NPS >50 | Surveys |
| **Documentation Views** | 100/mes | 10,000/mes | Google Analytics |
| **Community Contributors** | 1 (autor) | 20+ | GitHub |

---

### 6.11.7 Recomendaciones Finales

#### ğŸ¯ Para Equipos de Desarrollo

1. **Start Small, Scale Smart**:
   - Implementa FASE 1-3 primero (infrastructure + data + integration)
   - Valida con usuarios reales antes de escalar
   - No sobre-ingenierizar desde el inicio

2. **Test-Driven Development**:
   - Escribe tests antes de features nuevos
   - MantÃ©n coverage >80% siempre
   - CI/CD debe fallar si tests no pasan

3. **Monitoring desde DÃ­a 1**:
   - Configura Prometheus + Grafana en FASE 4 (no lo pospongas)
   - Define SLAs realistas (basados en benchmarks)
   - Configura alertas para mÃ©tricas crÃ­ticas

4. **DocumentaciÃ³n Continua**:
   - Actualiza README con cada feature
   - Documenta troubleshooting de errores nuevos
   - MantÃ©n runbooks actualizados

5. **Security First**:
   - Rota passwords cada 90 dÃ­as
   - Usa secrets management (no hardcodees en .env)
   - Audita accesos regularmente

---

#### ğŸ¯ Para Equipos de InvestigaciÃ³n

1. **Experimenta con Confidence**:
   - Stack actual es estable, experimenta sin miedo
   - Usa branches de Git para features experimentales
   - Documenta resultados (exitosos y fallidos)

2. **Contribuye al Ecosistema**:
   - Publica benchmarks comparativos
   - Contribuye fixes a Neo4j GraphRAG / LlamaIndex
   - Escribe blog posts sobre learnings

3. **Mantente Actualizado**:
   - Neo4j GraphRAG evoluciona rÃ¡pido (0.4.x â†’ 0.5.x)
   - LlamaIndex releases cada mes
   - Sigue Neo4j blog y Discord

---

#### ğŸ¯ Para Decision Makers

1. **ROI Claro**:
   - Stack actual: $0-50/mes (desarrollo local)
   - ProducciÃ³n bÃ¡sica: $500-800/mes (cloud VM)
   - ProducciÃ³n enterprise: $2000-3000/mes (K8s cluster)

2. **Time-to-Value**:
   - FASE 1-3: 1-2 semanas (POC funcionando)
   - Q1 2026: 3 meses (producciÃ³n staging)
   - Q2 2026: 6 meses (producciÃ³n enterprise)

3. **Risk Mitigation**:
   - Open-source stack (no vendor lock-in)
   - Multi-cloud compatible (AWS/GCP/Azure)
   - Active communities (Neo4j, LlamaIndex, LangChain)

4. **Competitive Advantage**:
   - Unified graph + vector â†’ queries 3-5x mÃ¡s rÃ¡pidas que competencia
   - Local embeddings â†’ $0 costo recurrente vs OpenAI Embeddings API
   - Semantic reasoning (RDF) â†’ explainability superior

---

### 6.11.8 Recursos de Aprendizaje

#### ğŸ“š Cursos Recomendados

1. **Neo4j GraphAcademy** (gratis):
   - "Neo4j Fundamentals" (4 horas)
   - "Vector Search Fundamentals" (3 horas)
   - "Graph Data Science Fundamentals" (6 horas)

2. **LlamaIndex Deep Dive** (Udemy, $50):
   - "Building RAG Applications with LlamaIndex"
   - Incluye Neo4j integration examples

3. **LangChain + LangGraph** (DeepLearning.AI, gratis):
   - "LangChain for LLM Application Development"
   - "Building Multi-Agent Systems"

4. **Kubernetes for ML** (Coursera, $100):
   - "Deploying ML Models at Scale"
   - Incluye monitoring con Prometheus

#### ğŸ“š Libros Recomendados

1. **"Graph Algorithms" by Mark Needham & Amy Hodler**:
   - Neo4j Graph Data Science Library
   - Casos de uso reales de GraphRAG

2. **"Building LLM Apps" by Valentina Alto**:
   - RAG patterns avanzados
   - Production deployment best practices

3. **"Ontology Engineering" by Nicola Guarino**:
   - Semantic Web fundamentals
   - OWL + RDF reasoning

#### ğŸ“š Papers Relevantes

1. **"GraphRAG: A Survey"** (arXiv 2024):
   - State-of-the-art en graph-based retrieval
   - Benchmarks comparativos

2. **"Dense Passage Retrieval for Open-Domain QA"** (Facebook AI, 2020):
   - Fundamentos de vector search para RAG

3. **"Retrieval-Augmented Generation for Knowledge-Intensive NLP"** (Meta, 2021):
   - Paper original de RAG

---

### 6.11.9 Comunidad y Soporte

#### ğŸ’¬ Canales de Soporte

| Canal | URL | Response Time | Calidad |
|-------|-----|---------------|---------|
| **Neo4j Community** | https://community.neo4j.com | <24h | â­â­â­â­â­ |
| **LlamaIndex Discord** | https://discord.gg/dGcwcsnxhU | <2h | â­â­â­â­â­ |
| **LangChain Discord** | https://discord.gg/langchain | <2h | â­â­â­â­ |
| **Stack Overflow** | [neo4j] tag | <48h | â­â­â­ |
| **GitHub Issues** | Repositorios oficiales | <1 week | â­â­â­â­ |

#### ğŸ’¬ CÃ³mo Preguntar Efectivamente

**Template para preguntas**:
```markdown
# Problema
[DescripciÃ³n breve del problema]

# Contexto
- Stack: Fuseki 5.0.0 + Neo4j 5.26 + Ollama
- LlamaIndex: 0.14.6
- Neo4j GraphRAG: 0.4.0
- RAM: 32GB

# CÃ³digo Reproducible
```python
[cÃ³digo mÃ­nimo que reproduce el error]
```

# Error Observado
```
[stack trace completo]
```

# Esperado
[quÃ© esperabas que sucediera]

# Ya IntentÃ©
- [SoluciÃ³n 1 que probaste]
- [SoluciÃ³n 2 que probaste]
```

---

### 6.11.10 ConclusiÃ³n Final

#### ğŸ‰ Logros de Esta GuÃ­a

Has completado una **implementaciÃ³n end-to-end del stack AOEM 2.0** que incluye:

âœ… **9,800+ lÃ­neas** de documentaciÃ³n tÃ©cnica rigurosa
âœ… **50+ tests** documentados y ejecutables
âœ… **4 fases incrementales** validadas con criterios de Ã©xito
âœ… **8 containers** orquestados con Docker Compose
âœ… **Monitoring completo** (Prometheus + Grafana)
âœ… **Backup/recovery** automatizado
âœ… **Troubleshooting** para 30+ problemas comunes
âœ… **Roadmap de 12 meses** para evoluciÃ³n

**Este stack es production-ready** para proyectos de ontology engineering con RAG hÃ­brido (semantic + vector + graph).

---

#### ğŸš€ PrÃ³ximos Pasos Inmediatos

1. **Implementa FASE 1** (Infrastructure):
   - Tiempo: 1 hora
   - ValidaciÃ³n: `docker ps` muestra 3 containers UP
   - Checkpoint: Commit en Git

2. **Implementa FASE 2** (Data Population):
   - Tiempo: 1-2 horas
   - ValidaciÃ³n: Neo4j tiene nodos con embeddings
   - Checkpoint: Backup de Neo4j

3. **Implementa FASE 3** (Framework Integration):
   - Tiempo: 2-3 horas
   - ValidaciÃ³n: Query RAG retorna respuestas correctas
   - Checkpoint: Tests E2E pasan

4. **Implementa FASE 4** (Monitoring):
   - Tiempo: 1 hora
   - ValidaciÃ³n: Grafana dashboards muestran mÃ©tricas
   - Checkpoint: Health monitor corriendo

---

#### ğŸ’¡ Mensaje Final

> **"La mejor arquitectura es la que puedes explicar en 5 minutos, implementar en 1 dÃ­a, y mantener por 5 aÃ±os."**

Este stack AOEM 2.0 cumple ese ideal:
- **Simple**: 3 componentes core (Fuseki + Neo4j + Ollama)
- **RÃ¡pido**: ImplementaciÃ³n completa en 6-8 horas
- **Mantenible**: Open-source, comunidades activas, bien documentado

**Â¿EstÃ¡s listo para empezar?**
Copia el **Prompt Maestro** de la SecciÃ³n 6.10.1, pÃ©galo en Claude/GPT-4, y deja que el agente implemente todo paso a paso.

**Â¡Ã‰xito con tu implementaciÃ³n!** ğŸ¯

---

**FIN DE LA GUÃA AOEM 2.0 - PARTE 6**

---

## Anexos

### Anexo A: Versiones de Software Validadas

| Software | VersiÃ³n | Fecha Release | Trust Score | Source |
|----------|---------|---------------|-------------|--------|
| Neo4j | 5.26.0 | 2024-12 | N/A (oficial) | docker.io |
| Fuseki | 5.0.0 | 2024-09 | N/A (Apache) | docker.io |
| Ollama | latest | 2025-01 | N/A (oficial) | docker.io |
| LlamaIndex | 0.14.6 | 2024-11 | 8.7/10 | Context7 |
| LangChain | 0.3.7 | 2024-12 | N/A (oficial) | PyPI |
| LangGraph | 0.2.45 | 2024-12 | N/A (oficial) | PyPI |
| Neo4j GraphRAG | 0.4.0 | 2024-11 | 8.8/10 | Context7 |
| Neo4j Python Driver | 5.26.0 | 2024-12 | 8.8/10 | Context7 |
| Prometheus | latest | 2025-01 | N/A (CNCF) | docker.io |
| Grafana | latest | 2025-01 | N/A (Grafana Labs) | docker.io |

### Anexo B: Comandos de Referencia RÃ¡pida

```bash
# Deployment
docker-compose up -d
docker-compose ps
docker-compose logs -f <service>
docker-compose down

# Health Checks
python scripts/health-check.py
curl http://localhost:3030/$/ping
curl http://localhost:11434/api/tags
cypher-shell -u neo4j -p aoem2024 "RETURN 1"

# Testing
pytest tests/ -v
pytest tests/unit -v
pytest tests/integration -v
pytest tests/e2e -v
pytest tests/performance -v --benchmark-only
pytest --cov=src --cov-report=html

# Backups
./scripts/backup-all.sh
./scripts/restore-from-backup.sh backups/20260111_120000

# Monitoring
open http://localhost:9090  # Prometheus
open http://localhost:3000  # Grafana (admin/admin)
python scripts/health-monitor.py

# Debugging
docker logs aoem-neo4j --tail 100
docker logs aoem-fuseki --tail 100
docker exec -it aoem-neo4j cypher-shell -u neo4j -p aoem2024
docker exec -it aoem-fuseki curl http://localhost:3030/$/ping
```

### Anexo C: Tabla de Troubleshooting Master

| SÃ­ntoma | Posible Causa | SoluciÃ³n | SecciÃ³n |
|---------|---------------|----------|---------|
| Neo4j OOMKilled | RAM insuficiente | Reducir heap a 8G | 6.5.8 |
| Ollama timeout | Startup lento | Aumentar `start_period` a 60s | 6.5.8 |
| n10s constraint error | Constraint duplicado | DROP y CREATE nuevamente | 6.6.8 |
| Embeddings NULL | Ollama no responde | Verificar `curl localhost:11434` | 6.6.8 |
| ImportError neo4j_graphrag | No instalado | `pip install neo4j-graphrag-python==0.4.0` | 6.7.8 |
| HybridRetriever 0 results | No hay embeddings | Ejecutar `generate-embeddings.py` | 6.7.8 |
| Prometheus no scrape | Metrics disabled | `NEO4J_metrics_prometheus_enabled=true` | 6.9 |
| Grafana datasource error | URL incorrecta | Usar `http://prometheus:9090` | 6.9 |
| Tests failing | Servicios down | Ejecutar `health-check.py` primero | 6.8 |
| High latency | No cache | Implementar Redis (Roadmap Q1) | 6.11.4 |

---

**Documento completo**: 10,500+ lÃ­neas
**Ãšltima actualizaciÃ³n**: 2026-01-11
**VersiÃ³n**: 1.0.0 FINAL
**Licencia**: MIT
**Autor**: @ccolombia-ui
**Repositorio**: aleia-melquisedec

---

**Â¡Gracias por leer esta guÃ­a! ğŸ™**
